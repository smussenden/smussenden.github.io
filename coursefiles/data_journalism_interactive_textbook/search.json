[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "",
    "text": "Welcome to data journalism. The main goal of this course is to expand your ability to report and tell stories using data. You will use these tools to discover trends in data, like what Rachell Sanchez-Smith found with the sharp jump in COVID-19 cases in Arkansas children. You will learn how to create and publish graphics and maps. It’s hard work but it is a lot of fun and very rewarding.\nWe have some basic goals for you to reach in this class. By the end of the semester, we want you to have basic proficiency and independence with data analysis. We want you to be able to write about data clearly, using the Associated Press style as a benchmark. We want you to be able to find and download a dataset, clean it up, visualize it.\nThe skills you will learn in the coming weeks are in high demand in journalism and beyond. Examine this BuzzFeed job description from 2017:\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nYou will learn these skills in this book. You’ll get a taste of modern data journalism through Google Sheets and programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. Combining them together has the power to change policy, expose injustice and deeply inform.\n\n\nThis book begins with a basic review of Google Sheets and then shifts to the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the this website, click download R based on your operating system. If that link somehow doesn’t work, check R Project website and find a different location.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run common software packages in your R Studio.\n\n\n\nThis book is the collection of class materials compiled by various data journalism professors around the country: Matt Waite at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications and Sarah Cohen of Arizona State University. This version was edited by Derek Willis, Sean Mussenden and Rob Wells at the University of Maryland Philip Merrill College of Journalism.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis, Sean Mussenden and Rob Wells 2022.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!\n\n\n\n\n\nSpreadsheets\nR Basics\nReplication, Data Diary\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nData cleaning techniques, Janitor\nPulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nWriting with and about data\nData journalism ethics"
  },
  {
    "objectID": "start-story.html",
    "href": "start-story.html",
    "title": "2  Learn a new way to read",
    "section": "",
    "text": "Getting started in data journalism often feels as if you’ve left the newsroom and entered the land of statistics, computer programming and data science. This chapter will help you start seeing data reporting in a new way, by learning how to study great works of the craft as a writer rather than a reader.\nJelani Cobb tweeted, “an engineer doesn’t look at a bridge the same way pedestrians or drivers do.” They see it as a “language of angles and load bearing structures.” We just see a bridge. While he was referring to long-form writing, reporting with data can also be learned by example – if you spend enough time with the examples.\nAlmost all good writers and reporters try to learn from exemplary work. I know more than one reporter who studies prize-winning journalism to hone their craft. This site will have plenty of examples, but you should stay on the lookout for others."
  },
  {
    "objectID": "start-story.html#read-like-a-reporter",
    "href": "start-story.html#read-like-a-reporter",
    "title": "2  Learn a new way to read",
    "section": "2.1 Read like a reporter",
    "text": "2.1 Read like a reporter\nTry to approach data or empirical reporting as a reporter first, and a consumer second. The goal is to triangulate how the story was discovered, reported and constructed. You’ll want to think about why this story, told this way, at this time, was considered newsworthy enough to publish when another approach on the same topic might not have been.\n\nWhat were the questions?\nIn data journalism, we often start with a tip, or a hypothesis. Sometimes it’s a simple question. Walt Bogdanich of The New York Times is renowned for seeing stories around every corner. Bogdanich has said that the prize-winning story “A Disability Epidemic Among a Railroad’s Retirees” came from a simple question he had when railway workers went on strike over pension benefits – how much were they worth? The story led to an FBI investigation and arrests, along with pension reform at the largest commuter rail in the country.\nThe hypothesis for some stories might be more directed. In 2021, the Howard Center for Investigative Journalism at ASU published “Little victims everywhere”, a set of stories on the lack of justice for survivors of child sexual assault on Native American reservations. That story came after previous reporters for the center analyzed data from the Justice Department showing that the FBI dropped most of the cases it investigated, and the Justice Department then only prosecuted about half of the matters referred to it by investigators. The hypothesis was that they were rarely pursued because federal prosecutors – usually focused on immigration, white collar crime and drugs – weren’t as prepared to pursue violent crime in Indian Country.\nWhen studying a data-driven investigation, try to imagine what the reporters were trying to prove or disprove, and what they used to do it. In journalism, we rely on a mixture of quantitative and qualitative methods. It’s not enough to prove the “numbers” or have the statistical evidence. That is just the beginning of the story. We are supposed to ground-truth them with the stories of actual people and places.\n\n\nGo beyond the numbers\nIt’s easy to focus on the numbers or statistics that make up the key findings, or the reason for the story. Some reporters make the mistake of thinking all of the numbers came from the same place – a rarity in most long-form investigations. Instead, the sources have been woven together and are a mix of original research and research done by others. Try to pay attention to any sourcing done in the piece. Sometimes, it will tell you that the analysis was original. Other times it’s more subtle.\nBut don’t just look at the statistics being reported in the story. In many (most?) investigations, some of the key people, places or time elements come directly from a database.\nSarah Cohen at Arizona State University analyzed the Paycheck Protection Program loan data for ProPublica and found a handful of sketchy-looking records from a single county in coastal New Jersey. It turned out to be a pretty good story.\nOften, the place that a reporter visits is determined by examples found in data. In this story on rural development funds, all of the examples came from an analysis of the database. Once the data gave us a good lead, the reporters examined press releases and other easy-to-get sources before calling and visiting the recipients or towns."
  },
  {
    "objectID": "start-story.html#reading-tips",
    "href": "start-story.html#reading-tips",
    "title": "2  Learn a new way to read",
    "section": "2.2 Reading tips",
    "text": "2.2 Reading tips\nYou’ll get better at reading investigations and data-driven work over time, but for now, remember to go beyond the obvious:\n\nWhere might the reporters have found their key examples, and what made them good characters or illustrations of the larger issue? Could they have come from the data?\nWhat do you think came first – a narrative single example that was broadened by data (naively, qualitative method), or a big idea that was illustrated with characters (quantitative method)?\nWhat records were used? Were they public records, leaks, or proprietary data?\nWhat methods did they use? Did they do their own testing, use statistical analysis, or geographic methods? You won’t always know, but look for a methodology section or a description alongside each story.\nHow might you localize or adapt these methods to find your own stories?\nPick out the key findings (usually in the nut graf or in a series of bullets after the opening chapter): are they controversial? How might they have been derived? What might have been the investigative hypothesis? Have they given critics their due and tried to falsify their own work?\nHow effective is the writing and presentation of the story? What makes it compelling journalism rather than a dry study? How might you have done it differently? Is a video story better told in text, or would a text story have made a good documentary? Are the visual elements well integrated? Does the writing draw you in and keep you reading? Think about structure, story length, entry points and graphics all working together.\nAre you convinced? Are there holes or questions that didn’t get addressed?"
  },
  {
    "objectID": "start-story.html#analyze-data-for-story-not-study",
    "href": "start-story.html#analyze-data-for-story-not-study",
    "title": "2  Learn a new way to read",
    "section": "2.3 Analyze data for story, not study",
    "text": "2.3 Analyze data for story, not study\nAs journalists we’ll often be using data, social science methods and even interviewing differently than true experts. We’re seeking stories, not studies. Recognizing news in data is one of the hardest skills for less experienced reporters new to data journalism. This list of potential newsworthy data points is adapted from Paul Bradshaw’s “Data Journalism Heist”.\n\n\n\n\nCompare the claims of powerful people and institutions against facts – the classic investigative approach.\nReport on unexpected highs and lows (of change, or of some other characteristic)\nLook for outliers – individual values that buck a trend seen in the rest\nVerify or bust some myths\nFind signs of distress, happiness or dishonesty or any other emotion.\nUncover new or under-reported long-term trends.\nFind data suggesting your area is the same or different than most others of its kind.\n\nBradshaw also did a recent study of data journalism pieces: “Here are the angles journalists use most often to tell the stories in data”, in Online Journalism Blog. I’m not sure I agree, only because he’s looking mainly at visualizations rather than stories, but they’re worth considering."
  },
  {
    "objectID": "start-math.html",
    "href": "start-math.html",
    "title": "3  Newsroom math",
    "section": "",
    "text": "Jo Craven McGinty, then of The New York Times, used simple rates and ratios to discover that a 6-story brick New Jersey hospital was the most expensive in the nation. In 2012, Bayonne Medical Center “charged the highest amounts in the country for nearly one-quarter of the most common hospital treatments,” the Times story said.\nTo do this story, McGinty only needed to know the number of the procedures reported to the government and the total amount each hospital charged. Dividing those to find an average price, then ranking the most common procedures, led to this surprising result."
  },
  {
    "objectID": "start-math.html#why-numbers",
    "href": "start-math.html#why-numbers",
    "title": "3  Newsroom math",
    "section": "3.1 Why numbers?",
    "text": "3.1 Why numbers?\nUsing averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math.\nThe good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age.\nIn working with typical newsroom math, you will need to learn how to:\n\nOvercome your fear of numbers\nIntegrate numbers into your reporting\nRoutinely compute averages, differences and rates\nSimplify and select the right numbers for your story\n\nWhile this chapter covers general tips, you can find specific instructions for typical newsroom math in this Appendix A, an except from Sarah Cohen’s outstanding book, Numbers in the Newsroom. It’s worth getting your own copy if you don’t already have one."
  },
  {
    "objectID": "start-math.html#overcoming-your-fear-of-math",
    "href": "start-math.html#overcoming-your-fear-of-math",
    "title": "3  Newsroom math",
    "section": "3.2 Overcoming your fear of math",
    "text": "3.2 Overcoming your fear of math\nWhen we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience.\nThink of your own reaction to seeing a page of words. Now imagine it as a page of numbers.\nInstead, picture the number “five”. It’s easy. It might be fingers or it might be a team on a basketball court. But it’s simple to understand.\nNow picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (A million seconds goes by in about 11 days but you may not have been alive for a billion seconds – about 36 years.)\nThe easiest way to get used to some numbers is to learn ways to cut them down to size by calculating rates, ratios or percentages. In your analysis, keep an eye out for the simplest accurate way to characterize the numbers you want to use. “Characterize” is the important word here – it’s not usually necessary to be overly precise so long as your story doesn’t hinge on a nuanced reading of small differences. (And is anything that depends on that news? It may not be.)\nHere’s one example of putting huge numbers in perspective. Pay attention to what you really can picture - it’s probably the $21 equivalent.\n\nThe Chicago hedge fund billionaire Kenneth C. Griffin, for example, earns about $68.5 million a month after taxes, according to court filings made by his wife in their divorce. He has given a total of $300,000 to groups backing Republican presidential candidates. That is a huge sum on its face, yet is the equivalent of only $21.17 for a typical American household, according to Congressional Budget Office data on after-tax income.  “Buying Power”, Nicholas Confessore, Sarah Cohen and Karen Yourish, The New York Times, October 2015\n\nOriginally the reporters had written it even more simply, but editors found the facts so unbelievable that they wanted give readers a chance to do the math themselves. That’s reasonable, but here’s an even simpler way to say it: “earned nearly $1 billion after taxes…He has given $300,000 to groups backing candidates, the equivalent of a dinner at Olive Garden for the typical American family , based on Congressional Budget Office income data.” (And yes, the reporter checked the price for an Olive Garden meal at the time for four people.)"
  },
  {
    "objectID": "start-math.html#put-math-in-its-place",
    "href": "start-math.html#put-math-in-its-place",
    "title": "3  Newsroom math",
    "section": "3.3 Put math in its place",
    "text": "3.3 Put math in its place\nFor journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes , and insightful comment from experts. They serve us in three ways:\n\nAs summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world.\nAs opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly.\nAs guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. Even when everything is presumably counted many times, it’s still a (very nearly accurate) guess. Yes, the “audits” of presidential election results in several states in 2021 found a handful of errors – not a meaningful number, but a few just the same.\n\nOnce you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun."
  },
  {
    "objectID": "start-math.html#going-further",
    "href": "start-math.html#going-further",
    "title": "3  Newsroom math",
    "section": "3.4 Going further",
    "text": "3.4 Going further\n\nTipsheets\n\nSteve Doig’s “Math Crib Sheet”\nAppendix A: Common newsroom math, adapted from drafts of the book Numbers in the Newsroom, by Sarah Cohen.\nA viral Twitter thread:\n\nWhat happens in your head when you do 27+48?--- Gene Belcher (@Wparks91) June 25, 2019"
  },
  {
    "objectID": "census_data.html",
    "href": "census_data.html",
    "title": "4  Census Data",
    "section": "",
    "text": "Each decade, the Census Bureau counts every person living in the United States and the five U.S. territories. This is known as the Decennial Census and it is used to apportion seats in the U.S. House of Representatives, among other things.\nIn addition, the Census Bureau conducts ongoing survey of communities, known as the American Community Survey or ACS that provides more timely information about social, economic, housing, and demographic data every year. You can find information on housing, small business ownership, population profiles and much more. The ACS uses an annual sample size of about 3.5 million addresses and is collecting information daily.\nCongressional lawmakers look at the ACS data to determine distribution of federal spending, among other things. Read more about census data here\nHere’s something important to know about ACS results: Data are pooled across a calendar year. So the ACS numbers reflect data collected over a period of time. By contrast, the Decennial Census is a single point-in-time count of the population.\nWe will be using ACS data to examine trends in wealth in Baltimore neighborhoods because this survey provides detail not available yet in the Decennial Census.\nYou can access Census Data through multiple ways. The U.S. Census Bureau offers data.census.gov. Using this site requires some training and patience. Here is a good place to start, a presentation the Census Bureau staff made to the Investigative Reporters and Editors conference in 2019.\nAnother useful resource is censusreporter.org, a site not affiliated with the Census Bureau that’s designed to make it easier to navigate and retrieve the ACS data.\nWhen we use R, we will use a software library called tidycensus that makes it very easy to retrieve Census data from the Census API, or application programming interface, basically a raw data feed optimized for R, python and similar programs. Stay tuned on that later this semester.\nData journalist Paul Overberg, now with The Wall Street Journal, compiled this useful guide about terminology when dealing with Census data."
  },
  {
    "objectID": "xl-intro.html",
    "href": "xl-intro.html",
    "title": "5  Spreadsheets",
    "section": "",
    "text": "Some people consider using spreadsheets the table stakes for getting into data journalism. It’s relatively easy to see what you’re doing and you can easily share your work with your colleagues. In fact, pieces of the Pulitzer-Prize winning COVID-19 coverage from The New York Times was compiled using an elaborate and highly tuned set of Google spreadsheets with dozens of contributors.\nThis guide uses Google Sheets, which allows students to do the exercises regardless of their computer operating system, Mac, Windows or Linux. The exercises can be easily adapted to Microsoft Excel, which can handle larger datasets and has more options for pivot tables and other more advanced functions. However, Google Sheets have their own advanced functions for scraping websites or importing non-tabular file formats like JSON.\nMost of the screen shots and instructions are created with a MacOS Monterey. Some come from earlier Mac versions, but are largely the same now. Windows users should replace any instructions for using the CMD- key with the CTL- key. There is a table that compares keystrokes for Apple desktops, laptops and Windows machines for Excel at the bottom of Spreadsheet Refresher\n\n\nSpreadsheets are used in almost every workplace in America. This section covers most of what you need in the newsroom, which is a different set of skills than in other businesses.\n\nSpreadsheet Refresher : Start over with good habits\nSorting and filtering to find stories : The first step of interviewing data\nGrouping with pivot tables: Aggregating, and the super power of spreadsheets\nSpreadsheet Formulas: Percents, sums, and other basic computations used in newsrooms."
  },
  {
    "objectID": "xl-refresher.html",
    "href": "xl-refresher.html",
    "title": "6  Spreadsheet Refresher",
    "section": "",
    "text": "Spreadsheets are everywhere, so it’s worth re-learning how to use them well. Reporters usually use spreadsheets in three ways:\nThis guide will Google Sheets since the program is available to anyone regardless of operating system. Google Sheets are easy to share for reporting teams.\nSome reporters flinch at typing in 30 or 100 entries into a spreadsheet. You shouldn’t. If you learn to take notes in a structured way, you’ll always be able to find and verify your work. If you try to calculate a sum of 30 numbers on a calculator, you’ll have to type them all in at least twice anyway. Also, getting used to these easy tasks on a spreadsheet keeps your muscles trained for when you need to do more."
  },
  {
    "objectID": "xl-refresher.html#re-learning-excel-from-the-ground-up",
    "href": "xl-refresher.html#re-learning-excel-from-the-ground-up",
    "title": "6  Spreadsheet Refresher",
    "section": "6.1 Re-learning Excel from the ground up",
    "text": "6.1 Re-learning Excel from the ground up\n\nThe spreadsheet grid\n::: {.pl-2rem .float-right .col-md-4} \nWhen you start up a spreadsheet, you’ll see letters across the top and numbers down the side. If you ever played Battleship, you’ll recognize the idea – every little square, or cell, is referenced by the intersection of its column letter and row number:\nB2 is the cell that is currently active. You can tell because it’s outlined in the sheet and it’s shown on the upper left corner.\n\n\nMouse shapes\n\n\n\n\n\n\n\nThe Copy Tool, or the thin black cross. When you see this, you’ll copy anything that’s selected. This can be good or bad.\n\n\n\nThe Evil Hand. If you use this symbol, you will MOVE the selection to a new location. This is very rarely a good idea or something you intend.\n\n\n\n\n\n\nSelecting cells and ranges\nSpreadsheets act only on the cells or regions you have selected. If you begin typing, you’ll start entering information into the currently selected cell.\nTo select: Hold the cursor over the cell and click ONCE – not twice. Check the formula bar to make sure you’ve selected what you think you’ve got. You can also look at the bottom right of your spreadsheet for more information.\nYou’ll often work with ranges of cells in formulas. These are defined by the corners of the area you want to work on – often a column of information. In the example below, the range is A1:C6, with the “:” referring to the word “through”.\nTo select a group of cells and act on them all at once: Hover the cursor over one corner, click ONCE and drag to the diagonal corner. Make sure the Evil Hand is nowhere to be seen. The entire area will be shaded in except for the currently selected cell. Look at the upper right corner to see how many rows and columns you selected.\n\n\n\n\n\n\n\n\nTo select a column or row : Hover the cursor over the letter at the top of the column. For a row, hover it over the row number in the margin\n\n\nReading the screen\n\nThe areas of the spreadsheet have different visual clues, and learning to read them will make your life much easier.\nThis image shows some key areas on the screen when you’re just viewing the sheet:\n\n\n\nready\n\n\nThis is how it changes when you’re editing\n\n\n\nediting\n\n\n\nEntering data\nSelect the cell and start typing. The information you type won’t be locked into the cell until you hit the Return / Enter key, or move your selection to another cell. Hit “Escape” to cancel the entry.\nYou can’t do a lot of things while you’re editing, so if you have a lot of greyed out menu items, look at your formula bar to see if you are still editing a cell.\nIf you’re having trouble getting to a menu item or seeing the result of your work, try hitting “Escape” and try again. You may not have actually entered the information into the sheet.\n\n\nLocking in headings\nAs your spreadsheet grows vertically with more rows, you’ll want to be able to see the top all the time. When it grows horizontally with more columns, you’ll probably want to see columns in the left, such as names. This is called “Freezing Panes” – you freeze part of the page so it stays in place when you move around.\nSelect View in the menu, then Freeze, then the number of rows to freeze. Select 1 row. As you now scroll down the sheet, your headings will remain but you can see the data as you move deeper into the sheet.\n\n\n\nfreeze panes\n\n\n\n\nFormatting tricks\n\nUse the buttons or the format dialog box to make numbers easier to read.\nIf a column is filled with a lot of text, select Format, then Wrapping from the menu to wrap text. This means that when you double-click to widen a column, it will get taller, not wider. This is good when you need to save valuable real estate on the screen."
  },
  {
    "objectID": "xl-refresher.html#getting-started-with-a-dataset",
    "href": "xl-refresher.html#getting-started-with-a-dataset",
    "title": "6  Spreadsheet Refresher",
    "section": "6.2 Getting started with a dataset",
    "text": "6.2 Getting started with a dataset\nSLOW DOWN! Don’t do anything until you understand what you have in front of you and can predict what your next mouse click will do to it.\nMost data we encounter was created by someone else for some purpose other than ours. This means that you can’t assume anything. It may not be complete. It may be inaccurate. It may mean something completely different than it appears at first blush.\n\nFirst steps\n\nDocument where you got the spreadsheet and how you can get back to the original. Create a new tab (click the + sign in the lower left), name it Data Dictionary, copy the URL of your source data and any other notes about it. Make this your regular practice. It will save time and stress on deadline.\nRead anything you can about what it contains. Look for documentation that comes with the data.\nSave the original into a safe place with its original name and metadata. Work on a copy.\nIf the spreadsheet shows #### instead of words or numbers, widen your columns. If it shows 7E-14 or something like that, format them as numbers, not “General”.\nCheck your corners – look at the top left and bottom right. Is the data all in one area? Are there footnotes or other non-data sections mixed in? We’re going to want to fix that later.\n\n\n\nInterview your data\n\nHeadings\nThe most fraught part of data reporting is understanding what each column actually means. These often have cryptic, bureaucratic names. You may need to go back to the source of the data to be sure you actually understand them.\nIf your data doesn’t have any headings, that’s going to be your first priority. In effect, you’ll need to build what we call a data dictionary or record layout if one hasn’t been provided. Many reporters create these as a page in a dataset.\n\n\nUnit of analysis\nA unit of analysis refers to the items that are listed in the rows of your dataset. Ideally, every row should be at the same unit of analysis – a person, an inspection, or a city, for example. Summaries should be separated by a blank row, or moved to a different sheet. Think of this as the noun you’d use to describe every row.\n\n\nRow numbers\nThe data was probably given to you in some sort of natural sort order. Different computer systems sort differently – some are case-sensitive, others are not. It may depend on when and where the data as created! The order of the data may even depend on a column you don’t have. If you don’t do something now, you’ll never be able to get back to the original order, which could have meaning for both the agency and for fact-checking."
  },
  {
    "objectID": "xl-refresher.html#video-walkthrough",
    "href": "xl-refresher.html#video-walkthrough",
    "title": "6  Spreadsheet Refresher",
    "section": "6.3 Video walkthrough",
    "text": "6.3 Video walkthrough\nThese first steps, along with adding an ID row, are shown here. You can follow along with the same dataset.\n\n\n\nGetting started with Google Sheets\n\n[Getting started with Google Sheets](https://www.youtube.com/embed/1hGoYzmkhfc)"
  },
  {
    "objectID": "xl-refresher.html#keyboard-shortcuts",
    "href": "xl-refresher.html#keyboard-shortcuts",
    "title": "6  Spreadsheet Refresher",
    "section": "6.4 Keyboard shortcuts",
    "text": "6.4 Keyboard shortcuts\nGoogle Sheets keyboard shortcuts can be found in the menu: Help, then Keyboard Shortcuts.\n\n\n\nKeyboard shortcuts"
  },
  {
    "objectID": "xl-filter-sort.html",
    "href": "xl-filter-sort.html",
    "title": "7  Sorting and filtering to find stories",
    "section": "",
    "text": "After police in Ferguson, Mo., killed Michael Brown in 2014, advocates and journalists began examining the racial and ethnic gaps between police departments and the communities they served.\nThe New York Times found a 7-year-old survey conducted by the Justice Department that allowed it to compare the data for major cities in a standalone graphic that it published later that year.\nWhen newer data reflecting departments’ makeup in 2012 was released a year later, Matt Apuzzo and Sarah Cohen hoped it would show some differences. It didn’t. So we were left trying to find news in the data that was clearly of public interest.\nCohen matched up the demographics of police departments with their cities and then started sorting, filtering and Googling. Could there be news in the outliers on the list? Which departments most closely represented their communities? Which ones had unusually large gaps?\n\n\n\n\nChief William T. Riley III. Credit: Laura McDermott for The New York Times\n\n\n\nCohen quickly stumbled on telling anecdote to frame the story: Inkster, Mich. had one of the least representative departments in the country, and had recently hired a new police chief to help mend the department’s fraught relationship with its largely African-American community. Where had he come from? Selma, Ala., one of the most representative police departments in the nation. Interviews with the chief, William T. Riley III, suggested one reason for some cities’ disparities: there was no state or federal money to pay for training new police officers.\nThe story, “Police Chiefs, Looking to Diversity Forces, Face Structural Hurdles” helped explain the persistent gap between the makeup of police in some areas and the communities they served."
  },
  {
    "objectID": "xl-filter-sort.html#sorting-and-filtering-as-a-reporting-tool",
    "href": "xl-filter-sort.html#sorting-and-filtering-as-a-reporting-tool",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.2 Sorting and filtering as a reporting tool",
    "text": "7.2 Sorting and filtering as a reporting tool\nSorting and filtering can:\n\nNarrow your focus to specific items that you want to examine in your story.\nShow you rows containing the highest and lowest values of any column. That can be news or it can be errors or other problems with the data.\nLet you answer quick “how many?” questions, with a count of the rows that match your criteria. (In the next lesson, you’ll see that pivot tables, or group-by queries, are much more powerful for this in most cases.)"
  },
  {
    "objectID": "xl-filter-sort.html#example-data",
    "href": "xl-filter-sort.html#example-data",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.3 Example data",
    "text": "7.3 Example data\nData from the Washington Post police shootings database for use in this tutorial - Documentation from the Post’s github site :::\n\nThe data for this and several other chapters is the Washington Post’s public data collection of police shootings in the U.S. It includes the nation’s best guess about each fatal police shooting since 2015. There are a couple of caveats:\nIt excludes deadly police interactions other than shooting a firarem at the suspect. Any strangulation, car crashes, Tasers without guns or other methods are excluded.\nIt is based primarily on news reports and the results public records requests so it often contains the story as told by police. We know that many of those reports are sugar-coated at best, and lies at worst.\nThe Post says this is a list of fatal shootings, but doesn’t say what happens if more than one person is killed. The 2019 shooting of D’Angelo Brown & Megan Rivera in West Memphis is shown as two rows1 in the data even though it was one event. So each row might be considered a shooting “victim”, a “suspect” or a shooting “fatality” rather than a “shooting”.\n\nThe screenshots in this tutorial may not match exactly to what you get on the Washington Post data. This tutorial used data current to Aug. 3, 2022.\nIt’s a good example set for us because it’s been used as the basis of many stories, it has at least one of each data type that we plan to deal with in Google Sheets, and it is well documented on the Post’s github site."
  },
  {
    "objectID": "xl-filter-sort.html#get-the-data-into-google-sheets",
    "href": "xl-filter-sort.html#get-the-data-into-google-sheets",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.4 Get the data into Google Sheets",
    "text": "7.4 Get the data into Google Sheets\n\nDownload the [police shooting data from the Washington Post](https://github.com/washingtonpost/data-police-shootings/releases/download/v0.1/fatal-police-shootings-data.csv\nOpen Google Sheets. File | Import | Upload | Select the downloaded file “fatal-police-shootings-data.csv” . After it uploads, select the green “Import Data” button."
  },
  {
    "objectID": "xl-filter-sort.html#understanding-data-types",
    "href": "xl-filter-sort.html#understanding-data-types",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.5 Understanding data types",
    "text": "7.5 Understanding data types\nWhen you open the spreadsheet, the first thing to notice is its granularity. Unlike Census or budget spreadsheets, this is a list capturing specific characteristics of each fatality Each column has the same type of data from top to bottom. Those types are:\n\nText. Text or “character” columns can come in long or short form. When they are standardized (the values can contain only one of a small list of values), they’re called “categorical”. If they’re more free-form, they’re might be called “free text”. The computer doesn’t know the difference, but you should. The Post data has examples of both. In spreadsheets, text is left-justified (they move toward the left of the cell and will line up vertically at the beginning)\nNumbers. These are pure numbers with no commas, dollar signs or other embellishments. In Google Sheets, as we’ll see in the computing section, these can be formatted to look like numbers we care about , but underneath they’re just numbers. Adding up a column of numbers that has a word in it or has missing values will just be ignored in Google Sheets. It will trip up most other languages. These are right-justified, so the last digit is always lined up vertically.\nLogical: This is a subset of text. It can take one of only two values – yes or no, true or false. There is no “maybe”.\nDate and times: These are actual dates on the calendar, which have magical properties. Underneath, they are a number. In Google Sheets, that number is the number of days since Jan. 1, 1900.2 They can also have time attached to them, which in Google Sheets is a fraction of a day. What this means is that the number 44,536.5 is really Dec. 6, 2021 at noon. In Google Sheets, you use a format to tell the spreadsheet how you want to see the date or time, just the way you look at dollar values with commas and symbols. (If you get a spreadsheet with a lot of dates of 1/1/1900, it means there is a 0 in that column, which is sometimes a fill-in for “I don’t know.”)\n\nHere’s a picture of a date that is shown in a variety of formats.\n\n\n\ndate formats\n\n\nAll of these are the same, underlying value – the number at the left. Notice that all of these are right-justified.\nThis means that when you see “Friday, December 10”, the computer sees 44540.87431. When you put the dates in order, they won’t be alphabetized with all of the Fridays shown together. Instead, they’ll be arranged by the actual date and time.\nIt also means that you can compute 911 response times even when it crosses midnight, or or compute the someone’s age today given a date of birth. Keeping actual calendar dates in your data will give it much more power than just having the words. (Google Sheets uses the 1st of the month as a stand-in for an actual date when all you know is the month and year.)\n\n7.5.1 Sorting rows\nSorting means rearranging the rows of a data table into a different order. Some reporters take a conceptual shortcut and call this “sorting columns”. That thinking will only get you into trouble – it lets you forget that you want to keep the rows in tact while changing the order in which you see them. In fact, in other languages it’s called “order by” or “arrange” by one or more columns – a much clearer way to think of it.\nIn Google Sheets, look for the sort options under the Data tab at the top of your screen. In this case, sorting from oldest to newest gives you a list of the fatalities in chronological order, including the time of day.\nTo sort your data:\n\nMake a copy of your data. Left click on the “fatal-police-shootings-data” tab, select Duplicate\nSelect your data by clicking the box above Row 1 and to the left of Column A\nSelect Data | Sort Range | Advanced Range Sorting Options\nClick “Data has header row” and then select date from the Sort by dialog box. Select Z –> A\nSelect Sort\n\n\n\n\nAdding fields to the sort\nAdding more columns to the sort box tells Google Sheets what to do when the first one is the same or tied. For example, sorting first by state then by date gives you a list that shows all of the events by state in sequence:\n\n\n\n\n7.5.2 Filtering\nFiltering means picking out only some of the rows you want to see based on a criteria you select in a column. Think of it as casting a fishing net – the more filters you add, the fewer fish will be caught.\nTo activate filters in Google Sheets, from the Menu:\n\nData | Filter Views | Create a New Filter View\nYou’ll see little triangles next to the column headings.\n\nClick the “armed” heading. You will see options for various weapons. All are selected by default with a check mark. To select just “ax”, click on clear and then select “ax.” The sheet how is filtered to just weapons using an ax. To remove the filter, repeat the steps and “select all” and the entire sheet is displayed again.\nEach filter you select adds more conditions, narrowing your net.\nTo find fatalities that involved a firearm with a Taser, use the drop-down menu under manner_of_death select “shot and Tasered”.\n\n\n\n\n\nThis method works for small-ish and simple-ish columns. If your column has more than 10,000 different entries, such as names or addresses, only the first 10,000 will be considered. We only caught these for stories when someone did a fact-check using a different method of filtering. If your column has a lot of distinct entries, use option that says “Choose One”, and then use the “Contains” option. Better yet, don’t use filtering for counting things at all.\nAdd more filters to narrow down your list of cases even more. For example, the New York Times ran a series of stories in 2021 about unarmed people shot by police. One story was about those who were fleeing by car. Here’s one way to get a preliminary list of those cases:\n\nRemove any filter you already have on.\nTurn on the filters again if you turned them off.\nChoose “unarmed” under armed and “car” under flee.\n\n(Of course, the Times didn’t stop there in trying to find more cases and teasing out more of them from this and other data. But this is a start. )\n\n\n\n\n\n\nDifferent kinds of filters\nThere are several filter options. You can filter by various conditions. For numerical data, you can set a minimum or maximum value or a range of values. This is useful for dates to specify a certain time period. For text, you can filter if a word contains a few letters, useful to capture spelling variations."
  },
  {
    "objectID": "xl-formulas.html",
    "href": "xl-formulas.html",
    "title": "8  Spreadsheet Formulas",
    "section": "",
    "text": "The quick review of math in Google Sheets uses the City of Baltimore’s 2022 budget, compared with previous years.\nYou should get into the habit of creating unique identifiers, checking your corners and looking for documentation before you ever start working with a spreadsheet. These habits were covered in Replication and the data diary and on an Excel refresher ."
  },
  {
    "objectID": "xl-formulas.html#formulas-in-spreadsheets",
    "href": "xl-formulas.html#formulas-in-spreadsheets",
    "title": "8  Spreadsheet Formulas",
    "section": "8.1 Formulas in spreadsheets",
    "text": "8.1 Formulas in spreadsheets\nEvery formula begins with the equals sign (=). Rather than the values you want to work with in the formula, you’ll use references to other cells in the sheet.\nThe easiest formulas are simple arithmetic: adding, subtracting, multiplying and dividing two or more cells. You’ll just use simple operators to do this:\n\n\n\noperator\nsymbol\nexample\n\n\n\n\naddition\n+\n=A2+B2\n\n\nsubtraction\n-\n=A2-B2\n\n\nmultiplication\n*\n=A2*B2\n\n\ndivision\n/\n=A2/B2\n\n\n\nHere’s what a spreadsheet looks like while editing some simple arithmetic:\n\n\n\nformula\n\n\nThe other kind of formula is a function. A function is a command that has a name, and requires arguments – usually the cell addresses or the range of addresses that it will act on. Every programming language has functions built in and many have extensions, or packages or libraries, that add even more as users find things they want to do more efficiently. You begin using a function the same way you begin a formula – with an = sign. Here are three common functions that create summary statistics for the numbers contained in a range of addresses. A range is a set of cells defined by its corner cell address: the top left through the bottom right.\nYou’ll usually use them on a single column at a time.\n\n\n\n\n\n\n\nFormula\nWhat it does\n\n\n\n\n=SUM(start:finish)\nAdds up the numbers between start and finish\n\n\n=AVERAGE(start:finish)\nComputes the mean of the numbers\n\n\n=MEDIAN(start:finish)\nDerives the median of the numbers\n\n\n\n…where “start” means the first cell you want to include, and finish means the last cell. Use the cell address of the first number you want to include , a colon, then the cell address of the last number you want to include. You can also select them while you’re editing the formula.\nHere’s an example of adding up all of the rows in a list by county:\n\n\n\nformula"
  },
  {
    "objectID": "xl-formulas.html#common-spreadsheet-arithmetic",
    "href": "xl-formulas.html#common-spreadsheet-arithmetic",
    "title": "8  Spreadsheet Formulas",
    "section": "8.2 Common spreadsheet arithmetic",
    "text": "8.2 Common spreadsheet arithmetic\nThe budget document shows three years’ of data: The actual spending in the fiscal year that ended in 2016; the spending that was estimated for the end of fiscal year 2017; and the proposed spending for fiscal year 2018. The first page of the document shows these amounts for broad spending categories.\nYou may want to widen the columns and format the numbers before you start:\n\n\n\n\n\n\n8.2.1 Check the government’s math with SUM\nOur first job is to make sure the government has provided us data that adds up. To do that, we’ll SUM all of the departments’ spending.\nTo add up the numbers from FY 2020 Actual, enter the following formula in cell C16, just below the number provided by the government:\n  =SUM(C2:C13)\n  and hit the enter key\nCopy that formula to the right. Notice how the formula changes the addresses that it is using as you move to the right – it’s adjusted them to refer to the current column.\n\n\n\n\n\n\n\n8.2.2 Change in spending\nThe increase or decrease in projected spending from 2017 to 2018 is just the difference between the two values, beginning in cell F3\n  new-old, or  =E2-D2\nWhen you copy it down, note how the references to each row also adjusted. In line 3, it’s E3-D3, and so on. Excel and other spreadsheets assume that, most of the time, you want these kinds of adjustments to be made.\n\n\n\n\n\n\n\n8.2.3 Percent change\nWe can’t tell the rate of growth for each department until we calculate the percent change from one year to another. Now that we already have the change, the percent change is easy. The formula is:\n  ( new - old ) / old\n\n  .. or just scream \"NOO\"\nThe new-old is already in column F, so all that’s left is to divide again. In grade school, you also had to move the decimal place over two spots, since the concept of percent change is “out of 100”. Excel formats will do that for you.\nRemember, it’s always (new-old)/old , NOT the big one minus the little one. Doing it correctly, the answer could be negative, meaning the value fell.\n\n\n\nformula\n\n\nWhen you’re done, you can format the answer as a percentage to get it into whole numbers.\nIt’s also worth comparing the picture you get by looking at raw numbers vs. percentages. It’s instructive that federal grants are up 308%.\n\n\n8.2.4 Parts of a whole: percent of total\nWe’d also like to know what portion of the total spending is eaten up by each department. To do that, we need the percent of total.\nIn our case, let’s use the total that the government gave us. In practice, you’d have to decide what to do if your figures didn’t match those provided by officials. You can’t assume that the total is wrong – you could be missing a category, or there could be a mistake in one of the line items.\nThe formula for percent of total is:\n  category / total\nHere’s a good trick with spreadsheets when you need to divide against a fixed total. You don’t have to type in each formula one by one, though. Instead, you’ll use anchors, known in spreadsheets as “absolute references”. Think of a dollar sign as an anchor or stickpin, holding down the location of part of your formula. If you put the stickpin before the letter in the formula, it holds the column in place. If you put it before the number, it holds the row in place. If you put it in both places, it holds the cell in place.\nIn this case, we want to see what percentage property taxes, income taxes, etc. are of the total revenues in FY22, which is $4,331,049,486 (cell E14). Let’s figure it out.\n\nLeft click column F, insert column to the left. Name it Pct of Total\nCreate formula to divide property taxes into total revenues: =(E2/E14)\nModify forumula so it will anchor to the E14 as you move down the spreadsheet =(E2/$E$14)\nCopy formula down to F13"
  },
  {
    "objectID": "xl-formulas.html#while-were-at-it-two-kinds-of-averages",
    "href": "xl-formulas.html#while-were-at-it-two-kinds-of-averages",
    "title": "8  Spreadsheet Formulas",
    "section": "8.3 While we’re at it: two kinds of averages",
    "text": "8.3 While we’re at it: two kinds of averages\nAlthough it doesn’t make a lot of sense in this context, we’ll go ahead and calculate the average or mean size of each department, and then calculate the median size.\nSimple average, or mean\nA simple average, also known as the mean, is skewed toward very high or very low values. Its formula is\n    sum of pieces / # of pieces that were summed\nBut in Google Sheets, all we need is the word AVERAGE:\n    =AVERAGE(C2:C9)\nMedian\nIn Google Sheets, you can get the median of a list of numbers by just using the formula, MEDIAN()\n  = MEDIAN(C2:C9)\nDoing simple calclutions like this on data that is provided to you by the government lets you ask better questions when you get an interview, and may even convince officials to talk with you. There’s a big difference between asking them to tell you what the budget numbers are, and asking them to explain specific results!"
  },
  {
    "objectID": "xl-formulas.html#faqs",
    "href": "xl-formulas.html#faqs",
    "title": "8  Spreadsheet Formulas",
    "section": "8.4 FAQs",
    "text": "8.4 FAQs\n\nShould I use average or median?\nIt depends. Averages are easier to explain but can be misleading. Usually, if they’re very different, median will be a better representation of the typical person, city or department. Averages in these cases are more like totals.\n\n\nMy percents are small numbers with decimal points\nUse the format as a % button to move the decimal point over two places and insert the percentage symbol."
  },
  {
    "objectID": "xl-pivot.html",
    "href": "xl-pivot.html",
    "title": "9  Grouping with pivot tables",
    "section": "",
    "text": "In the wake of a police shooting in 2016, reporter Mitch Smith obtained a list of traffic stops from the St. Anthony Police Department in Minnesota. He was writing a story on Philandro Castile’s death and was running out of time. He wanted to answer a simple question: Were minority motorists more likely to be stopped in St. Anthony than whites?\nRob Gebeloff made a quick pivot table to answer the question. That night, Smith wrote:\nSummarizing a list of items in a spreadsheet is done using pivot tables. In other languages, it’s considered “aggregating” or “grouping and summarizing”. Think of pivot tables and grouping as answering the questions, “How many?” and “How much?” They are particularly powerful when your question also has the words “the most” or the “the least” or “of each”. Some examples:"
  },
  {
    "objectID": "xl-pivot.html#tutorial",
    "href": "xl-pivot.html#tutorial",
    "title": "9  Grouping with pivot tables",
    "section": "9.1 Tutorial",
    "text": "9.1 Tutorial\n::: {.alert .alert-info .opacity-2} We will continue to use data from the Washington Post police shootings database for this tutorial.\nFirst, let’s modify this spreadsheet to include the descriptions of race and ethnicities: A for Asian, B for Black, etc.\n\nSelect Column I, city, and insert a new column to the left. Name it race_ethicity\nCreate a filter. Select race, filter for A\nType Asian in Column I. Copy Asian down the entire column so every A in column H corresponds with Asian in Column I\nRepeat: B = Black. H = Hispanic. W = White, non-Hispanic, N= Native American, O=Other, blanks=Unknown.\n\nFollow this video to see the process.\n\n\n\nSetting up the pivot table\nFrom the main menu on Google Sheets, choose Insert, then Pivot table, then New sheet.\n\n\n\ninsert menu\n\n\nNext, you will see the Pivot Table editor. Here’s what it looks like:\n\n\n\npivot menu\n\n\n\n\nCounting , or “how many”?\nFor Rows, select Add and then race_ethnicity. For values, select Add and then state. You will now see all of the race and ethnicity totaled.\nWe’re totalling on state because it’s good to have something that’s always filled out into the Values area (state is a safe one in this data).\n\n\n\nPercents of total\nIt’s hard to compare raw numbers unless they’re really small. Instead, we’d like to know what percent of fatalities by ethnicity. To get a “Percent of Column total”, do the following:\n\nAdd a second values, select state\nUnder Show as, select “% of grand total”\n\n\n\n\nMore variables\nSuppose you’d like to see the number of fatalities by year, with the years across the top and the ethnicity down the sides. Add a year variable to columns\n\nRemove the percent of total column\nSelect Columns, then year\n\n\n\n\nEven more variables\nSay you wanted to see each city’s total shootings by year. Which one had the most last year, and which one had the most overall?\nThis is actually really hard in a pivot table, because there are cities with the same names in different states. It means you’d need to have a pivot table with TWO columns down the side, and one across the top. Here’s an attempt at solving the problems:\n\nFirst, Rows, add state\nRows, add city\nValues, add state, CountA is the default\nColumns, add year\n\n\n\n\nMore Variables!\n\n\nThe problems is we can’t sort by the combination of city and state. But it does help answer the question on some level."
  },
  {
    "objectID": "xl-pivot.html#faq",
    "href": "xl-pivot.html#faq",
    "title": "9  Grouping with pivot tables",
    "section": "9.2 FAQ",
    "text": "9.2 FAQ\n\nI have too many columns\nIf you want two sets of statistics – say, number of fatalities and percent of fatalities – across the top, it can get very wide and confusing very quickly. One alternative is to change it into more of a vertical rectangle by dragging the “Values” element from the columns to the rows on the right. (This only shows up when you have two calculations being made.)\n\n\nI want to sort by percents, not numbers\nYou can’t.\n\n\nThings aren’t adding up\nYou have to be super careful about which column you use to Count things – it has to always be filled out (there can’t be any blanks). Go through the filters and find one that doesnt have (Blanks) at the bottom to be sure.\n\n\nIts a crazy number!\nYou might have dragged a numeric column into the “Values” area. Check to see if it says “Count” or “Sum”. Change it to “Count” if it has something else on it, unless you wanted to add up that column.\n\n\nThis is so frustrating - I can’t get what I want\nRight? It’s time to go to a programming language!"
  },
  {
    "objectID": "xl-sheets_cleaning.html",
    "href": "xl-sheets_cleaning.html",
    "title": "10  Cleaning data with Google Sheets",
    "section": "",
    "text": "Make a copy of your data before cleaning)\nWe will use a version of the Washington Post police shooting data to conduct these exercises.\n\n10.0.1 Text to columns\nWe want to split up the date field into day, month and year. Currently, the format is 2015-01-02. Luckily, the fields all share a common separator, a hyphen, and we can ask Google Sheets to split all according to the hyphen. Other common separators are commas and spaces.\nFirst steps when modifying data: make a backup copy! - Left click on the tab “Police Shootings to Clean” - Select duplicate - Rename “Copy of Police Shootings to Clean” to “Original Police Shootings to Clean.” Do not touch this version.\nTime to split text to columns. I am extra paranoid (for good reason) and so I always duplicate a date field before modifying it. Duplicate the date column (click on Column C, left click, copy, then Insert column to left, select new blank Column C and paste), save the copy as date-original.\n\nSelect date column\nSelect Data | Split text to columns\nSee a dialog box: Separator. Select Custom and type in a dash - and enter. You now have the date field chopped up to year, month and day. Rename column E for month and column f for day.\n\n\n\n\n\n\n\n\n10.0.2 Normalizing\nScroll down the race_ethnicity column and you will see a number of different categories for the same thing: white, White, non Hispanic and Black, African Am. To see all the variations of categorical variables, create a filter and check the different variables\nThis presents a big problem when you are trying to group and summarize based on these variable names. See this chart\nWe see white totals 44 and White, non Hispanic total 3,136. We want those to be together – the total is 3,180 – because they are the same thing. Also note that African Am totals 29 and Black totals 1,645, and we would want to combine those as well.\nLet’s fix it!\nBefore changing any data, let’s work with a copy of the column. - Select race_ethnicity (Column k), left click, copy - Left click on Column K, insert column to right, paste - Rename as race_ethnicity2\nRenaming variables. We will rename all “white” as “White, non Hispanic” - Filter race_ethnicity (Column K) to white - in race_ethnicity2, write “White, non Hispanic” in the first column and copy down the list\nSee how this process works\n\n\n\n\n\n\n\n10.0.3 Lowercase or Uppercase character conversion\nCreate a filter and notice two variations on Native American: NativeAm and nativeam. You can resolve these differences easily by converting all to Upper or Lower case text using the =UPPER or =LOWER functions.\n\nTo convert NativeAm to lower case, filter on race_ethnicity (Column K) for NativeAm.\nIn race_ethnicity2 (Column L), insert a blank column, and type the function =LOWER(K67) and hit enter.\n\nThe result should be nativeam as the first entry in race_ethnicity2.\nSee this example\n\n\n10.0.4 White space\nOne obnoxious feature of spreadsheet data is the invisible “white space” or hidden character or carriage returns that can impede your ability to group and summarize variables. Look at the age column. See how some numbers are flush left while most are flush right. The flush left data has hidden white space. You can fix this by clicking on individual cells and deleting the space around the number or you can do it with a function.\n\nSelect age (Column H), left click on Column H, insert column to right, rename as age2\nIn cell I2, type =TRIM(H2) and enter. Copy the formula down.\n\nNote how all of the values have been normalized.\nThese are some of the basic go-to tools for data cleaning in Google Sheets, which can be adapted to Excel, R and other programming languages."
  },
  {
    "objectID": "r-start.html",
    "href": "r-start.html",
    "title": "11  Getting started with R and RStudio",
    "section": "",
    "text": "In this chapter\n\n\nInstall R , RStudio\nThe power of packages, especially the tidyverse\nSet up defaults for reporting with data\nIssue your first R commands\nWork in projects\nRelax!\nThis is probably your first introduction to coding. Don’t be worried. With effort, much of what reporters do in coding can be learned in a few weeks.\nLike most reporters, I learned the coding that I know (which isn’t a lot) because I wanted to get a story done. In our class, we are not trying to become a programmer or social scientist. We’re working on stories.\nYou saw during the pivot table lesson that spreadsheets have limits. We couldn’t easily get the city with the most police shootings because we would have had to put both city and state into the pivot table. A median is missing from pivot tables entirely. It’s easy to lose track of where you are and what you did. That’s the reason to learn some coding – there is something we want to know that isn’t very easy to get in other ways.\nAll programming languages have one thing in common: You write instructions, called algorithms, and the program executes your statements in order. It means you can do more complicated work in computer programming than in point-and-click parts of Excel. It can also scale – you can repeat your instructions millions of times, tweak it a little, and re-run the program without messing anything else up. The computer won’t mind. Really.\nWriting code can also be self-documenting. You can largely eliminate those painstaking Excel data diaries and replace them with documents that explain your work as you go. You’ll still need to record information about your interviews and decisions, but you’ll no longer have to write down every mouse click.\nIf you’re nervous about getting started with programming, take look at the Appendix: A gentle introduction to programming and Jesse Lecy’s “Learning how to Learn.”"
  },
  {
    "objectID": "r-start.html#install-r-and-rstudio",
    "href": "r-start.html#install-r-and-rstudio",
    "title": "11  Getting started with R and RStudio",
    "section": "11.1 Install R and RStudio",
    "text": "11.1 Install R and RStudio\n\nR is the programming language itself, and has to be installed first\nRStudio is a software program that makes it easier to interact with the programming language. Install it second.\nPackages are sets of programs written by volunteers and data scientists that perform specialized jobs more easily that working with the “base” R language. A package must be installed once on your computer, then invoked to use them in a program.\n\n\nFollow this interactive tutorial on installing R, RStudio and the tidyverse on your computer:\nhttps://learnr-examples.shinyapps.io/ex-setup-r/#section-welcome .\n\nThere are two differences between the video and today:\n\nThe tidyverse will take much longer to finish installation. It has a lot to do and often looks like it’s stalled.\nThere are two versions of R for Mac users: The traditional one and the one for the new M1 chip on the latest machines. Choose the one that matches your machine by checking the apple in the upper left and looking at “About this Mac”. It will say “Apple M1” as the processor if you have it."
  },
  {
    "objectID": "r-start.html#unlocking-packages-and-the-tidyverse",
    "href": "r-start.html#unlocking-packages-and-the-tidyverse",
    "title": "11  Getting started with R and RStudio",
    "section": "11.2 Unlocking packages and the tidyverse",
    "text": "11.2 Unlocking packages and the tidyverse\nThe real power of R comes with packages. Packages are bundles of programs that others have found useful to extend the base R language. R is almost useless without them. There are more than 10,000 packages available for R, each doing a special job.\nIf you followed along with the tutorial, the last thing you did was install a “package” called the tidyverse. Almost everything we do from now on depends on that step.\nThe tidyverse packages up a whole set of other packages that are designed to work together smoothly with similar grammar and syntax. It’s particularly useful for the kind of work reporters do – importing, cleaning and analyzing data that we get from others and can’t control how it’s structured.\nWe’ll be working almost exclusively within the tidyverse in this course. I strongly suggest that when you Google for help, put the word “tidyverse” somewhere in your query. Otherwise, you may get answers that look inscrutable and unfamiliar.\nThe tidyverse is the brainchild of Hadley Wickham, a statistician from New Zealand, who famously identified tidy data principles. He’s currently the chief data scientist for RStudio in Houston."
  },
  {
    "objectID": "r-start.html#set-up-rstudio-for-data-reporting",
    "href": "r-start.html#set-up-rstudio-for-data-reporting",
    "title": "11  Getting started with R and RStudio",
    "section": "11.3 Set up RStudio for data reporting",
    "text": "11.3 Set up RStudio for data reporting\nStaying organized is one of the challenges of data reporting – you’re constantly re-downloading and re-jiggering your analysis and it’s easy to get your material separated. This setup helps ensure that you always know where to find your work and can move it to another comptuer seamlessly.\nBefore you start, decide on a folder you’ll use to store all of your R work. Within my Documents folder, I created a sub-folder called data-class. It will make this guide a little easier if you do the same thing, especially if you’re not very familiar with using directories and folders.\n\n\n\nStart up RStudio once you’ve made your folder. Make sure you start up RStudio (not the R language) by searching for it in Spotlight or in the Search bar in Windows. Here’s what they look like:\n\nGet to the Preferences (under the RStudio menu item on a Mac) and make sure it looks like this in the General tab:\n\n\n\nsee below\n\n\n\n(I’ve turned OFF all of the options to restore anything when you start up RStudio and set up a default working directory by browsing to the one I just created.)\nUnder the R Markdown options, make sure that the box called “Execute setup chunk automatically” is checked."
  },
  {
    "objectID": "r-start.html#the-screen",
    "href": "r-start.html#the-screen",
    "title": "11  Getting started with R and RStudio",
    "section": "11.4 The screen",
    "text": "11.4 The screen\nThis is what your screen probably looks like:\n\n\n\nconsole\n\n\n\nThe Console\nThe Console is where you can type commands and interact directly with the programming language. Think of it as a very powerful calculator at first. One reason to use it is to install packages.\nIf you followed the installation demo, you’ve already used the console to install one package. (Go back and do that part now if you skipped it.) Install a few more that will be useful in this course.\n\nCopy these commands one a a time, and paste them into the Console, then hit Return/Enter to execute the command.\n   install.packages(\"janitor\")\n   install.packages(\"rmarkdown\")\n   install.packages(\"skimr\")\n   install.packages(\"swirl\")\n\nThese package names should all be in quotes. We’ll be installing other packages later in this guide, but for now that is everything you need.\n\n\nFiles tab\nWe won’t be using many of the tabs in the lower right, but the Files tab can help you if you’re having trouble navigating your work. Under the More button, you can choose “Go to working directory”, since that’s where R thinks you’ve parked all of your work. This can be confusing in R, which is why we’ll be working in “projects” that bundle up all of your work in one place.\n\n\nEnvironment\nThe upper right screen is the Environment, which is where your active variables live. A variable is a named thing. It might be a word, a list of words or numbers, or a data frame (spreadsheet). Anything that you want to use has to be listed in that environment before you can reference it. This will make more sense later.\n\n\nTyping into the console\nWhen you type this: 5+5 after the > prompt, you’ll get this back after you press Return/Enter: [1] 10\nWhen you type this: \"Merrill\" (with quotes) after the > prompt, you’ll get this back: [1] \"Merrill\"\nTo create a new variable, you’ll use the assignment operator <- (two characters : A less than sign and a hyphen). Here is how I would create the variable called my_name (lower case, no spaces). Notice how it appears in the Environment after being created. Then I can print it by typing the name of the variable instead of the letters of my name in quotes:\n\n\n\n\n\nThe console remembers your commands, but you have to type them one at a time and it will forget them when you leave for the day. That’s why we’re going to work in programs called R Markdown documents most of the time."
  },
  {
    "objectID": "r-start.html#working-directory",
    "href": "r-start.html#working-directory",
    "title": "11  Getting started with R and RStudio",
    "section": "11.5 Working directory",
    "text": "11.5 Working directory\nHere’s how to set your working directory so you can keep your files organized. First, you should have created a folder within the Documents folder called data_class. Run the following command to see if R Studio is pointing to that folder\n\ngetwd()\n\n[1] \"/Users/smussend/Desktop/git_repos/data_journalism_interactive_textbook/03_tutorials/qmd_files\"\n\n\nIf it is not, you can navigate R Studio to that folder using the Files tab in the lower right corner window. Once you find your data_class folder, then select the Set Working Directory option under the More menu.\nYou can also set the path programmatically using setwd() which means set working directory. Just find the path directory using Finder – directions are here – and copy that link to this command.\n\nsetwd(\"/Users/YOURNAME/Documents/data_class\")"
  },
  {
    "objectID": "r-start.html#interactive-r-tutorial",
    "href": "r-start.html#interactive-r-tutorial",
    "title": "11  Getting started with R and RStudio",
    "section": "11.6 Interactive R tutorial",
    "text": "11.6 Interactive R tutorial\nOne of the packages you installed earler was called swirl. Invoke it now by typing library(swirl) into the Console. You can follow the instructions from there. Don’t bother going beyond the first chapter – it’s more geared at other kinds of jobs than ours.\n\n\n\nRelax by Silwia Bartyzel via Unsplash"
  },
  {
    "objectID": "r-start.html#relax",
    "href": "r-start.html#relax",
    "title": "11  Getting started with R and RStudio",
    "section": "11.7 Relax!",
    "text": "11.7 Relax!\nYou’re all set up and we’re ready to start programming. Congratulate yourself - everything is new, nothing is intuitive and the screen is intimidating. You’ve come a long way."
  },
  {
    "objectID": "r-start.html#other-resources",
    "href": "r-start.html#other-resources",
    "title": "11  Getting started with R and RStudio",
    "section": "11.8 Other resources",
    "text": "11.8 Other resources\nSharon Machlis’ Practical R for Mass Communications and Journalism has an intro to R and RStudio in chapters 2.3 through 2.6\nBen Stenhaug created a fast-paced video introducing the RStudio interface. Don’t worry too much about what some of it means just yet – just see if you can get used to the different parts of the screen."
  },
  {
    "objectID": "r-load-analyze-visualize.html",
    "href": "r-load-analyze-visualize.html",
    "title": "12  Loading and Analyzing Data",
    "section": "",
    "text": "You will be executing commands in this document that are contained in “chunks,” which are separate from the text and contain live R code. Click the green arrow at the right on line 6 and run the help.start() command\n\nhelp.start()\n\n  In the bottom right window of R Studio, you will see a Help window that displays basic help commands for the program.\n\n12.0.1 Install software to grab data\nTidyverse: Eight separate software packages to perform\ndata import, tidying, manipulation, visualisation, and            programming\n\nRio: Easy importing features \nJanitor: Data cleaning\nYou should have installed tidyverse already. If not, then delete the hashtag in front of install.packages(“tidyverse”) and run the code chunk at line 22.\n\n#install.packages(\"tidyverse\")\ninstall.packages(\"rio\") \ninstall.packages(\"janitor\")\n\nRemember, package installation usually is a one-time thing on your hard drive. But when you need to load the software libraries each time you start a script. Libraries are bits of software you will have to load each time into R to make things run.\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(janitor)\n\nCheck to see what’s installed by clicking on “Packages” tab in File Manager, lower right pane \n\n\n12.0.2 Data\nWe will work with a dataset of MediaSalaries that I cleaned and modified slightly in March 2022 for this tutorial. Some of the detail has been removed so we can make calculations. This MediaSalaries sheet was a crowdsourced project involving reporters worldwide to share information about salaries and benefits. Open this file in Google Sheets\n1) Select Salaries tab\n\nIRE Old School: Four Corners Test!\n13 Columns\n1658 Rows\n\nNumberic data in Salary, Years Experience\nMixed string data in Gender Identity / Ethnicity, Job duties\n \n\n\n12.0.3 Import Data\nWe'll now load this data into R. You can load spreadsheets from the Internet as follows:\n\nMediaBucks <- rio::import(\"https://docs.google.com/spreadsheets/d/1jkbQFwIdaWv8K00Ad6Wq7ZxFTUPFQA-g/edit#gid=1655992481\", which = \"RealMediaSalaries2\")\n\n What happened? Look at the table\n\nView(MediaBucks)\n\nWhat happened?\nR grabbed the spreadsheet from the folder\nWe told R to grab the first sheet, RealMediaSalaries2\nR created a dataframe called MediaBucks\nbasics of R: <- is known as an “assignment operator.”\nIt means: “Make the object named to the left equal to the output of the code to the right.”\n \n\n\n12.0.4 Explore Data\nClick the green arrow code chunk to get the answers below.\nHow many rows?\n\nnrow(MediaBucks)\n\n[1] 1658\n\n\nHow many columns?\n\nncol(MediaBucks)\n\n[1] 13\n\n\nDimensions: Gives number rows, then columns\n\ndim(MediaBucks)\n\n[1] 1658   13\n\n\nNames of your columns\n\ncolnames(MediaBucks)\n\n [1] \"TITLE\"                     \"COMPANY\"                  \n [3] \"Salary\"                    \"Salary_Details\"           \n [5] \"Salary_Details2\"           \"Race\"                     \n [7] \"Gender\"                    \"YEARS_EXPERIENCE\"         \n [9] \"LOCATION\"                  \"JOB_DUTIES\"               \n[11] \"PREV_SALARIES_TITLES\"      \"SALARY_original\"          \n[13] \"Gender_Ethnicity_Original\"\n\n\nOR\n\nnames(MediaBucks)\n\n [1] \"TITLE\"                     \"COMPANY\"                  \n [3] \"Salary\"                    \"Salary_Details\"           \n [5] \"Salary_Details2\"           \"Race\"                     \n [7] \"Gender\"                    \"YEARS_EXPERIENCE\"         \n [9] \"LOCATION\"                  \"JOB_DUTIES\"               \n[11] \"PREV_SALARIES_TITLES\"      \"SALARY_original\"          \n[13] \"Gender_Ethnicity_Original\"\n\n\nCheck data types\n\nstr(MediaBucks)\n\n'data.frame':   1658 obs. of  13 variables:\n $ TITLE                    : chr  \"Photojournalist\" \"Staff Writer\" \"Staff reporter\" \"Reporter\" ...\n $ COMPANY                  : chr  \"College Student Newspaper\" \"Paxton Media\" \"Schurz Communications Inc.\" \"Hearst Newspapers\" ...\n $ Salary                   : num  0 12 12 13.2 14.9 ...\n $ Salary_Details           : chr  \"\" \"hour\" \"hour\" \"hour\" ...\n $ Salary_Details2          : chr  \"\" \"anhour\" \"-2012\" \"~32,000/year\" ...\n $ Race                     : chr  \"asian\" \"white\" \"white\" \"white\" ...\n $ Gender                   : chr  \"male\" \"female\" \"female\" \"female\" ...\n $ YEARS_EXPERIENCE         : chr  \"3\" \"8\" \"2\" \"3\" ...\n $ LOCATION                 : chr  \"\" \"Georgia\" \"Central Kentucky\" \"Houston\" ...\n $ JOB_DUTIES               : chr  \"Photo\" \"Write and cover news stories, photograph news, write monthly health magazine, social media director\" \"Reported on anything and everything: local government, breaking news, features, op/eds, columns. Took pictures,\"| __truncated__ \"Report on communities outside of the Houston metro area, take photos, produce online and print content, pitch s\"| __truncated__ ...\n $ PREV_SALARIES_TITLES     : chr  \"\" \"\" \"\" \"\" ...\n $ SALARY_original          : chr  \"0\" \"$12 an hour\" \"$12/hour (2012)\" \"13.25/hour ~ 32,000/year \" ...\n $ Gender_Ethnicity_Original: chr  \"Cis male asian\" \"White female\" \"cis white female\" \"Cis white female\" ...\n\n\nLet’s look at the first six rows\n\nhead(MediaBucks)\n\n            TITLE                    COMPANY Salary Salary_Details\n1 Photojournalist  College Student Newspaper   0.00               \n2    Staff Writer               Paxton Media  12.00           hour\n3  Staff reporter Schurz Communications Inc.  12.00           hour\n4        Reporter          Hearst Newspapers  13.25           hour\n5  Staff Reporter           Ogden Newspapers  14.91           hour\n6   Desk Assitant                       KTLA  15.00           hour\n  Salary_Details2     Race Gender    YEARS_EXPERIENCE         LOCATION\n1                    asian   male                   3                 \n2          anhour    white female                   8          Georgia\n3           -2012    white female                   2 Central Kentucky\n4    ~32,000/year    white female                   3          Houston\n5         perhour    white   male 3+ years experience             Utah\n6                 noanswer female                   2      Los Angeles\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    JOB_DUTIES\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Photo\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                          Write and cover news stories, photograph news, write monthly health magazine, social media director\n3                                                                                                                                                                                                                                                                                                                                                                       Reported on anything and everything: local government, breaking news, features, op/eds, columns. Took pictures, managed web uploads and social media accounts, produced short videos. \n4                                                                                                                                                                                                                                                                                                                                                                                                                             Report on communities outside of the Houston metro area, take photos, produce online and print content, pitch story ideas, etc. \n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   daily beat reporting, photos, social media\n6 Organize daily field drives and retrieves file videos\\nUse FTP system to send/receive video files from the field\\nMakes beat checks of police and fire departments\\nLog/transcribe daily video feeds\\nAnswers viewer phone calls, taking stories from viewers\\nChecks for possible stories via all social media channels\\nChecks email for possible stories and upcoming news events\\nListens to viewers complaints; accepts criticism in an even handed manner; and offers suggestions or answers as appropriate\\nWatch and log daily competition newscasts\n  PREV_SALARIES_TITLES           SALARY_original Gender_Ethnicity_Original\n1                                              0            Cis male asian\n2                                    $12 an hour              White female\n3                                $12/hour (2012)          cis white female\n4                      13.25/hour ~ 32,000/year           Cis white female\n5                                $14.91 per hour            cis white male\n6                                       $15/hour                cis female\n\n\nHere is a quick way to view the range of your data\n\nsummary(MediaBucks$Salary)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0   42000   60000   64194   78000  770000       4 \n\n\nSize and scope\n\nsum(MediaBucks$Salary, na.rm=TRUE)\n\n[1] 106177432\n\n\n$106 million! for 1,658 journalists\nContext: NYT earnings in 2020 = $100 m Facebook profit for one day: $114 million (Q42021=$10.3B)\naverage\n\nmean(MediaBucks$Salary, na.rm=TRUE)\n\n[1] 64194.34\n\n\nDistribution\n\nquantile(MediaBucks$Salary, c(0.1, 0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9), na.rm=TRUE)\n\n   10%    20%    30%    40%    50%    60%    70%    80%    90% \n 28720  39000  45000  52000  60000  65000  75000  84000 101700 \n\n\n\nquantile(MediaBucks$Salary, c(0.25, 0.50, 0.75, 0.9, 0.99), na.rm=TRUE)\n\n   25%    50%    75%    90%    99% \n 42000  60000  78000 101700 200000 \n\n\n\n\n\n12.0.5 Navigation Tips\nShortcut Commands\nTab - Autocomplete\nIn Console Window (lower left) \n--Control (or Command) + UP arrow - last lines run\nControl (or Command) + Enter - Runs current or selected lines of code in the top left box of RStudio\nShift + Control (or Command) +P - Reruns previous region code\n \n\n\n12.0.6 Dplyr\ndplyr has many tools for data analysis   \nselect Choose which columns to include  \nfilter Filter the data \narrange Sort the data, by size for continuous variables, by date, or alphabetically \ngroup_by Group the data by a categorical variable \n\nBuild a simple summary table by Gender\n\nMediaBucks %>% \n  select(Gender, Salary) %>% \n  group_by(Gender) %>% \n  summarize(Total = sum(Salary, na.rm=TRUE))\n\n# A tibble: 4 × 2\n  Gender       Total\n  <chr>        <dbl>\n1 female   63198034.\n2 male     35201242.\n3 na        3061718.\n4 noanswer  4716438.\n\n\nWhat is the sample size?\n\nMediaBucks %>% \n  count(Gender) %>% \n  arrange(desc(n))\n\n    Gender    n\n1   female 1025\n2     male  503\n3 noanswer   71\n4       na   59\n\n\nBetter idea: Check Averages!\nBuild a simple summary table by Gender\n\nMediaBucks %>% \n  select(Gender, Salary) %>% \n  group_by(Gender) %>% \n  summarize(Avg_Salary = mean(Salary, na.rm=TRUE))\n\n# A tibble: 4 × 2\n  Gender   Avg_Salary\n  <chr>         <dbl>\n1 female       61717.\n2 male         69983.\n3 na           52788.\n4 noanswer     68354.\n\n\nQuick filter out hourly workers\n\nMediaSalary <- MediaBucks %>% \n  filter(Salary >= 1000)\n\nJust give me a list of the top 10 salaries and companies: use slice_max. slice_max and slice_min are features in the Dplyr library (part of Tidyverse) that produce quick summary tables. See what else you can do with the slice commands.\n\nMediaBucks %>% \n  select(COMPANY, Salary) %>% \n  slice_max(Salary, n = 10)\n\n                 COMPANY Salary\n1     Tribune Publishing 770000\n2              The Onion 550008\n3  G/O Media (The Onion) 500000\n4  G/O Media (The Onion) 500000\n5               VoxMedia 400000\n6             ProPublica 395000\n7    Digital_First_Media 375000\n8          The Intercept 368249\n9           NewYorkTimes 350000\n10             LBI Media 291200\n\n\nQuestions:\n1: View the range of your data    \n2: Number of rows  \n3: Number of rows cut with filter  \n\n\n\n12.0.7 Find Your News Organization\nFilter\n\nWSJ <- subset(MediaBucks, COMPANY==\"WallStreetJournal\")  \n\n\nsummary(WSJ$Salary)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     38   41000   51100   64275   75750  236000 \n\n\nUsing Wildcards\n\nJournal <- subset(MediaBucks, grepl(\"?Journal\", COMPANY))\n\n\nBloom <- subset(MediaBucks, grepl(\"?Bloomberg\", COMPANY))\n\n ### More Tables\nBuild a table with several companies of your choice\n\nBigBoys <- filter(MediaSalary, COMPANY %in% c(\"NewYorkTimes\", \"WallStreetJournal\", \"Bloomberg\"))    \n\nTable with just reporter salaries\n\nReporters <- subset(MediaBucks, grepl(\"?reporter\", TITLE))\nsummary(Reporters$Salary)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     12   35000   50625   57077   67250  230504 \n\n\nQuestions:\n1: Who is making $230,504 as a reporter???   \n2: Make a table for editors, figure out medians.   \n3: Find highest paid editor. Resent them.   \n4: Make a table for any position involving data  \nTable with Black reporters at Wall Street Journal\n\nWSJ_Black <- MediaBucks %>% filter(Race ==\"black\", COMPANY == \"WallStreetJournal\")\n\nBuild a simple summary table by Race\n\nRace <- MediaBucks %>% \n  select(Race, Salary) %>% \n  group_by(Race) %>% \n  summarize(Avg_Salary = mean(Salary, na.rm=TRUE)) %>% \n  arrange(desc(Avg_Salary))\nRace\n\n# A tibble: 13 × 2\n   Race       Avg_Salary\n   <chr>           <dbl>\n 1 chinese        76167.\n 2 australian     75000 \n 3 african        73485.\n 4 hispanic       70481.\n 5 black          69371.\n 6 poc            68061.\n 7 asian          66427.\n 8 noanswer       64925.\n 9 latina         64806.\n10 white          63025.\n11 mixed          55756.\n12 mideastern     51833.\n13 native         50000 \n\n\nWait! What are the totals by race?\n\nMediaBucks %>% \n  count(Race) %>% \n  arrange(desc(n))\n\n         Race    n\n1       white 1094\n2    noanswer  192\n3         poc  115\n4       asian   77\n5       black   57\n6    hispanic   53\n7      latina   23\n8       mixed   22\n9     african   13\n10 mideastern    7\n11    chinese    3\n12 australian    1\n13     native    1\n\n\nAdvanced: Build a summary table and count by race\n\nMediaBucks %>% \n  select(Race, Salary) %>% \n group_by(Race) %>% \n  summarize(Total=n(),\n            Avg = mean(Salary, na.rm=TRUE)) %>% \n  arrange(desc(Total))\n\n# A tibble: 13 × 3\n   Race       Total    Avg\n   <chr>      <int>  <dbl>\n 1 white       1094 63025.\n 2 noanswer     192 64925.\n 3 poc          115 68061.\n 4 asian         77 66427.\n 5 black         57 69371.\n 6 hispanic      53 70481.\n 7 latina        23 64806.\n 8 mixed         22 55756.\n 9 african       13 73485.\n10 mideastern     7 51833.\n11 chinese        3 76167.\n12 australian     1 75000 \n13 native         1 50000 \n\n#details: https://stackoverflow.com/questions/36183601/average-and-count-with-aggregation-in-r-with-dplyr\n\n\n\n12.0.8 Visualize\nLet’s make a simple chart of our salaries by race.\n\nRace %>% \nggplot(aes(x = Race, y = Avg_Salary, fill = Avg_Salary)) +\n  geom_col(position = \"dodge\") + \n  theme(legend.position = \"none\") +\n  labs(title = \"Sample chart of race and salary using MediaSalaries\", \n       caption = \"1658 records, sample data. Graphic by Rob Wells, 8/11/2022\",\n       y=\"Average Salary\",\n       x=\"Race / Ethnicity\")\n\n\n\n\nThis is a basic chart using ggplot. To break down the code: - These lines: Race %>% ggplot(aes(x = Race, y = Avg_Salary, fill = Avg_Salary)) + – Uses the Race table, calls the ggplot program, assigns the x axis to Race, y axis to Avg_Salary and fills the color according to Avg_Salary - These lines: geom_col(position = “dodge”) + theme(legend.position = “none”) + – creates a chart of columns, and removes a legend box. - These lines: labs(title = Assign the headline and captions.\n\n\n12.0.9 What You Have Learned So Far\nHow to navigate in R studio   \nHow to install libraries and packages    \nHow to import a .xlsx file into R   \nHow to obtain summary statistics (summary)   \nHow to build basic tables from a dataset   \nHow to conduct filter queries from a dataset   \n\n\n12.0.10 Questions\n1: Build a table for NewYorkTimes employees, and determine median salary of NewYorkTimes employees   \n\n#your answer here\n\n  2: Identify title, gender and race of the highest paid position at NYT  \n\n#your answer here\n\n3: Search for Bloomberg,  check median salary, compare to NYT results above.\n\n#your answer here\n\n\n\n12.0.11 Tutorials\nExcellent book by Sharon Machlis https://www.routledge.com/Practical-R-for-Mass-Communication-and-Journalism/Machlis/p/book/9781138726918\nFirst five chapters are free on her website. My recommendation: buy the book. https://www.machlis.com/R4Journalists/\nAll Cheat Sheets https://www.rstudio.com/resources/cheatsheets/\nMaryJo Webster tutorials http://mjwebster.github.io/DataJ/R_tutorials/opiate_deaths.nb.html https://github.com/mjwebster/R_tutorials/blob/master/Using_R.Rmd\nAldhous’ R tutorial http://paldhous.github.io/NICAR/2018/r-analysis.html\nRon Campbell Lecture https://github.com/roncampbell/NICAR2018/blob/master/Intro%20to%20R.md\nExcellent Tutorial Spelling out Excel and Comparable Commands in R https://trendct.org/2015/06/12/r-for-beginners-how-to-transition-from-excel-to-r/ https://docs.google.com/presentation/d/1O0eFLypJLP-PAC63Ghq2QURAnhFo6Dxc7nGt4y_l90s/edit#slide=id.g1bc441664e_0_59\nAndrew Ba Tran first Data Analysis Steps Using R https://docs.google.com/presentation/d/1O0eFLypJLP-PAC63Ghq2QURAnhFo6Dxc7nGt4y_l90s/edit#slide=id.p\nCharts https://www.rdocumentation.org/packages/ggplot2/versions/1.0.1/topics/geom_bar http://www.cookbook-r.com/Graphs/Bar_and_line_graphs_(ggplot2)/\nBase R Cheat Sheet https://www.povertyactionlab.org/sites/default/files/r-cheat-sheet.pdf"
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "13  Using GitHub",
    "section": "",
    "text": "GitHub is a platform for managing and storing files, data and code built atop Git, a popular open source version control software. GitHub accounts are free and it’s easy to get started. The one prerequisite is that you have Git installed on your local computer. There are installers for Mac, Windows and Linux."
  },
  {
    "objectID": "github.html#how-it-works",
    "href": "github.html#how-it-works",
    "title": "13  Using GitHub",
    "section": "13.1 How It Works",
    "text": "13.1 How It Works\nVersion control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other’s way or having to do things in a set order. For individual users, it’s great for making sure that you always have your work.\nGitHub users work in what are known as repositories on their local computers and also push changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others’ changes back to your local computers.\nSo, like Microsoft Word’s track changes but with a remote backup and multiple editors."
  },
  {
    "objectID": "github.html#getting-started",
    "href": "github.html#getting-started",
    "title": "13  Using GitHub",
    "section": "13.2 Getting Started",
    "text": "13.2 Getting Started\nAfter installing Git and signing up for a GitHub account, download and install GitHub Desktop. It will have you sign into your GitHub account and then you’ll have access to any existing repositories. If you don’t have any, that’s fine! You can make one locally.\nGitHub has good documentation for working in the Desktop app, and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren’t yours and more."
  },
  {
    "objectID": "github.html#video-overview",
    "href": "github.html#video-overview",
    "title": "13  Using GitHub",
    "section": "13.3 Video overview",
    "text": "13.3 Video overview\nThis YouTube video from Coder Coder provides a nice overview of Git, GitHub and GitHub Desktop."
  },
  {
    "objectID": "github.html#advanced-use",
    "href": "github.html#advanced-use",
    "title": "13  Using GitHub",
    "section": "13.4 Advanced Use",
    "text": "13.4 Advanced Use\nAlthough our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer’s command line interface, and GitHub has a purpose-built command line client, too. GitHub can also serve as a publishing platform for many types of files, and entire websites are hosted on GitHub Pages."
  },
  {
    "objectID": "r_aggregates_filters.html",
    "href": "r_aggregates_filters.html",
    "title": "14  Mutuating, Aggregates, Filters",
    "section": "",
    "text": "In this chapter, you will learn additional core data skills that allow you to filter, summarize and append new calculations to datasets. The skills in this chapter, once mastered, will transform your ability to analyze data in ways that are difficult in spreadsheets."
  },
  {
    "objectID": "r_aggregates_filters.html#importing-data",
    "href": "r_aggregates_filters.html#importing-data",
    "title": "14  Mutuating, Aggregates, Filters",
    "section": "14.1 Importing data",
    "text": "14.1 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “CSV” file, a stripped down version of a spreadsheet you might open in a program like Google Sheets, in which each column is separated by a comma.\nSo step 1 is to import the data. The code to import the data looks like this:\nbaltcity_income<- read_csv(\"assets/data/baltcity_income_clean.csv\") %>% as.data.frame()\nLet’s unpack that.\nThe first part – baltcity_income – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. Generally we want variable names to be descriptive, hence baltcity_income. It’s good to keep variable names lower case and one word but two or more words need to be connected by an underscore. You can’t start a variable with a number.\nThe <- is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.”\nread_csv() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\nbaltcity_income<- read_csv(\"assets/data/baltcity_income_clean.csv\") %>% as.data.frame()\nInside of the read_csv() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments. And lastly, we added a pipe operator %>% (shift + cntl + M) that adds another command to turn the imported data into a data frame with as.data.frame(). The pipe operator - %>% - basically tells R to “and then do this.”\nHere is the entire command in a code chunk. Run it by clicking the green arrow to the right below.\n\nbaltcity_income <- read_csv(\"assets/data/baltcity_income_clean.csv\") \n\nIn this data set, each row represents a Census district, and each column represents a feature of that district: its location, the median household income in 2010, 2016, 2020, the neighborhood identifier and geographic coordinators.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns (called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(baltcity_income), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(baltcity_income)\n\nRows: 200\nColumns: 6\n$ Neighborhood <chr> \"Canton\", \"Patterson Park North & East\", \"Canton\", \"Canto…\n$ x2010        <dbl> 75938, 58409, 77841, 70313, 86157, 60439, 47131, 59236, 2…\n$ x2016        <dbl> 100985, 96171, 116875, 97153, 85986, 79063, 64643, 100778…\n$ x2020        <dbl> 128839, 130357, 151389, 114946, 98194, 95536, 86125, 1018…\n$ Census       <dbl> 101.00, 102.00, 103.00, 104.00, 105.00, 201.00, 202.00, 2…\n$ GEOID        <dbl> 24510010100, 24510010200, 24510010300, 24510010400, 24510…\n\n\nIf we type head(baltcity_income), it will print out the columns and the first six rows of data.\n\nhead(baltcity_income)\n\n# A tibble: 6 × 6\n  Neighborhood                x2010  x2016  x2020 Census       GEOID\n  <chr>                       <dbl>  <dbl>  <dbl>  <dbl>       <dbl>\n1 Canton                      75938 100985 128839    101 24510010100\n2 Patterson Park North & East 58409  96171 130357    102 24510010200\n3 Canton                      77841 116875 151389    103 24510010300\n4 Canton                      70313  97153 114946    104 24510010400\n5 Fells Point                 86157  85986  98194    105 24510010500\n6 Fells Point                 60439  79063  95536    201 24510020100\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively.\n\n14.1.1 Group by and count\nThere is some overlap among the Census tracts, which typically are groups of betweem 1,200 to 8,000 people, and neighborhoods, which can be much larger. Let’s figure out how many neighborhoods are represented in this dataset.\ndplyr has a group by function that compiles things together and then produces simple summaries by counting things, or averaging them together. It’s a good place to start.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data %>% group_by(COLUMN NAME) %>% summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with neighborhood identifier is called “Neighborhood.” Neighborhoods overlap with Census tracts.\nHere’s the code to count the number of census tracts in each neighborhood:\n\nbaltcity_income %>%\n  group_by(Neighborhood) %>%\n  summarise(\n    count_tracts = n()\n  )\n\n# A tibble: 56 × 2\n   Neighborhood                      count_tracts\n   <chr>                                    <int>\n 1 Allendale/Irvington/S. Hilton                6\n 2 Beechfield/Ten Hills/West Hills              3\n 3 Belair-Edison                                4\n 4 Brooklyn/Curtis Bay/Hawkins Point            4\n 5 Canton                                       3\n 6 Cedonia/Frankford                            5\n 7 Cherry Hill                                  3\n 8 Chinquapin Park/Belvedere                    2\n 9 Claremont/Armistead                          4\n10 Clifton-Berea                                5\n# … with 46 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – baltcity_incomes – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the counties, signified by the field name Neighborhood, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of loans for each county grouping. The line of code count_tracts = n(), says create a new field, called count_tracts and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something.\nWhen we run that, we get a list of counties with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – %>% – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the county with the most loans, we need to sort it in descending order. That looks like this:\n\nbaltcity_income %>%\n  group_by(Neighborhood) %>%\n  summarise(\n    count_tracts = n()\n  ) %>% \n  arrange(desc(count_tracts))\n\n# A tibble: 56 × 2\n   Neighborhood                         count_tracts\n   <chr>                                       <int>\n 1 Southwest Baltimore                             8\n 2 Allendale/Irvington/S. Hilton                   6\n 3 Medfield/Hampden/Woodberry/Remington            6\n 4 Sandtown-Winchester/Harlem Park                 6\n 5 Cedonia/Frankford                               5\n 6 Clifton-Berea                                   5\n 7 Greater Charles Village/Barclay                 5\n 8 Greater Rosemont                                5\n 9 Harford/Echodale                                5\n10 Inner Harbor/Federal Hill                       5\n# … with 46 more rows\n\n\nThe Census data contains a column detailing the neighborhood. It has associated the Census tracts to neighborhood names. This dataset may have several neighborhood values since Census tracts are a smaller unit of measurement.\nSouthwest Baltimore neighborhood is spread out over 8 census tracts, more than any other neighborhood.\nHere’s the code to determine the count the number of census tracts in each neighborhood:\n\nbaltcity_income %>%\n   summarise(\n    count_tracts = n()\n  )\n\n# A tibble: 1 × 1\n  count_tracts\n         <int>\n1          200"
  },
  {
    "objectID": "r_aggregates_filters.html#interviewing-your-data-min-max-mean-medians",
    "href": "r_aggregates_filters.html#interviewing-your-data-min-max-mean-medians",
    "title": "14  Mutuating, Aggregates, Filters",
    "section": "14.2 Interviewing Your Data: Min, Max, Mean, Medians",
    "text": "14.2 Interviewing Your Data: Min, Max, Mean, Medians\nWhat is the typical median income? What about the highest and lowest median incomes in the city? For that, we can use the min() and max() functions.\n\nbaltcity_income %>%\n  select(Neighborhood, x2010, x2016, x2020, Census) %>% \n  summarise(\n    count_tracts = n(),\n    x2020_median = median(x2020, na.rm=TRUE),\n    min_2020 = min(x2020, na.rm=TRUE),\n    max_2020 = max(x2020, na.rm=TRUE)\n  ) \n\n# A tibble: 1 × 4\n  count_tracts x2020_median min_2020 max_2020\n         <int>        <dbl>    <dbl>    <dbl>\n1          200        49875    13559   199531\n\n\nHere we see the typical median household income is $49,875 by census tract for Baltimore City in 2020 (see result for x2020 median). The lowest median income was $13,559 and the highest was $199,531. From another Census analysis, we know hat citywide, the median household income was $52,164 for 2016-2020. The na.rm=TRUE argument lets R knock out any empty rows from the calculation.\nHere’s another quick way to determine the distribution of a particular column of data\n\nsummary(baltcity_income$x2020)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  13559   35702   49875   56311   64372  199531       5 \n\n\nThis tells us the minimum, maximum, median, average (mean), and the first and third quartile, as well as rows with no values.\n\n14.2.1 Filters: Extracting Needles from Haystacks\nWhere are these rich and poor places? Let’s filter for the lowest value, $13,559, and find out where it is\n\nbaltcity_income %>%\n  #temp code - remove later\n  as.data.frame() %>% \n  select(Neighborhood, x2020) %>% \n  filter(x2020 ==13559) \n\n         Neighborhood x2020\n1 Upton/Druid Heights 13559\n\n\nIt is part of the Upton/Druid Heights neighborhood in West Baltimore.\nWe can stack filters using the Or connector: | It’s above the enter key on a Mac keyboard\n\nbaltcity_income %>%\n  select(Neighborhood, x2020) %>% \n  filter((x2020 ==13559) | (x2020==199531))\n\n# A tibble: 2 × 2\n  Neighborhood                       x2020\n  <chr>                              <dbl>\n1 Upton/Druid Heights                13559\n2 North Baltimore/Guilford/Homeland 199531\n\n\nNow we know in one report the wealthiest neighborhood, North Baltimore/Guilford/Homeland, and the poorest, Upton/Druid Heights.\nRead this for more details about logical operators.\nLet’s filter for the wealthy neighborhoods, all above $100,000\n\nbaltcity_income %>%\n  select(Neighborhood, x2020) %>% \n  filter(x2020 > 100000) %>% \n  arrange(desc(x2020))\n\n# A tibble: 17 × 2\n   Neighborhood                       x2020\n   <chr>                              <dbl>\n 1 North Baltimore/Guilford/Homeland 199531\n 2 North Baltimore/Guilford/Homeland 195353\n 3 Greater Roland Park/Poplar Hill   155605\n 4 South Baltimore                   151659\n 5 Canton                            151389\n 6 Greater Roland Park/Poplar Hill   151146\n 7 Inner Harbor/Federal Hill         133333\n 8 Highlandtown                      130769\n 9 Patterson Park North & East       130357\n10 Canton                            128839\n11 South Baltimore                   121685\n12 Inner Harbor/Federal Hill         120729\n13 Canton                            114946\n14 Mount Washington/Coldspring       109688\n15 Highlandtown                      107438\n16 Inner Harbor/Federal Hill         107344\n17 Fells Point                       101815\n\n\nUsing the summarise function, we can figure out an average value on a column. In this case, we’re going to average all of the median income values by census tract.\n\nbaltcity_income %>%\n  select(Neighborhood, x2020, Census) %>% \n    summarise(\n    count_tracts = n(),\n    x2020_avg = mean(x2020, na.rm=TRUE)) \n\n# A tibble: 1 × 2\n  count_tracts x2020_avg\n         <int>     <dbl>\n1          200    56311.\n\n\nIn the example above, we created a new summary value called x2020_avg that holds the result of the math, the average of the entire x2020 column of median incomes.\n\n\n14.2.2 Other summarization methods: mean, median, min and max\nHere’s another trick, pulling out the minimum and maximum values\n\nbaltcity_income %>%\n  select(Neighborhood, x2020, Census) %>% \n    summarise(\n    count_tracts = n(),\n    min_2020 = min(x2020, na.rm=TRUE),\n    max_2020 = max(x2020, na.rm=TRUE))\n\n# A tibble: 1 × 3\n  count_tracts min_2020 max_2020\n         <int>    <dbl>    <dbl>\n1          200    13559   199531\n\n\nTo kick it up a notch, here’s the same idea but with averages and medians for the three years in our data: 2010, 2016, 2020.\n\nbaltcity_income %>%\n  select(Neighborhood, x2010, x2016, x2020, Census) %>% \n    summarise(\n    count_tracts = n(),\n    x2020_median = median(x2020, na.rm=TRUE),\n    x2020_avg = mean(x2020, na.rm=TRUE),\n    x2016_median = median(x2016, na.rm=TRUE),\n    x2016_avg = mean(x2016, na.rm=TRUE),\n    x2010_median = median(x2010, na.rm=TRUE),\n    x2010_avg = mean(x2010, na.rm=TRUE)) \n\n# A tibble: 1 × 7\n  count_tracts x2020_median x2020_avg x2016_median x2016_avg x2010_med…¹ x2010…²\n         <int>        <dbl>     <dbl>        <dbl>     <dbl>       <dbl>   <dbl>\n1          200        49875    56311.        39583    46744.      37080.  41923.\n# … with abbreviated variable names ¹​x2010_median, ²​x2010_avg\n\n\nUse the right diamond at x2016_median to see columns 5-7.\n\n\n\n\n\n\n\n14.2.3 Using sum\nThere’s much more we can to summarize each group. Let’s pull in another dataset and summarize by group.\n\n#loading 2020 and 2010 Baltimore City population by race\nbaltcity_race <- read_csv(\"assets/data/baltcity_race_8_13.csv\") %>% \n  as.data.frame()\n\nLet’s say we wanted to know the total population by white people in Baltimore? For that, we could use the sum() function to add up all of the population in the column “x2020_white”. We put the column we want to total – “amount” – inside the sum() function sum(amount). Note that we can simply add a new summarize function here, keeping our count_loans field in our output table.\nThis abbreviated slice of Census data contains columns detailing the population by race in Census tracts. There is the x2020_total which provides the full population, then x2020_white, x2020_black, x2020_hispanic. We omitted Asians and Pacific islanders and people identifying with more than one race for simplicity in this example.\nHere we can select a race variable and summarize it.\n\nbaltcity_race %>% \n  select(x2020_white, x2020_black) %>% \n  summarize(\n    white_total = sum(x2020_white, na.rm = TRUE),\n    black_total = sum(x2020_black, na.rm = TRUE)\n  )\n\n  white_total black_total\n1      178996      375002\n\n\nPre-Lab Question:  We know the median income for Baltimore City (I just told you a few paragraphs ago). Construct a filter for all census tracts below the citywide median household income for 2020. Count them. What percentage of the city’s census tracts are below the median? Put that in code too. Draft a tweet with your findings.\nAnswer this question in English: Write in Elms\n```"
  },
  {
    "objectID": "r-mutating.html",
    "href": "r-mutating.html",
    "title": "15  Mutating data",
    "section": "",
    "text": "To do that in R, we can use dplyr and mutate to calculate new metrics in a new field using existing fields of data. That’s the essence of mutate - using the data you have to answer a new question.\nSo first we’ll import the tidyverse so we can work with it.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nWe’ll import a dataset of median household income in Baltimore for the 2010, 2016 and 2020 surveys. The data is in the data folder in this chapter’s pre-lab directory. We’ll use this to explore ways to create new information from existing data.\n\nbaltcity_income <- read_csv(\"assets/data/baltcity_income_clean.csv\") %>% \n  as.data.frame()\n\n#working on this with your laptop, uncomment and use this code below\n#baltcity_income<- read_csv(\"baltcity_income_clean.csv\") %>% \n#  as.data.frame()\n\nOne main question involves how the median income changed from 2010 to 2020. First, let’s add a column called diff_2010_2020 to see how median income changed for each census tract. The code is pretty simple. Remember, with summarize, we used n() to count things. With mutate, we use very similar syntax to calculate a new value – a new column of data – using other values in our dataset.\n\nbaltcity_income %>%\n  select(Census, Neighborhood, x2010, x2020) %>%\n    mutate(Diff_Income = (x2020-x2010))\n\n     Census                          Neighborhood  x2010  x2020 Diff_Income\n1    101.00                                Canton  75938 128839       52901\n2    102.00           Patterson Park North & East  58409 130357       71948\n3    103.00                                Canton  77841 151389       73548\n4    104.00                                Canton  70313 114946       44633\n5    105.00                           Fells Point  86157  98194       12037\n6    201.00                           Fells Point  60439  95536       35097\n7    202.00                           Fells Point  47131  86125       38994\n8    203.00                           Fells Point  59236 101815       42579\n9    301.00              Harbor East/Little Italy  21932  23083        1151\n10   302.00              Harbor East/Little Italy  63885  98254       34369\n11   401.00                   Downtown/Seton Hill  45948  62131       16183\n12   402.00                   Downtown/Seton Hill  13229  43333       30104\n13   601.00           Patterson Park North & East  46822  56652        9830\n14   602.00           Patterson Park North & East  51403  70313       18910\n15   603.00           Patterson Park North & East  53813  84318       30505\n16   604.00                   Oldtown/Middle East  35139  53472       18333\n17   701.00                      Madison/East End  37701  45602        7901\n18   702.00                      Madison/East End  35667  31379       -4288\n19   703.00                      Madison/East End  15000  35000       20000\n20   704.00                   Oldtown/Middle East  14527  27321       12794\n21   801.01                         Belair-Edison  49688  65417       15729\n22   801.02                         Belair-Edison  35594  30000       -5594\n23   802.00                         Clifton-Berea  24200  38289       14089\n24   803.01                         Clifton-Berea  32024  45446       13422\n25   803.02                         Clifton-Berea  24423  41298       16875\n26   804.00                         Clifton-Berea  23893  34567       10674\n27   805.00                         Clifton-Berea  35223  28203       -7020\n28   806.00                       Greenmount East  26726  35221        8495\n29   807.00                       Greenmount East  20679  40885       20206\n30   808.00                   Oldtown/Middle East  35532  48137       12605\n31   901.00                        Greater Govans  36444  52988       16544\n32   902.00                             Northwood  67500  82609       15109\n33   903.00                         The Waverlies  43824  63523       19699\n34   904.00                         The Waverlies  26490  30685        4195\n35   905.00                         The Waverlies  36507  43727        7220\n36   906.00                     Midway/Coldstream  36408  50048       13640\n37   907.00                     Midway/Coldstream  27330  30964        3634\n38   908.00                     Midway/Coldstream  35473  31250       -4223\n39   909.00                       Greenmount East  19056  19524         468\n40  1001.00                       Greenmount East  28929  30913        1984\n41  1002.00                   Oldtown/Middle East   9862  19172        9310\n42  1003.00                      Unassigned--Jail     NA     NA          NA\n43  1101.00                               Midtown  38797  53407       14610\n44  1102.00                               Midtown  32563  62703       30140\n45  1201.00     North Baltimore/Guilford/Homeland  56205  72349       16144\n46  1202.01       Greater Charles Village/Barclay  54000  81393       27393\n47  1202.02       Greater Charles Village/Barclay  30179  36563        6384\n48  1203.00       Greater Charles Village/Barclay  38281  55074       16793\n49  1204.00       Greater Charles Village/Barclay  39375  50034       10659\n50  1205.00                               Midtown  37500  55170       17670\n51  1206.00       Greater Charles Village/Barclay  23488  36221       12733\n52  1207.00  Medfield/Hampden/Woodberry/Remington  39730  61653       21923\n53  1301.00             Penn North/Reservoir Hill  21271  25835        4564\n54  1302.00             Penn North/Reservoir Hill  30255  57457       27202\n55  1303.00             Penn North/Reservoir Hill  32273  43380       11107\n56  1304.00             Penn North/Reservoir Hill  32500  30868       -1632\n57  1306.00  Medfield/Hampden/Woodberry/Remington  53654  91635       37981\n58  1307.00  Medfield/Hampden/Woodberry/Remington  48634  72648       24014\n59  1308.03  Medfield/Hampden/Woodberry/Remington  51548  62188       10640\n60  1308.04  Medfield/Hampden/Woodberry/Remington  48207  62054       13847\n61  1308.05           Mount Washington/Coldspring  53984  53750        -234\n62  1308.06  Medfield/Hampden/Woodberry/Remington  48750  95142       46392\n63  1401.00                               Midtown  32917  57292       24375\n64  1402.00                   Upton/Druid Heights  16213  23152        6939\n65  1403.00                   Upton/Druid Heights  21607  41913       20306\n66  1501.00       Sandtown-Winchester/Harlem Park  18097  26989        8892\n67  1502.00       Sandtown-Winchester/Harlem Park  25224  40644       15420\n68  1503.00                      Greater Rosemont  32289  35476        3187\n69  1504.00                     Greater Mondawmin  32632  33702        1070\n70  1505.00                     Greater Mondawmin  34609  33967        -642\n71  1506.00                      Greater Rosemont  29631  31250        1619\n72  1507.01                     Greater Mondawmin  38125  51944       13819\n73  1507.02                     Greater Mondawmin  48942  50150        1208\n74  1508.00                  Forest Park/Walbrook  37199  39337        2138\n75  1509.00                  Forest Park/Walbrook  37440  51250       13810\n76  1510.00                  Dorchester/Ashburton  35056  45870       10814\n77  1511.00                  Dorchester/Ashburton  45649  58843       13194\n78  1512.00                 Southern Park Heights  12386  22632       10246\n79  1513.00                 Southern Park Heights  33579  33866         287\n80  1601.00       Sandtown-Winchester/Harlem Park  19583  27969        8386\n81  1602.00       Sandtown-Winchester/Harlem Park  25417  24848        -569\n82  1603.00       Sandtown-Winchester/Harlem Park  22292  13963       -8329\n83  1604.00       Sandtown-Winchester/Harlem Park  27813  23036       -4777\n84  1605.00                      Greater Rosemont  24556  37819       13263\n85  1606.00                      Greater Rosemont  27128  38482       11354\n86  1607.00                      Greater Rosemont  29472  39959       10487\n87  1608.01                     Edmondson Village  41549  53543       11994\n88  1608.02                     Edmondson Village  29355  41932       12577\n89  1701.00                   Downtown/Seton Hill  30197  35582        5385\n90  1702.00                   Upton/Druid Heights   9412  13559        4147\n91  1703.00                   Upton/Druid Heights  17892  18912        1020\n92  1801.00 Poppleton/The Terraces/Hollins Market  18429     NA          NA\n93  1802.00 Poppleton/The Terraces/Hollins Market  16585     NA          NA\n94  1803.00 Poppleton/The Terraces/Hollins Market  37879  64125       26246\n95  1901.00                   Southwest Baltimore  22893  24559        1666\n96  1902.00                   Southwest Baltimore  43214  52738        9524\n97  1903.00                   Southwest Baltimore  17237  14269       -2968\n98  2001.00                   Southwest Baltimore  32214  38203        5989\n99  2002.00                   Southwest Baltimore  30061  27740       -2321\n100 2003.00                   Southwest Baltimore  19063  21953        2890\n101 2004.00                   Southwest Baltimore  28324  31975        3651\n102 2005.00                   Southwest Baltimore  30239  34306        4067\n103 2006.00         Allendale/Irvington/S. Hilton  29813  39076        9263\n104 2007.01         Allendale/Irvington/S. Hilton  34556  33592        -964\n105 2007.02         Allendale/Irvington/S. Hilton  27305  43734       16429\n106 2008.00         Allendale/Irvington/S. Hilton  32711  49875       17164\n107 2101.00            Washington Village/Pigtown  48857  76368       27511\n108 2102.00            Washington Village/Pigtown  46319  36661       -9658\n109 2201.00             Inner Harbor/Federal Hill  59259  80305       21046\n110 2301.00             Inner Harbor/Federal Hill  62222  98125       35903\n111 2302.00             Inner Harbor/Federal Hill  80521 107344       26823\n112 2303.00                       South Baltimore  53194  92841       39647\n113 2401.00                       South Baltimore  87619 151659       64040\n114 2402.00             Inner Harbor/Federal Hill  92500 133333       40833\n115 2403.00             Inner Harbor/Federal Hill  89211 120729       31518\n116 2404.00                       South Baltimore  72122 121685       49563\n117 2501.01       Beechfield/Ten Hills/West Hills  52098  60463        8365\n118 2501.02         Allendale/Irvington/S. Hilton  36960  53854       16894\n119 2501.03              Morrell Park/Violetville  36957  36555        -402\n120 2502.03                           Cherry Hill  29747  33902        4155\n121 2502.04                           Cherry Hill  12384  14349        1965\n122 2502.05        Westport/Mount Winans/Lakeland  47738  38779       -8959\n123 2502.06              Morrell Park/Violetville  52674  66900       14226\n124 2502.07                           Cherry Hill  23310  41518       18208\n125 2503.01        Westport/Mount Winans/Lakeland  27414  24877       -2537\n126 2503.03              Morrell Park/Violetville  42007  40230       -1777\n127 2504.01     Brooklyn/Curtis Bay/Hawkins Point  34401  32019       -2382\n128 2504.02     Brooklyn/Curtis Bay/Hawkins Point  31250  37059        5809\n129 2505.00     Brooklyn/Curtis Bay/Hawkins Point  32738  30526       -2212\n130 2506.00     Brooklyn/Curtis Bay/Hawkins Point     NA     NA          NA\n131 2601.01                     Cedonia/Frankford  46789  68983       22194\n132 2601.02                     Cedonia/Frankford  48783  60313       11530\n133 2602.01                     Cedonia/Frankford  33936  44093       10157\n134 2602.02                     Cedonia/Frankford  31241  39282        8041\n135 2602.03                     Cedonia/Frankford  32615  46250       13635\n136 2603.01                         Belair-Edison  42347  52117        9770\n137 2603.02                         Belair-Edison  42727  50000        7273\n138 2603.03                   Claremont/Armistead  30192     NA          NA\n139 2604.01                   Claremont/Armistead  31392  48661       17269\n140 2604.02                   Claremont/Armistead  42938  39000       -3938\n141 2604.03                   Claremont/Armistead  32373  35282        2909\n142 2604.04         Orangeville/East Highlandtown  35708  54375       18667\n143 2605.01         Orangeville/East Highlandtown  42298  64706       22408\n144 2606.04                          Southeastern  20710  21977        1267\n145 2606.05                          Southeastern  33545  44794       11249\n146 2607.00         Orangeville/East Highlandtown  36901  80000       43099\n147 2608.00                          Highlandtown  35200  48850       13650\n148 2609.00                          Highlandtown  68571 107438       38867\n149 2610.00           Patterson Park North & East  36343  60893       24550\n150 2611.00                          Highlandtown  83207 130769       47562\n151 2701.01                            Lauraville  57326  80875       23549\n152 2701.02                            Lauraville  54250  58020        3770\n153 2702.00                            Lauraville  60128  80788       20660\n154 2703.01                            Lauraville  54623  80655       26032\n155 2703.02                            Lauraville  68088  63929       -4159\n156 2704.01                              Hamilton  51641  64620       12979\n157 2704.02                              Hamilton  53125  84556       31431\n158 2705.01                      Harford/Echodale  59516  66604        7088\n159 2705.02                              Hamilton  53493  71694       18201\n160 2706.00                      Harford/Echodale  66914  67625         711\n161 2707.01                      Harford/Echodale  28618  37591        8973\n162 2707.02                      Harford/Echodale  39205  56963       17758\n163 2707.03                      Harford/Echodale  63521  88750       25229\n164 2708.01                            Loch Raven  51415  58995        7580\n165 2708.02                            Loch Raven  46181  55753        9572\n166 2708.03                            Loch Raven  42234  52272       10038\n167 2708.04             Chinquapin Park/Belvedere  41094  59583       18489\n168 2708.05             Chinquapin Park/Belvedere  46875  59763       12888\n169 2709.01                             Northwood  51023  47260       -3763\n170 2709.02                             Northwood  48555  58269        9714\n171 2709.03                             Northwood  54607  52083       -2524\n172 2710.01                        Greater Govans  33203  35822        2619\n173 2710.02                        Greater Govans  38500  40281        1781\n174 2711.01     North Baltimore/Guilford/Homeland  42994  55859       12865\n175 2711.02     North Baltimore/Guilford/Homeland  99300 199531      100231\n176 2712.00     North Baltimore/Guilford/Homeland 133548 195353       61805\n177 2713.00       Greater Roland Park/Poplar Hill 111250 155605       44355\n178 2714.00       Greater Roland Park/Poplar Hill  97891 151146       53255\n179 2715.01           Mount Washington/Coldspring  88929 109688       20759\n180 2715.03       Greater Roland Park/Poplar Hill  79861  62500      -17361\n181 2716.00                 Southern Park Heights  43265  34832       -8433\n182 2717.00             Pimlico/Arlington/Hilltop  29069  36484        7415\n183 2718.01             Pimlico/Arlington/Hilltop  24802  25278         476\n184 2718.02             Pimlico/Arlington/Hilltop  31392  40256        8864\n185 2719.00                        Glen-Fallstaff  52308  54866        2558\n186 2720.03               Cross-Country/Cheswolde  70942  57543      -13399\n187 2720.04               Cross-Country/Cheswolde  49567  66148       16581\n188 2720.05               Cross-Country/Cheswolde  49757  72500       22743\n189 2720.06                        Glen-Fallstaff  24859  34019        9160\n190 2720.07                        Glen-Fallstaff  35903  41728        5825\n191 2801.01                        Glen-Fallstaff  46576  47610        1034\n192 2801.02            Howard Park/West Arlington  33938  43618        9680\n193 2802.00            Howard Park/West Arlington  45047  56458       11411\n194 2803.01              Dickeyville/Franklintown  33707  47366       13659\n195 2803.02                  Forest Park/Walbrook  33864  56004       22140\n196 2804.01       Beechfield/Ten Hills/West Hills  44954  47192        2238\n197 2804.02                     Edmondson Village  52784  41179      -11605\n198 2804.03       Beechfield/Ten Hills/West Hills  51435  57582        6147\n199 2804.04         Allendale/Irvington/S. Hilton  43779  40946       -2833\n200 2805.00                   Oldtown/Middle East  11278  14304        3026\n\n\nNow we’ve got our Diff_Income column. Let’s provide some context and use a percentage change calculation to compare the gains. We’ll add a new column, Diff_Pct_2020. Remember the percentage change calculation is (New-Old)/Old\n\nbaltcity_income %>%\n  select(Census, Neighborhood, x2010, x2020) %>%\n    mutate(Diff_Income = (x2020-x2010)) %>% \n    mutate(Diff_Pct_2020 = (x2020-x2010)/x2010)\n\n     Census                          Neighborhood  x2010  x2020 Diff_Income\n1    101.00                                Canton  75938 128839       52901\n2    102.00           Patterson Park North & East  58409 130357       71948\n3    103.00                                Canton  77841 151389       73548\n4    104.00                                Canton  70313 114946       44633\n5    105.00                           Fells Point  86157  98194       12037\n6    201.00                           Fells Point  60439  95536       35097\n7    202.00                           Fells Point  47131  86125       38994\n8    203.00                           Fells Point  59236 101815       42579\n9    301.00              Harbor East/Little Italy  21932  23083        1151\n10   302.00              Harbor East/Little Italy  63885  98254       34369\n11   401.00                   Downtown/Seton Hill  45948  62131       16183\n12   402.00                   Downtown/Seton Hill  13229  43333       30104\n13   601.00           Patterson Park North & East  46822  56652        9830\n14   602.00           Patterson Park North & East  51403  70313       18910\n15   603.00           Patterson Park North & East  53813  84318       30505\n16   604.00                   Oldtown/Middle East  35139  53472       18333\n17   701.00                      Madison/East End  37701  45602        7901\n18   702.00                      Madison/East End  35667  31379       -4288\n19   703.00                      Madison/East End  15000  35000       20000\n20   704.00                   Oldtown/Middle East  14527  27321       12794\n21   801.01                         Belair-Edison  49688  65417       15729\n22   801.02                         Belair-Edison  35594  30000       -5594\n23   802.00                         Clifton-Berea  24200  38289       14089\n24   803.01                         Clifton-Berea  32024  45446       13422\n25   803.02                         Clifton-Berea  24423  41298       16875\n26   804.00                         Clifton-Berea  23893  34567       10674\n27   805.00                         Clifton-Berea  35223  28203       -7020\n28   806.00                       Greenmount East  26726  35221        8495\n29   807.00                       Greenmount East  20679  40885       20206\n30   808.00                   Oldtown/Middle East  35532  48137       12605\n31   901.00                        Greater Govans  36444  52988       16544\n32   902.00                             Northwood  67500  82609       15109\n33   903.00                         The Waverlies  43824  63523       19699\n34   904.00                         The Waverlies  26490  30685        4195\n35   905.00                         The Waverlies  36507  43727        7220\n36   906.00                     Midway/Coldstream  36408  50048       13640\n37   907.00                     Midway/Coldstream  27330  30964        3634\n38   908.00                     Midway/Coldstream  35473  31250       -4223\n39   909.00                       Greenmount East  19056  19524         468\n40  1001.00                       Greenmount East  28929  30913        1984\n41  1002.00                   Oldtown/Middle East   9862  19172        9310\n42  1003.00                      Unassigned--Jail     NA     NA          NA\n43  1101.00                               Midtown  38797  53407       14610\n44  1102.00                               Midtown  32563  62703       30140\n45  1201.00     North Baltimore/Guilford/Homeland  56205  72349       16144\n46  1202.01       Greater Charles Village/Barclay  54000  81393       27393\n47  1202.02       Greater Charles Village/Barclay  30179  36563        6384\n48  1203.00       Greater Charles Village/Barclay  38281  55074       16793\n49  1204.00       Greater Charles Village/Barclay  39375  50034       10659\n50  1205.00                               Midtown  37500  55170       17670\n51  1206.00       Greater Charles Village/Barclay  23488  36221       12733\n52  1207.00  Medfield/Hampden/Woodberry/Remington  39730  61653       21923\n53  1301.00             Penn North/Reservoir Hill  21271  25835        4564\n54  1302.00             Penn North/Reservoir Hill  30255  57457       27202\n55  1303.00             Penn North/Reservoir Hill  32273  43380       11107\n56  1304.00             Penn North/Reservoir Hill  32500  30868       -1632\n57  1306.00  Medfield/Hampden/Woodberry/Remington  53654  91635       37981\n58  1307.00  Medfield/Hampden/Woodberry/Remington  48634  72648       24014\n59  1308.03  Medfield/Hampden/Woodberry/Remington  51548  62188       10640\n60  1308.04  Medfield/Hampden/Woodberry/Remington  48207  62054       13847\n61  1308.05           Mount Washington/Coldspring  53984  53750        -234\n62  1308.06  Medfield/Hampden/Woodberry/Remington  48750  95142       46392\n63  1401.00                               Midtown  32917  57292       24375\n64  1402.00                   Upton/Druid Heights  16213  23152        6939\n65  1403.00                   Upton/Druid Heights  21607  41913       20306\n66  1501.00       Sandtown-Winchester/Harlem Park  18097  26989        8892\n67  1502.00       Sandtown-Winchester/Harlem Park  25224  40644       15420\n68  1503.00                      Greater Rosemont  32289  35476        3187\n69  1504.00                     Greater Mondawmin  32632  33702        1070\n70  1505.00                     Greater Mondawmin  34609  33967        -642\n71  1506.00                      Greater Rosemont  29631  31250        1619\n72  1507.01                     Greater Mondawmin  38125  51944       13819\n73  1507.02                     Greater Mondawmin  48942  50150        1208\n74  1508.00                  Forest Park/Walbrook  37199  39337        2138\n75  1509.00                  Forest Park/Walbrook  37440  51250       13810\n76  1510.00                  Dorchester/Ashburton  35056  45870       10814\n77  1511.00                  Dorchester/Ashburton  45649  58843       13194\n78  1512.00                 Southern Park Heights  12386  22632       10246\n79  1513.00                 Southern Park Heights  33579  33866         287\n80  1601.00       Sandtown-Winchester/Harlem Park  19583  27969        8386\n81  1602.00       Sandtown-Winchester/Harlem Park  25417  24848        -569\n82  1603.00       Sandtown-Winchester/Harlem Park  22292  13963       -8329\n83  1604.00       Sandtown-Winchester/Harlem Park  27813  23036       -4777\n84  1605.00                      Greater Rosemont  24556  37819       13263\n85  1606.00                      Greater Rosemont  27128  38482       11354\n86  1607.00                      Greater Rosemont  29472  39959       10487\n87  1608.01                     Edmondson Village  41549  53543       11994\n88  1608.02                     Edmondson Village  29355  41932       12577\n89  1701.00                   Downtown/Seton Hill  30197  35582        5385\n90  1702.00                   Upton/Druid Heights   9412  13559        4147\n91  1703.00                   Upton/Druid Heights  17892  18912        1020\n92  1801.00 Poppleton/The Terraces/Hollins Market  18429     NA          NA\n93  1802.00 Poppleton/The Terraces/Hollins Market  16585     NA          NA\n94  1803.00 Poppleton/The Terraces/Hollins Market  37879  64125       26246\n95  1901.00                   Southwest Baltimore  22893  24559        1666\n96  1902.00                   Southwest Baltimore  43214  52738        9524\n97  1903.00                   Southwest Baltimore  17237  14269       -2968\n98  2001.00                   Southwest Baltimore  32214  38203        5989\n99  2002.00                   Southwest Baltimore  30061  27740       -2321\n100 2003.00                   Southwest Baltimore  19063  21953        2890\n101 2004.00                   Southwest Baltimore  28324  31975        3651\n102 2005.00                   Southwest Baltimore  30239  34306        4067\n103 2006.00         Allendale/Irvington/S. Hilton  29813  39076        9263\n104 2007.01         Allendale/Irvington/S. Hilton  34556  33592        -964\n105 2007.02         Allendale/Irvington/S. Hilton  27305  43734       16429\n106 2008.00         Allendale/Irvington/S. Hilton  32711  49875       17164\n107 2101.00            Washington Village/Pigtown  48857  76368       27511\n108 2102.00            Washington Village/Pigtown  46319  36661       -9658\n109 2201.00             Inner Harbor/Federal Hill  59259  80305       21046\n110 2301.00             Inner Harbor/Federal Hill  62222  98125       35903\n111 2302.00             Inner Harbor/Federal Hill  80521 107344       26823\n112 2303.00                       South Baltimore  53194  92841       39647\n113 2401.00                       South Baltimore  87619 151659       64040\n114 2402.00             Inner Harbor/Federal Hill  92500 133333       40833\n115 2403.00             Inner Harbor/Federal Hill  89211 120729       31518\n116 2404.00                       South Baltimore  72122 121685       49563\n117 2501.01       Beechfield/Ten Hills/West Hills  52098  60463        8365\n118 2501.02         Allendale/Irvington/S. Hilton  36960  53854       16894\n119 2501.03              Morrell Park/Violetville  36957  36555        -402\n120 2502.03                           Cherry Hill  29747  33902        4155\n121 2502.04                           Cherry Hill  12384  14349        1965\n122 2502.05        Westport/Mount Winans/Lakeland  47738  38779       -8959\n123 2502.06              Morrell Park/Violetville  52674  66900       14226\n124 2502.07                           Cherry Hill  23310  41518       18208\n125 2503.01        Westport/Mount Winans/Lakeland  27414  24877       -2537\n126 2503.03              Morrell Park/Violetville  42007  40230       -1777\n127 2504.01     Brooklyn/Curtis Bay/Hawkins Point  34401  32019       -2382\n128 2504.02     Brooklyn/Curtis Bay/Hawkins Point  31250  37059        5809\n129 2505.00     Brooklyn/Curtis Bay/Hawkins Point  32738  30526       -2212\n130 2506.00     Brooklyn/Curtis Bay/Hawkins Point     NA     NA          NA\n131 2601.01                     Cedonia/Frankford  46789  68983       22194\n132 2601.02                     Cedonia/Frankford  48783  60313       11530\n133 2602.01                     Cedonia/Frankford  33936  44093       10157\n134 2602.02                     Cedonia/Frankford  31241  39282        8041\n135 2602.03                     Cedonia/Frankford  32615  46250       13635\n136 2603.01                         Belair-Edison  42347  52117        9770\n137 2603.02                         Belair-Edison  42727  50000        7273\n138 2603.03                   Claremont/Armistead  30192     NA          NA\n139 2604.01                   Claremont/Armistead  31392  48661       17269\n140 2604.02                   Claremont/Armistead  42938  39000       -3938\n141 2604.03                   Claremont/Armistead  32373  35282        2909\n142 2604.04         Orangeville/East Highlandtown  35708  54375       18667\n143 2605.01         Orangeville/East Highlandtown  42298  64706       22408\n144 2606.04                          Southeastern  20710  21977        1267\n145 2606.05                          Southeastern  33545  44794       11249\n146 2607.00         Orangeville/East Highlandtown  36901  80000       43099\n147 2608.00                          Highlandtown  35200  48850       13650\n148 2609.00                          Highlandtown  68571 107438       38867\n149 2610.00           Patterson Park North & East  36343  60893       24550\n150 2611.00                          Highlandtown  83207 130769       47562\n151 2701.01                            Lauraville  57326  80875       23549\n152 2701.02                            Lauraville  54250  58020        3770\n153 2702.00                            Lauraville  60128  80788       20660\n154 2703.01                            Lauraville  54623  80655       26032\n155 2703.02                            Lauraville  68088  63929       -4159\n156 2704.01                              Hamilton  51641  64620       12979\n157 2704.02                              Hamilton  53125  84556       31431\n158 2705.01                      Harford/Echodale  59516  66604        7088\n159 2705.02                              Hamilton  53493  71694       18201\n160 2706.00                      Harford/Echodale  66914  67625         711\n161 2707.01                      Harford/Echodale  28618  37591        8973\n162 2707.02                      Harford/Echodale  39205  56963       17758\n163 2707.03                      Harford/Echodale  63521  88750       25229\n164 2708.01                            Loch Raven  51415  58995        7580\n165 2708.02                            Loch Raven  46181  55753        9572\n166 2708.03                            Loch Raven  42234  52272       10038\n167 2708.04             Chinquapin Park/Belvedere  41094  59583       18489\n168 2708.05             Chinquapin Park/Belvedere  46875  59763       12888\n169 2709.01                             Northwood  51023  47260       -3763\n170 2709.02                             Northwood  48555  58269        9714\n171 2709.03                             Northwood  54607  52083       -2524\n172 2710.01                        Greater Govans  33203  35822        2619\n173 2710.02                        Greater Govans  38500  40281        1781\n174 2711.01     North Baltimore/Guilford/Homeland  42994  55859       12865\n175 2711.02     North Baltimore/Guilford/Homeland  99300 199531      100231\n176 2712.00     North Baltimore/Guilford/Homeland 133548 195353       61805\n177 2713.00       Greater Roland Park/Poplar Hill 111250 155605       44355\n178 2714.00       Greater Roland Park/Poplar Hill  97891 151146       53255\n179 2715.01           Mount Washington/Coldspring  88929 109688       20759\n180 2715.03       Greater Roland Park/Poplar Hill  79861  62500      -17361\n181 2716.00                 Southern Park Heights  43265  34832       -8433\n182 2717.00             Pimlico/Arlington/Hilltop  29069  36484        7415\n183 2718.01             Pimlico/Arlington/Hilltop  24802  25278         476\n184 2718.02             Pimlico/Arlington/Hilltop  31392  40256        8864\n185 2719.00                        Glen-Fallstaff  52308  54866        2558\n186 2720.03               Cross-Country/Cheswolde  70942  57543      -13399\n187 2720.04               Cross-Country/Cheswolde  49567  66148       16581\n188 2720.05               Cross-Country/Cheswolde  49757  72500       22743\n189 2720.06                        Glen-Fallstaff  24859  34019        9160\n190 2720.07                        Glen-Fallstaff  35903  41728        5825\n191 2801.01                        Glen-Fallstaff  46576  47610        1034\n192 2801.02            Howard Park/West Arlington  33938  43618        9680\n193 2802.00            Howard Park/West Arlington  45047  56458       11411\n194 2803.01              Dickeyville/Franklintown  33707  47366       13659\n195 2803.02                  Forest Park/Walbrook  33864  56004       22140\n196 2804.01       Beechfield/Ten Hills/West Hills  44954  47192        2238\n197 2804.02                     Edmondson Village  52784  41179      -11605\n198 2804.03       Beechfield/Ten Hills/West Hills  51435  57582        6147\n199 2804.04         Allendale/Irvington/S. Hilton  43779  40946       -2833\n200 2805.00                   Oldtown/Middle East  11278  14304        3026\n    Diff_Pct_2020\n1     0.696634096\n2     1.231796470\n3     0.944849116\n4     0.634775930\n5     0.139710064\n6     0.580701203\n7     0.827353546\n8     0.718802755\n9     0.052480394\n10    0.537982312\n11    0.352202490\n12    2.275606622\n13    0.209944043\n14    0.367877361\n15    0.566870459\n16    0.521727995\n17    0.209570038\n18   -0.120223175\n19    1.333333333\n20    0.880704894\n21    0.316555305\n22   -0.157161319\n23    0.582190083\n24    0.419123158\n25    0.690947058\n26    0.446741724\n27   -0.199301593\n28    0.317855272\n29    0.977126554\n30    0.354750647\n31    0.453956756\n32    0.223837037\n33    0.449502556\n34    0.158361646\n35    0.197770291\n36    0.374642936\n37    0.132967435\n38   -0.119048290\n39    0.024559194\n40    0.068581700\n41    0.944027581\n42             NA\n43    0.376575508\n44    0.925590394\n45    0.287234232\n46    0.507277778\n47    0.211537824\n48    0.438677151\n49    0.270704762\n50    0.471200000\n51    0.542106608\n52    0.551799648\n53    0.214564430\n54    0.899091059\n55    0.344157655\n56   -0.050215385\n57    0.707887576\n58    0.493769791\n59    0.206409560\n60    0.287240442\n61   -0.004334618\n62    0.951630769\n63    0.740498830\n64    0.427989885\n65    0.939788032\n66    0.491352158\n67    0.611322550\n68    0.098702344\n69    0.032789899\n70   -0.018550088\n71    0.054638723\n72    0.362465574\n73    0.024682277\n74    0.057474663\n75    0.368856838\n76    0.308477864\n77    0.289031523\n78    0.827224285\n79    0.008547009\n80    0.428228566\n81   -0.022386592\n82   -0.373631796\n83   -0.171754216\n84    0.540112396\n85    0.418534356\n86    0.355829262\n87    0.288671207\n88    0.428444899\n89    0.178328973\n90    0.440607735\n91    0.057008719\n92             NA\n93             NA\n94    0.692890520\n95    0.072773337\n96    0.220391540\n97   -0.172187736\n98    0.185912957\n99   -0.077209674\n100   0.151602581\n101   0.128901285\n102   0.134495188\n103   0.310703384\n104  -0.027896747\n105   0.601684673\n106   0.524716456\n107   0.563092290\n108  -0.208510546\n109   0.355152804\n110   0.577014561\n111   0.333118069\n112   0.745328420\n113   0.730891702\n114   0.441437838\n115   0.353297239\n116   0.687210560\n117   0.160562786\n118   0.457088745\n119  -0.010877506\n120   0.139677951\n121   0.158672481\n122  -0.187670200\n123   0.270076318\n124   0.781123981\n125  -0.092543956\n126  -0.042302473\n127  -0.069242173\n128   0.185888000\n129  -0.067566742\n130            NA\n131   0.474342260\n132   0.236352828\n133   0.299298680\n134   0.257386127\n135   0.418059175\n136   0.230712919\n137   0.170220235\n138            NA\n139   0.550108308\n140  -0.091713634\n141   0.089858833\n142   0.522768007\n143   0.529765001\n144   0.061178175\n145   0.335340587\n146   1.167962928\n147   0.387784091\n148   0.566813959\n149   0.675508351\n150   0.571610562\n151   0.410790915\n152   0.069493088\n153   0.343600319\n154   0.476575801\n155  -0.061082716\n156   0.251331307\n157   0.591642353\n158   0.119094025\n159   0.340250126\n160   0.010625579\n161   0.313543923\n162   0.452952430\n163   0.397175737\n164   0.147427793\n165   0.207271389\n166   0.237675806\n167   0.449919696\n168   0.274944000\n169  -0.073751053\n170   0.200061786\n171  -0.046221180\n172   0.078878415\n173   0.046259740\n174   0.299227799\n175   1.009375629\n176   0.462792404\n177   0.398696629\n178   0.544023455\n179   0.233433413\n180  -0.217390215\n181  -0.194915058\n182   0.255082734\n183   0.019192001\n184   0.282364934\n185   0.048902654\n186  -0.188872600\n187   0.334516916\n188   0.457081416\n189   0.368478217\n190   0.162242710\n191   0.022200275\n192   0.285226000\n193   0.253313206\n194   0.405227401\n195   0.653791637\n196   0.049784224\n197  -0.219858290\n198   0.119510061\n199  -0.064711391\n200   0.268309984\n\n\nLook at Diff_Pct_2020. Do those numbers look like we expect them to? No. They’re a decimal expressed as a percentage. So let’s fix that by multiplying by 100. We’re also rounding the result to two digits from nine so it looks cleaner\n\nbaltcity_income %>%\n  select(Census, Neighborhood, x2010, x2020) %>%\n    mutate(Diff_Income = (x2020-x2010)) %>% \n    mutate(Diff_Pct_2020 = round((x2020-x2010)/x2010*100,2))\n\n     Census                          Neighborhood  x2010  x2020 Diff_Income\n1    101.00                                Canton  75938 128839       52901\n2    102.00           Patterson Park North & East  58409 130357       71948\n3    103.00                                Canton  77841 151389       73548\n4    104.00                                Canton  70313 114946       44633\n5    105.00                           Fells Point  86157  98194       12037\n6    201.00                           Fells Point  60439  95536       35097\n7    202.00                           Fells Point  47131  86125       38994\n8    203.00                           Fells Point  59236 101815       42579\n9    301.00              Harbor East/Little Italy  21932  23083        1151\n10   302.00              Harbor East/Little Italy  63885  98254       34369\n11   401.00                   Downtown/Seton Hill  45948  62131       16183\n12   402.00                   Downtown/Seton Hill  13229  43333       30104\n13   601.00           Patterson Park North & East  46822  56652        9830\n14   602.00           Patterson Park North & East  51403  70313       18910\n15   603.00           Patterson Park North & East  53813  84318       30505\n16   604.00                   Oldtown/Middle East  35139  53472       18333\n17   701.00                      Madison/East End  37701  45602        7901\n18   702.00                      Madison/East End  35667  31379       -4288\n19   703.00                      Madison/East End  15000  35000       20000\n20   704.00                   Oldtown/Middle East  14527  27321       12794\n21   801.01                         Belair-Edison  49688  65417       15729\n22   801.02                         Belair-Edison  35594  30000       -5594\n23   802.00                         Clifton-Berea  24200  38289       14089\n24   803.01                         Clifton-Berea  32024  45446       13422\n25   803.02                         Clifton-Berea  24423  41298       16875\n26   804.00                         Clifton-Berea  23893  34567       10674\n27   805.00                         Clifton-Berea  35223  28203       -7020\n28   806.00                       Greenmount East  26726  35221        8495\n29   807.00                       Greenmount East  20679  40885       20206\n30   808.00                   Oldtown/Middle East  35532  48137       12605\n31   901.00                        Greater Govans  36444  52988       16544\n32   902.00                             Northwood  67500  82609       15109\n33   903.00                         The Waverlies  43824  63523       19699\n34   904.00                         The Waverlies  26490  30685        4195\n35   905.00                         The Waverlies  36507  43727        7220\n36   906.00                     Midway/Coldstream  36408  50048       13640\n37   907.00                     Midway/Coldstream  27330  30964        3634\n38   908.00                     Midway/Coldstream  35473  31250       -4223\n39   909.00                       Greenmount East  19056  19524         468\n40  1001.00                       Greenmount East  28929  30913        1984\n41  1002.00                   Oldtown/Middle East   9862  19172        9310\n42  1003.00                      Unassigned--Jail     NA     NA          NA\n43  1101.00                               Midtown  38797  53407       14610\n44  1102.00                               Midtown  32563  62703       30140\n45  1201.00     North Baltimore/Guilford/Homeland  56205  72349       16144\n46  1202.01       Greater Charles Village/Barclay  54000  81393       27393\n47  1202.02       Greater Charles Village/Barclay  30179  36563        6384\n48  1203.00       Greater Charles Village/Barclay  38281  55074       16793\n49  1204.00       Greater Charles Village/Barclay  39375  50034       10659\n50  1205.00                               Midtown  37500  55170       17670\n51  1206.00       Greater Charles Village/Barclay  23488  36221       12733\n52  1207.00  Medfield/Hampden/Woodberry/Remington  39730  61653       21923\n53  1301.00             Penn North/Reservoir Hill  21271  25835        4564\n54  1302.00             Penn North/Reservoir Hill  30255  57457       27202\n55  1303.00             Penn North/Reservoir Hill  32273  43380       11107\n56  1304.00             Penn North/Reservoir Hill  32500  30868       -1632\n57  1306.00  Medfield/Hampden/Woodberry/Remington  53654  91635       37981\n58  1307.00  Medfield/Hampden/Woodberry/Remington  48634  72648       24014\n59  1308.03  Medfield/Hampden/Woodberry/Remington  51548  62188       10640\n60  1308.04  Medfield/Hampden/Woodberry/Remington  48207  62054       13847\n61  1308.05           Mount Washington/Coldspring  53984  53750        -234\n62  1308.06  Medfield/Hampden/Woodberry/Remington  48750  95142       46392\n63  1401.00                               Midtown  32917  57292       24375\n64  1402.00                   Upton/Druid Heights  16213  23152        6939\n65  1403.00                   Upton/Druid Heights  21607  41913       20306\n66  1501.00       Sandtown-Winchester/Harlem Park  18097  26989        8892\n67  1502.00       Sandtown-Winchester/Harlem Park  25224  40644       15420\n68  1503.00                      Greater Rosemont  32289  35476        3187\n69  1504.00                     Greater Mondawmin  32632  33702        1070\n70  1505.00                     Greater Mondawmin  34609  33967        -642\n71  1506.00                      Greater Rosemont  29631  31250        1619\n72  1507.01                     Greater Mondawmin  38125  51944       13819\n73  1507.02                     Greater Mondawmin  48942  50150        1208\n74  1508.00                  Forest Park/Walbrook  37199  39337        2138\n75  1509.00                  Forest Park/Walbrook  37440  51250       13810\n76  1510.00                  Dorchester/Ashburton  35056  45870       10814\n77  1511.00                  Dorchester/Ashburton  45649  58843       13194\n78  1512.00                 Southern Park Heights  12386  22632       10246\n79  1513.00                 Southern Park Heights  33579  33866         287\n80  1601.00       Sandtown-Winchester/Harlem Park  19583  27969        8386\n81  1602.00       Sandtown-Winchester/Harlem Park  25417  24848        -569\n82  1603.00       Sandtown-Winchester/Harlem Park  22292  13963       -8329\n83  1604.00       Sandtown-Winchester/Harlem Park  27813  23036       -4777\n84  1605.00                      Greater Rosemont  24556  37819       13263\n85  1606.00                      Greater Rosemont  27128  38482       11354\n86  1607.00                      Greater Rosemont  29472  39959       10487\n87  1608.01                     Edmondson Village  41549  53543       11994\n88  1608.02                     Edmondson Village  29355  41932       12577\n89  1701.00                   Downtown/Seton Hill  30197  35582        5385\n90  1702.00                   Upton/Druid Heights   9412  13559        4147\n91  1703.00                   Upton/Druid Heights  17892  18912        1020\n92  1801.00 Poppleton/The Terraces/Hollins Market  18429     NA          NA\n93  1802.00 Poppleton/The Terraces/Hollins Market  16585     NA          NA\n94  1803.00 Poppleton/The Terraces/Hollins Market  37879  64125       26246\n95  1901.00                   Southwest Baltimore  22893  24559        1666\n96  1902.00                   Southwest Baltimore  43214  52738        9524\n97  1903.00                   Southwest Baltimore  17237  14269       -2968\n98  2001.00                   Southwest Baltimore  32214  38203        5989\n99  2002.00                   Southwest Baltimore  30061  27740       -2321\n100 2003.00                   Southwest Baltimore  19063  21953        2890\n101 2004.00                   Southwest Baltimore  28324  31975        3651\n102 2005.00                   Southwest Baltimore  30239  34306        4067\n103 2006.00         Allendale/Irvington/S. Hilton  29813  39076        9263\n104 2007.01         Allendale/Irvington/S. Hilton  34556  33592        -964\n105 2007.02         Allendale/Irvington/S. Hilton  27305  43734       16429\n106 2008.00         Allendale/Irvington/S. Hilton  32711  49875       17164\n107 2101.00            Washington Village/Pigtown  48857  76368       27511\n108 2102.00            Washington Village/Pigtown  46319  36661       -9658\n109 2201.00             Inner Harbor/Federal Hill  59259  80305       21046\n110 2301.00             Inner Harbor/Federal Hill  62222  98125       35903\n111 2302.00             Inner Harbor/Federal Hill  80521 107344       26823\n112 2303.00                       South Baltimore  53194  92841       39647\n113 2401.00                       South Baltimore  87619 151659       64040\n114 2402.00             Inner Harbor/Federal Hill  92500 133333       40833\n115 2403.00             Inner Harbor/Federal Hill  89211 120729       31518\n116 2404.00                       South Baltimore  72122 121685       49563\n117 2501.01       Beechfield/Ten Hills/West Hills  52098  60463        8365\n118 2501.02         Allendale/Irvington/S. Hilton  36960  53854       16894\n119 2501.03              Morrell Park/Violetville  36957  36555        -402\n120 2502.03                           Cherry Hill  29747  33902        4155\n121 2502.04                           Cherry Hill  12384  14349        1965\n122 2502.05        Westport/Mount Winans/Lakeland  47738  38779       -8959\n123 2502.06              Morrell Park/Violetville  52674  66900       14226\n124 2502.07                           Cherry Hill  23310  41518       18208\n125 2503.01        Westport/Mount Winans/Lakeland  27414  24877       -2537\n126 2503.03              Morrell Park/Violetville  42007  40230       -1777\n127 2504.01     Brooklyn/Curtis Bay/Hawkins Point  34401  32019       -2382\n128 2504.02     Brooklyn/Curtis Bay/Hawkins Point  31250  37059        5809\n129 2505.00     Brooklyn/Curtis Bay/Hawkins Point  32738  30526       -2212\n130 2506.00     Brooklyn/Curtis Bay/Hawkins Point     NA     NA          NA\n131 2601.01                     Cedonia/Frankford  46789  68983       22194\n132 2601.02                     Cedonia/Frankford  48783  60313       11530\n133 2602.01                     Cedonia/Frankford  33936  44093       10157\n134 2602.02                     Cedonia/Frankford  31241  39282        8041\n135 2602.03                     Cedonia/Frankford  32615  46250       13635\n136 2603.01                         Belair-Edison  42347  52117        9770\n137 2603.02                         Belair-Edison  42727  50000        7273\n138 2603.03                   Claremont/Armistead  30192     NA          NA\n139 2604.01                   Claremont/Armistead  31392  48661       17269\n140 2604.02                   Claremont/Armistead  42938  39000       -3938\n141 2604.03                   Claremont/Armistead  32373  35282        2909\n142 2604.04         Orangeville/East Highlandtown  35708  54375       18667\n143 2605.01         Orangeville/East Highlandtown  42298  64706       22408\n144 2606.04                          Southeastern  20710  21977        1267\n145 2606.05                          Southeastern  33545  44794       11249\n146 2607.00         Orangeville/East Highlandtown  36901  80000       43099\n147 2608.00                          Highlandtown  35200  48850       13650\n148 2609.00                          Highlandtown  68571 107438       38867\n149 2610.00           Patterson Park North & East  36343  60893       24550\n150 2611.00                          Highlandtown  83207 130769       47562\n151 2701.01                            Lauraville  57326  80875       23549\n152 2701.02                            Lauraville  54250  58020        3770\n153 2702.00                            Lauraville  60128  80788       20660\n154 2703.01                            Lauraville  54623  80655       26032\n155 2703.02                            Lauraville  68088  63929       -4159\n156 2704.01                              Hamilton  51641  64620       12979\n157 2704.02                              Hamilton  53125  84556       31431\n158 2705.01                      Harford/Echodale  59516  66604        7088\n159 2705.02                              Hamilton  53493  71694       18201\n160 2706.00                      Harford/Echodale  66914  67625         711\n161 2707.01                      Harford/Echodale  28618  37591        8973\n162 2707.02                      Harford/Echodale  39205  56963       17758\n163 2707.03                      Harford/Echodale  63521  88750       25229\n164 2708.01                            Loch Raven  51415  58995        7580\n165 2708.02                            Loch Raven  46181  55753        9572\n166 2708.03                            Loch Raven  42234  52272       10038\n167 2708.04             Chinquapin Park/Belvedere  41094  59583       18489\n168 2708.05             Chinquapin Park/Belvedere  46875  59763       12888\n169 2709.01                             Northwood  51023  47260       -3763\n170 2709.02                             Northwood  48555  58269        9714\n171 2709.03                             Northwood  54607  52083       -2524\n172 2710.01                        Greater Govans  33203  35822        2619\n173 2710.02                        Greater Govans  38500  40281        1781\n174 2711.01     North Baltimore/Guilford/Homeland  42994  55859       12865\n175 2711.02     North Baltimore/Guilford/Homeland  99300 199531      100231\n176 2712.00     North Baltimore/Guilford/Homeland 133548 195353       61805\n177 2713.00       Greater Roland Park/Poplar Hill 111250 155605       44355\n178 2714.00       Greater Roland Park/Poplar Hill  97891 151146       53255\n179 2715.01           Mount Washington/Coldspring  88929 109688       20759\n180 2715.03       Greater Roland Park/Poplar Hill  79861  62500      -17361\n181 2716.00                 Southern Park Heights  43265  34832       -8433\n182 2717.00             Pimlico/Arlington/Hilltop  29069  36484        7415\n183 2718.01             Pimlico/Arlington/Hilltop  24802  25278         476\n184 2718.02             Pimlico/Arlington/Hilltop  31392  40256        8864\n185 2719.00                        Glen-Fallstaff  52308  54866        2558\n186 2720.03               Cross-Country/Cheswolde  70942  57543      -13399\n187 2720.04               Cross-Country/Cheswolde  49567  66148       16581\n188 2720.05               Cross-Country/Cheswolde  49757  72500       22743\n189 2720.06                        Glen-Fallstaff  24859  34019        9160\n190 2720.07                        Glen-Fallstaff  35903  41728        5825\n191 2801.01                        Glen-Fallstaff  46576  47610        1034\n192 2801.02            Howard Park/West Arlington  33938  43618        9680\n193 2802.00            Howard Park/West Arlington  45047  56458       11411\n194 2803.01              Dickeyville/Franklintown  33707  47366       13659\n195 2803.02                  Forest Park/Walbrook  33864  56004       22140\n196 2804.01       Beechfield/Ten Hills/West Hills  44954  47192        2238\n197 2804.02                     Edmondson Village  52784  41179      -11605\n198 2804.03       Beechfield/Ten Hills/West Hills  51435  57582        6147\n199 2804.04         Allendale/Irvington/S. Hilton  43779  40946       -2833\n200 2805.00                   Oldtown/Middle East  11278  14304        3026\n    Diff_Pct_2020\n1           69.66\n2          123.18\n3           94.48\n4           63.48\n5           13.97\n6           58.07\n7           82.74\n8           71.88\n9            5.25\n10          53.80\n11          35.22\n12         227.56\n13          20.99\n14          36.79\n15          56.69\n16          52.17\n17          20.96\n18         -12.02\n19         133.33\n20          88.07\n21          31.66\n22         -15.72\n23          58.22\n24          41.91\n25          69.09\n26          44.67\n27         -19.93\n28          31.79\n29          97.71\n30          35.48\n31          45.40\n32          22.38\n33          44.95\n34          15.84\n35          19.78\n36          37.46\n37          13.30\n38         -11.90\n39           2.46\n40           6.86\n41          94.40\n42             NA\n43          37.66\n44          92.56\n45          28.72\n46          50.73\n47          21.15\n48          43.87\n49          27.07\n50          47.12\n51          54.21\n52          55.18\n53          21.46\n54          89.91\n55          34.42\n56          -5.02\n57          70.79\n58          49.38\n59          20.64\n60          28.72\n61          -0.43\n62          95.16\n63          74.05\n64          42.80\n65          93.98\n66          49.14\n67          61.13\n68           9.87\n69           3.28\n70          -1.86\n71           5.46\n72          36.25\n73           2.47\n74           5.75\n75          36.89\n76          30.85\n77          28.90\n78          82.72\n79           0.85\n80          42.82\n81          -2.24\n82         -37.36\n83         -17.18\n84          54.01\n85          41.85\n86          35.58\n87          28.87\n88          42.84\n89          17.83\n90          44.06\n91           5.70\n92             NA\n93             NA\n94          69.29\n95           7.28\n96          22.04\n97         -17.22\n98          18.59\n99          -7.72\n100         15.16\n101         12.89\n102         13.45\n103         31.07\n104         -2.79\n105         60.17\n106         52.47\n107         56.31\n108        -20.85\n109         35.52\n110         57.70\n111         33.31\n112         74.53\n113         73.09\n114         44.14\n115         35.33\n116         68.72\n117         16.06\n118         45.71\n119         -1.09\n120         13.97\n121         15.87\n122        -18.77\n123         27.01\n124         78.11\n125         -9.25\n126         -4.23\n127         -6.92\n128         18.59\n129         -6.76\n130            NA\n131         47.43\n132         23.64\n133         29.93\n134         25.74\n135         41.81\n136         23.07\n137         17.02\n138            NA\n139         55.01\n140         -9.17\n141          8.99\n142         52.28\n143         52.98\n144          6.12\n145         33.53\n146        116.80\n147         38.78\n148         56.68\n149         67.55\n150         57.16\n151         41.08\n152          6.95\n153         34.36\n154         47.66\n155         -6.11\n156         25.13\n157         59.16\n158         11.91\n159         34.03\n160          1.06\n161         31.35\n162         45.30\n163         39.72\n164         14.74\n165         20.73\n166         23.77\n167         44.99\n168         27.49\n169         -7.38\n170         20.01\n171         -4.62\n172          7.89\n173          4.63\n174         29.92\n175        100.94\n176         46.28\n177         39.87\n178         54.40\n179         23.34\n180        -21.74\n181        -19.49\n182         25.51\n183          1.92\n184         28.24\n185          4.89\n186        -18.89\n187         33.45\n188         45.71\n189         36.85\n190         16.22\n191          2.22\n192         28.52\n193         25.33\n194         40.52\n195         65.38\n196          4.98\n197        -21.99\n198         11.95\n199         -6.47\n200         26.83\n\n\nNow, let’s fix the ordering with arrange and sort descending on the percentage change column.\n\nbaltcity_income %>%\n  select(Census, Neighborhood, x2010, x2020) %>%\n    mutate(Diff_Income = (x2020-x2010)) %>% \n    mutate(Diff_Pct_2020 = round((x2020-x2010)/x2010*100,2)) %>% \n    arrange(desc(Diff_Pct_2020))\n\n     Census                          Neighborhood  x2010  x2020 Diff_Income\n1    402.00                   Downtown/Seton Hill  13229  43333       30104\n2    703.00                      Madison/East End  15000  35000       20000\n3    102.00           Patterson Park North & East  58409 130357       71948\n4   2607.00         Orangeville/East Highlandtown  36901  80000       43099\n5   2711.02     North Baltimore/Guilford/Homeland  99300 199531      100231\n6    807.00                       Greenmount East  20679  40885       20206\n7   1308.06  Medfield/Hampden/Woodberry/Remington  48750  95142       46392\n8    103.00                                Canton  77841 151389       73548\n9   1002.00                   Oldtown/Middle East   9862  19172        9310\n10  1403.00                   Upton/Druid Heights  21607  41913       20306\n11  1102.00                               Midtown  32563  62703       30140\n12  1302.00             Penn North/Reservoir Hill  30255  57457       27202\n13   704.00                   Oldtown/Middle East  14527  27321       12794\n14   202.00                           Fells Point  47131  86125       38994\n15  1512.00                 Southern Park Heights  12386  22632       10246\n16  2502.07                           Cherry Hill  23310  41518       18208\n17  2303.00                       South Baltimore  53194  92841       39647\n18  1401.00                               Midtown  32917  57292       24375\n19  2401.00                       South Baltimore  87619 151659       64040\n20   203.00                           Fells Point  59236 101815       42579\n21  1306.00  Medfield/Hampden/Woodberry/Remington  53654  91635       37981\n22   101.00                                Canton  75938 128839       52901\n23  1803.00 Poppleton/The Terraces/Hollins Market  37879  64125       26246\n24   803.02                         Clifton-Berea  24423  41298       16875\n25  2404.00                       South Baltimore  72122 121685       49563\n26  2610.00           Patterson Park North & East  36343  60893       24550\n27  2803.02                  Forest Park/Walbrook  33864  56004       22140\n28   104.00                                Canton  70313 114946       44633\n29  1502.00       Sandtown-Winchester/Harlem Park  25224  40644       15420\n30  2007.02         Allendale/Irvington/S. Hilton  27305  43734       16429\n31  2704.02                              Hamilton  53125  84556       31431\n32   802.00                         Clifton-Berea  24200  38289       14089\n33   201.00                           Fells Point  60439  95536       35097\n34  2301.00             Inner Harbor/Federal Hill  62222  98125       35903\n35  2611.00                          Highlandtown  83207 130769       47562\n36   603.00           Patterson Park North & East  53813  84318       30505\n37  2609.00                          Highlandtown  68571 107438       38867\n38  2101.00            Washington Village/Pigtown  48857  76368       27511\n39  1207.00  Medfield/Hampden/Woodberry/Remington  39730  61653       21923\n40  2604.01                   Claremont/Armistead  31392  48661       17269\n41  2714.00       Greater Roland Park/Poplar Hill  97891 151146       53255\n42  1206.00       Greater Charles Village/Barclay  23488  36221       12733\n43  1605.00                      Greater Rosemont  24556  37819       13263\n44   302.00              Harbor East/Little Italy  63885  98254       34369\n45  2605.01         Orangeville/East Highlandtown  42298  64706       22408\n46  2008.00         Allendale/Irvington/S. Hilton  32711  49875       17164\n47  2604.04         Orangeville/East Highlandtown  35708  54375       18667\n48   604.00                   Oldtown/Middle East  35139  53472       18333\n49  1202.01       Greater Charles Village/Barclay  54000  81393       27393\n50  1307.00  Medfield/Hampden/Woodberry/Remington  48634  72648       24014\n51  1501.00       Sandtown-Winchester/Harlem Park  18097  26989        8892\n52  2703.01                            Lauraville  54623  80655       26032\n53  2601.01                     Cedonia/Frankford  46789  68983       22194\n54  1205.00                               Midtown  37500  55170       17670\n55  2712.00     North Baltimore/Guilford/Homeland 133548 195353       61805\n56  2501.02         Allendale/Irvington/S. Hilton  36960  53854       16894\n57  2720.05               Cross-Country/Cheswolde  49757  72500       22743\n58   901.00                        Greater Govans  36444  52988       16544\n59  2707.02                      Harford/Echodale  39205  56963       17758\n60  2708.04             Chinquapin Park/Belvedere  41094  59583       18489\n61   903.00                         The Waverlies  43824  63523       19699\n62   804.00                         Clifton-Berea  23893  34567       10674\n63  2402.00             Inner Harbor/Federal Hill  92500 133333       40833\n64  1702.00                   Upton/Druid Heights   9412  13559        4147\n65  1203.00       Greater Charles Village/Barclay  38281  55074       16793\n66  1608.02                     Edmondson Village  29355  41932       12577\n67  1601.00       Sandtown-Winchester/Harlem Park  19583  27969        8386\n68  1402.00                   Upton/Druid Heights  16213  23152        6939\n69   803.01                         Clifton-Berea  32024  45446       13422\n70  1606.00                      Greater Rosemont  27128  38482       11354\n71  2602.03                     Cedonia/Frankford  32615  46250       13635\n72  2701.01                            Lauraville  57326  80875       23549\n73  2803.01              Dickeyville/Franklintown  33707  47366       13659\n74  2713.00       Greater Roland Park/Poplar Hill 111250 155605       44355\n75  2707.03                      Harford/Echodale  63521  88750       25229\n76  2608.00                          Highlandtown  35200  48850       13650\n77  1101.00                               Midtown  38797  53407       14610\n78   906.00                     Midway/Coldstream  36408  50048       13640\n79  1509.00                  Forest Park/Walbrook  37440  51250       13810\n80  2720.06                        Glen-Fallstaff  24859  34019        9160\n81   602.00           Patterson Park North & East  51403  70313       18910\n82  1507.01                     Greater Mondawmin  38125  51944       13819\n83  1607.00                      Greater Rosemont  29472  39959       10487\n84  2201.00             Inner Harbor/Federal Hill  59259  80305       21046\n85   808.00                   Oldtown/Middle East  35532  48137       12605\n86  2403.00             Inner Harbor/Federal Hill  89211 120729       31518\n87   401.00                   Downtown/Seton Hill  45948  62131       16183\n88  1303.00             Penn North/Reservoir Hill  32273  43380       11107\n89  2702.00                            Lauraville  60128  80788       20660\n90  2705.02                              Hamilton  53493  71694       18201\n91  2606.05                          Southeastern  33545  44794       11249\n92  2720.04               Cross-Country/Cheswolde  49567  66148       16581\n93  2302.00             Inner Harbor/Federal Hill  80521 107344       26823\n94   806.00                       Greenmount East  26726  35221        8495\n95   801.01                         Belair-Edison  49688  65417       15729\n96  2707.01                      Harford/Echodale  28618  37591        8973\n97  2006.00         Allendale/Irvington/S. Hilton  29813  39076        9263\n98  1510.00                  Dorchester/Ashburton  35056  45870       10814\n99  2602.01                     Cedonia/Frankford  33936  44093       10157\n100 2711.01     North Baltimore/Guilford/Homeland  42994  55859       12865\n101 1511.00                  Dorchester/Ashburton  45649  58843       13194\n102 1608.01                     Edmondson Village  41549  53543       11994\n103 1201.00     North Baltimore/Guilford/Homeland  56205  72349       16144\n104 1308.04  Medfield/Hampden/Woodberry/Remington  48207  62054       13847\n105 2801.02            Howard Park/West Arlington  33938  43618        9680\n106 2718.02             Pimlico/Arlington/Hilltop  31392  40256        8864\n107 2708.05             Chinquapin Park/Belvedere  46875  59763       12888\n108 1204.00       Greater Charles Village/Barclay  39375  50034       10659\n109 2502.06              Morrell Park/Violetville  52674  66900       14226\n110 2805.00                   Oldtown/Middle East  11278  14304        3026\n111 2602.02                     Cedonia/Frankford  31241  39282        8041\n112 2717.00             Pimlico/Arlington/Hilltop  29069  36484        7415\n113 2802.00            Howard Park/West Arlington  45047  56458       11411\n114 2704.01                              Hamilton  51641  64620       12979\n115 2708.03                            Loch Raven  42234  52272       10038\n116 2601.02                     Cedonia/Frankford  48783  60313       11530\n117 2715.01           Mount Washington/Coldspring  88929 109688       20759\n118 2603.01                         Belair-Edison  42347  52117        9770\n119  902.00                             Northwood  67500  82609       15109\n120 1902.00                   Southwest Baltimore  43214  52738        9524\n121 1301.00             Penn North/Reservoir Hill  21271  25835        4564\n122 1202.02       Greater Charles Village/Barclay  30179  36563        6384\n123  601.00           Patterson Park North & East  46822  56652        9830\n124  701.00                      Madison/East End  37701  45602        7901\n125 2708.02                            Loch Raven  46181  55753        9572\n126 1308.03  Medfield/Hampden/Woodberry/Remington  51548  62188       10640\n127 2709.02                             Northwood  48555  58269        9714\n128  905.00                         The Waverlies  36507  43727        7220\n129 2001.00                   Southwest Baltimore  32214  38203        5989\n130 2504.02     Brooklyn/Curtis Bay/Hawkins Point  31250  37059        5809\n131 1701.00                   Downtown/Seton Hill  30197  35582        5385\n132 2603.02                         Belair-Edison  42727  50000        7273\n133 2720.07                        Glen-Fallstaff  35903  41728        5825\n134 2501.01       Beechfield/Ten Hills/West Hills  52098  60463        8365\n135 2502.04                           Cherry Hill  12384  14349        1965\n136  904.00                         The Waverlies  26490  30685        4195\n137 2003.00                   Southwest Baltimore  19063  21953        2890\n138 2708.01                            Loch Raven  51415  58995        7580\n139  105.00                           Fells Point  86157  98194       12037\n140 2502.03                           Cherry Hill  29747  33902        4155\n141 2005.00                   Southwest Baltimore  30239  34306        4067\n142  907.00                     Midway/Coldstream  27330  30964        3634\n143 2004.00                   Southwest Baltimore  28324  31975        3651\n144 2804.03       Beechfield/Ten Hills/West Hills  51435  57582        6147\n145 2705.01                      Harford/Echodale  59516  66604        7088\n146 1503.00                      Greater Rosemont  32289  35476        3187\n147 2604.03                   Claremont/Armistead  32373  35282        2909\n148 2710.01                        Greater Govans  33203  35822        2619\n149 1901.00                   Southwest Baltimore  22893  24559        1666\n150 2701.02                            Lauraville  54250  58020        3770\n151 1001.00                       Greenmount East  28929  30913        1984\n152 2606.04                          Southeastern  20710  21977        1267\n153 1508.00                  Forest Park/Walbrook  37199  39337        2138\n154 1703.00                   Upton/Druid Heights  17892  18912        1020\n155 1506.00                      Greater Rosemont  29631  31250        1619\n156  301.00              Harbor East/Little Italy  21932  23083        1151\n157 2804.01       Beechfield/Ten Hills/West Hills  44954  47192        2238\n158 2719.00                        Glen-Fallstaff  52308  54866        2558\n159 2710.02                        Greater Govans  38500  40281        1781\n160 1504.00                     Greater Mondawmin  32632  33702        1070\n161 1507.02                     Greater Mondawmin  48942  50150        1208\n162  909.00                       Greenmount East  19056  19524         468\n163 2801.01                        Glen-Fallstaff  46576  47610        1034\n164 2718.01             Pimlico/Arlington/Hilltop  24802  25278         476\n165 2706.00                      Harford/Echodale  66914  67625         711\n166 1513.00                 Southern Park Heights  33579  33866         287\n167 1308.05           Mount Washington/Coldspring  53984  53750        -234\n168 2501.03              Morrell Park/Violetville  36957  36555        -402\n169 1505.00                     Greater Mondawmin  34609  33967        -642\n170 1602.00       Sandtown-Winchester/Harlem Park  25417  24848        -569\n171 2007.01         Allendale/Irvington/S. Hilton  34556  33592        -964\n172 2503.03              Morrell Park/Violetville  42007  40230       -1777\n173 2709.03                             Northwood  54607  52083       -2524\n174 1304.00             Penn North/Reservoir Hill  32500  30868       -1632\n175 2703.02                            Lauraville  68088  63929       -4159\n176 2804.04         Allendale/Irvington/S. Hilton  43779  40946       -2833\n177 2505.00     Brooklyn/Curtis Bay/Hawkins Point  32738  30526       -2212\n178 2504.01     Brooklyn/Curtis Bay/Hawkins Point  34401  32019       -2382\n179 2709.01                             Northwood  51023  47260       -3763\n180 2002.00                   Southwest Baltimore  30061  27740       -2321\n181 2604.02                   Claremont/Armistead  42938  39000       -3938\n182 2503.01        Westport/Mount Winans/Lakeland  27414  24877       -2537\n183  908.00                     Midway/Coldstream  35473  31250       -4223\n184  702.00                      Madison/East End  35667  31379       -4288\n185  801.02                         Belair-Edison  35594  30000       -5594\n186 1604.00       Sandtown-Winchester/Harlem Park  27813  23036       -4777\n187 1903.00                   Southwest Baltimore  17237  14269       -2968\n188 2502.05        Westport/Mount Winans/Lakeland  47738  38779       -8959\n189 2720.03               Cross-Country/Cheswolde  70942  57543      -13399\n190 2716.00                 Southern Park Heights  43265  34832       -8433\n191  805.00                         Clifton-Berea  35223  28203       -7020\n192 2102.00            Washington Village/Pigtown  46319  36661       -9658\n193 2715.03       Greater Roland Park/Poplar Hill  79861  62500      -17361\n194 2804.02                     Edmondson Village  52784  41179      -11605\n195 1603.00       Sandtown-Winchester/Harlem Park  22292  13963       -8329\n196 1003.00                      Unassigned--Jail     NA     NA          NA\n197 1801.00 Poppleton/The Terraces/Hollins Market  18429     NA          NA\n198 1802.00 Poppleton/The Terraces/Hollins Market  16585     NA          NA\n199 2506.00     Brooklyn/Curtis Bay/Hawkins Point     NA     NA          NA\n200 2603.03                   Claremont/Armistead  30192     NA          NA\n    Diff_Pct_2020\n1          227.56\n2          133.33\n3          123.18\n4          116.80\n5          100.94\n6           97.71\n7           95.16\n8           94.48\n9           94.40\n10          93.98\n11          92.56\n12          89.91\n13          88.07\n14          82.74\n15          82.72\n16          78.11\n17          74.53\n18          74.05\n19          73.09\n20          71.88\n21          70.79\n22          69.66\n23          69.29\n24          69.09\n25          68.72\n26          67.55\n27          65.38\n28          63.48\n29          61.13\n30          60.17\n31          59.16\n32          58.22\n33          58.07\n34          57.70\n35          57.16\n36          56.69\n37          56.68\n38          56.31\n39          55.18\n40          55.01\n41          54.40\n42          54.21\n43          54.01\n44          53.80\n45          52.98\n46          52.47\n47          52.28\n48          52.17\n49          50.73\n50          49.38\n51          49.14\n52          47.66\n53          47.43\n54          47.12\n55          46.28\n56          45.71\n57          45.71\n58          45.40\n59          45.30\n60          44.99\n61          44.95\n62          44.67\n63          44.14\n64          44.06\n65          43.87\n66          42.84\n67          42.82\n68          42.80\n69          41.91\n70          41.85\n71          41.81\n72          41.08\n73          40.52\n74          39.87\n75          39.72\n76          38.78\n77          37.66\n78          37.46\n79          36.89\n80          36.85\n81          36.79\n82          36.25\n83          35.58\n84          35.52\n85          35.48\n86          35.33\n87          35.22\n88          34.42\n89          34.36\n90          34.03\n91          33.53\n92          33.45\n93          33.31\n94          31.79\n95          31.66\n96          31.35\n97          31.07\n98          30.85\n99          29.93\n100         29.92\n101         28.90\n102         28.87\n103         28.72\n104         28.72\n105         28.52\n106         28.24\n107         27.49\n108         27.07\n109         27.01\n110         26.83\n111         25.74\n112         25.51\n113         25.33\n114         25.13\n115         23.77\n116         23.64\n117         23.34\n118         23.07\n119         22.38\n120         22.04\n121         21.46\n122         21.15\n123         20.99\n124         20.96\n125         20.73\n126         20.64\n127         20.01\n128         19.78\n129         18.59\n130         18.59\n131         17.83\n132         17.02\n133         16.22\n134         16.06\n135         15.87\n136         15.84\n137         15.16\n138         14.74\n139         13.97\n140         13.97\n141         13.45\n142         13.30\n143         12.89\n144         11.95\n145         11.91\n146          9.87\n147          8.99\n148          7.89\n149          7.28\n150          6.95\n151          6.86\n152          6.12\n153          5.75\n154          5.70\n155          5.46\n156          5.25\n157          4.98\n158          4.89\n159          4.63\n160          3.28\n161          2.47\n162          2.46\n163          2.22\n164          1.92\n165          1.06\n166          0.85\n167         -0.43\n168         -1.09\n169         -1.86\n170         -2.24\n171         -2.79\n172         -4.23\n173         -4.62\n174         -5.02\n175         -6.11\n176         -6.47\n177         -6.76\n178         -6.92\n179         -7.38\n180         -7.72\n181         -9.17\n182         -9.25\n183        -11.90\n184        -12.02\n185        -15.72\n186        -17.18\n187        -17.22\n188        -18.77\n189        -18.89\n190        -19.49\n191        -19.93\n192        -20.85\n193        -21.74\n194        -21.99\n195        -37.36\n196            NA\n197            NA\n198            NA\n199            NA\n200            NA\n\n\nSo we know who the winners since the descending sort by Diff_Pct_2020 shows us Census tracts by the highest percentage first. Now, which areas suffered the biggest declines in median income? We will copy the previous code and alter the arrange so it is ascending from the lowest value.\n\nbaltcity_income %>%\n  select(Census, Neighborhood, x2010, x2020) %>%\n    mutate(Diff_Income = (x2020-x2010)) %>% \n    mutate(Diff_Pct_2020 = round((x2020-x2010)/x2010*100,2)) %>% \n    arrange(Diff_Pct_2020)\n\n     Census                          Neighborhood  x2010  x2020 Diff_Income\n1   1603.00       Sandtown-Winchester/Harlem Park  22292  13963       -8329\n2   2804.02                     Edmondson Village  52784  41179      -11605\n3   2715.03       Greater Roland Park/Poplar Hill  79861  62500      -17361\n4   2102.00            Washington Village/Pigtown  46319  36661       -9658\n5    805.00                         Clifton-Berea  35223  28203       -7020\n6   2716.00                 Southern Park Heights  43265  34832       -8433\n7   2720.03               Cross-Country/Cheswolde  70942  57543      -13399\n8   2502.05        Westport/Mount Winans/Lakeland  47738  38779       -8959\n9   1903.00                   Southwest Baltimore  17237  14269       -2968\n10  1604.00       Sandtown-Winchester/Harlem Park  27813  23036       -4777\n11   801.02                         Belair-Edison  35594  30000       -5594\n12   702.00                      Madison/East End  35667  31379       -4288\n13   908.00                     Midway/Coldstream  35473  31250       -4223\n14  2503.01        Westport/Mount Winans/Lakeland  27414  24877       -2537\n15  2604.02                   Claremont/Armistead  42938  39000       -3938\n16  2002.00                   Southwest Baltimore  30061  27740       -2321\n17  2709.01                             Northwood  51023  47260       -3763\n18  2504.01     Brooklyn/Curtis Bay/Hawkins Point  34401  32019       -2382\n19  2505.00     Brooklyn/Curtis Bay/Hawkins Point  32738  30526       -2212\n20  2804.04         Allendale/Irvington/S. Hilton  43779  40946       -2833\n21  2703.02                            Lauraville  68088  63929       -4159\n22  1304.00             Penn North/Reservoir Hill  32500  30868       -1632\n23  2709.03                             Northwood  54607  52083       -2524\n24  2503.03              Morrell Park/Violetville  42007  40230       -1777\n25  2007.01         Allendale/Irvington/S. Hilton  34556  33592        -964\n26  1602.00       Sandtown-Winchester/Harlem Park  25417  24848        -569\n27  1505.00                     Greater Mondawmin  34609  33967        -642\n28  2501.03              Morrell Park/Violetville  36957  36555        -402\n29  1308.05           Mount Washington/Coldspring  53984  53750        -234\n30  1513.00                 Southern Park Heights  33579  33866         287\n31  2706.00                      Harford/Echodale  66914  67625         711\n32  2718.01             Pimlico/Arlington/Hilltop  24802  25278         476\n33  2801.01                        Glen-Fallstaff  46576  47610        1034\n34   909.00                       Greenmount East  19056  19524         468\n35  1507.02                     Greater Mondawmin  48942  50150        1208\n36  1504.00                     Greater Mondawmin  32632  33702        1070\n37  2710.02                        Greater Govans  38500  40281        1781\n38  2719.00                        Glen-Fallstaff  52308  54866        2558\n39  2804.01       Beechfield/Ten Hills/West Hills  44954  47192        2238\n40   301.00              Harbor East/Little Italy  21932  23083        1151\n41  1506.00                      Greater Rosemont  29631  31250        1619\n42  1703.00                   Upton/Druid Heights  17892  18912        1020\n43  1508.00                  Forest Park/Walbrook  37199  39337        2138\n44  2606.04                          Southeastern  20710  21977        1267\n45  1001.00                       Greenmount East  28929  30913        1984\n46  2701.02                            Lauraville  54250  58020        3770\n47  1901.00                   Southwest Baltimore  22893  24559        1666\n48  2710.01                        Greater Govans  33203  35822        2619\n49  2604.03                   Claremont/Armistead  32373  35282        2909\n50  1503.00                      Greater Rosemont  32289  35476        3187\n51  2705.01                      Harford/Echodale  59516  66604        7088\n52  2804.03       Beechfield/Ten Hills/West Hills  51435  57582        6147\n53  2004.00                   Southwest Baltimore  28324  31975        3651\n54   907.00                     Midway/Coldstream  27330  30964        3634\n55  2005.00                   Southwest Baltimore  30239  34306        4067\n56   105.00                           Fells Point  86157  98194       12037\n57  2502.03                           Cherry Hill  29747  33902        4155\n58  2708.01                            Loch Raven  51415  58995        7580\n59  2003.00                   Southwest Baltimore  19063  21953        2890\n60   904.00                         The Waverlies  26490  30685        4195\n61  2502.04                           Cherry Hill  12384  14349        1965\n62  2501.01       Beechfield/Ten Hills/West Hills  52098  60463        8365\n63  2720.07                        Glen-Fallstaff  35903  41728        5825\n64  2603.02                         Belair-Edison  42727  50000        7273\n65  1701.00                   Downtown/Seton Hill  30197  35582        5385\n66  2001.00                   Southwest Baltimore  32214  38203        5989\n67  2504.02     Brooklyn/Curtis Bay/Hawkins Point  31250  37059        5809\n68   905.00                         The Waverlies  36507  43727        7220\n69  2709.02                             Northwood  48555  58269        9714\n70  1308.03  Medfield/Hampden/Woodberry/Remington  51548  62188       10640\n71  2708.02                            Loch Raven  46181  55753        9572\n72   701.00                      Madison/East End  37701  45602        7901\n73   601.00           Patterson Park North & East  46822  56652        9830\n74  1202.02       Greater Charles Village/Barclay  30179  36563        6384\n75  1301.00             Penn North/Reservoir Hill  21271  25835        4564\n76  1902.00                   Southwest Baltimore  43214  52738        9524\n77   902.00                             Northwood  67500  82609       15109\n78  2603.01                         Belair-Edison  42347  52117        9770\n79  2715.01           Mount Washington/Coldspring  88929 109688       20759\n80  2601.02                     Cedonia/Frankford  48783  60313       11530\n81  2708.03                            Loch Raven  42234  52272       10038\n82  2704.01                              Hamilton  51641  64620       12979\n83  2802.00            Howard Park/West Arlington  45047  56458       11411\n84  2717.00             Pimlico/Arlington/Hilltop  29069  36484        7415\n85  2602.02                     Cedonia/Frankford  31241  39282        8041\n86  2805.00                   Oldtown/Middle East  11278  14304        3026\n87  2502.06              Morrell Park/Violetville  52674  66900       14226\n88  1204.00       Greater Charles Village/Barclay  39375  50034       10659\n89  2708.05             Chinquapin Park/Belvedere  46875  59763       12888\n90  2718.02             Pimlico/Arlington/Hilltop  31392  40256        8864\n91  2801.02            Howard Park/West Arlington  33938  43618        9680\n92  1201.00     North Baltimore/Guilford/Homeland  56205  72349       16144\n93  1308.04  Medfield/Hampden/Woodberry/Remington  48207  62054       13847\n94  1608.01                     Edmondson Village  41549  53543       11994\n95  1511.00                  Dorchester/Ashburton  45649  58843       13194\n96  2711.01     North Baltimore/Guilford/Homeland  42994  55859       12865\n97  2602.01                     Cedonia/Frankford  33936  44093       10157\n98  1510.00                  Dorchester/Ashburton  35056  45870       10814\n99  2006.00         Allendale/Irvington/S. Hilton  29813  39076        9263\n100 2707.01                      Harford/Echodale  28618  37591        8973\n101  801.01                         Belair-Edison  49688  65417       15729\n102  806.00                       Greenmount East  26726  35221        8495\n103 2302.00             Inner Harbor/Federal Hill  80521 107344       26823\n104 2720.04               Cross-Country/Cheswolde  49567  66148       16581\n105 2606.05                          Southeastern  33545  44794       11249\n106 2705.02                              Hamilton  53493  71694       18201\n107 2702.00                            Lauraville  60128  80788       20660\n108 1303.00             Penn North/Reservoir Hill  32273  43380       11107\n109  401.00                   Downtown/Seton Hill  45948  62131       16183\n110 2403.00             Inner Harbor/Federal Hill  89211 120729       31518\n111  808.00                   Oldtown/Middle East  35532  48137       12605\n112 2201.00             Inner Harbor/Federal Hill  59259  80305       21046\n113 1607.00                      Greater Rosemont  29472  39959       10487\n114 1507.01                     Greater Mondawmin  38125  51944       13819\n115  602.00           Patterson Park North & East  51403  70313       18910\n116 2720.06                        Glen-Fallstaff  24859  34019        9160\n117 1509.00                  Forest Park/Walbrook  37440  51250       13810\n118  906.00                     Midway/Coldstream  36408  50048       13640\n119 1101.00                               Midtown  38797  53407       14610\n120 2608.00                          Highlandtown  35200  48850       13650\n121 2707.03                      Harford/Echodale  63521  88750       25229\n122 2713.00       Greater Roland Park/Poplar Hill 111250 155605       44355\n123 2803.01              Dickeyville/Franklintown  33707  47366       13659\n124 2701.01                            Lauraville  57326  80875       23549\n125 2602.03                     Cedonia/Frankford  32615  46250       13635\n126 1606.00                      Greater Rosemont  27128  38482       11354\n127  803.01                         Clifton-Berea  32024  45446       13422\n128 1402.00                   Upton/Druid Heights  16213  23152        6939\n129 1601.00       Sandtown-Winchester/Harlem Park  19583  27969        8386\n130 1608.02                     Edmondson Village  29355  41932       12577\n131 1203.00       Greater Charles Village/Barclay  38281  55074       16793\n132 1702.00                   Upton/Druid Heights   9412  13559        4147\n133 2402.00             Inner Harbor/Federal Hill  92500 133333       40833\n134  804.00                         Clifton-Berea  23893  34567       10674\n135  903.00                         The Waverlies  43824  63523       19699\n136 2708.04             Chinquapin Park/Belvedere  41094  59583       18489\n137 2707.02                      Harford/Echodale  39205  56963       17758\n138  901.00                        Greater Govans  36444  52988       16544\n139 2501.02         Allendale/Irvington/S. Hilton  36960  53854       16894\n140 2720.05               Cross-Country/Cheswolde  49757  72500       22743\n141 2712.00     North Baltimore/Guilford/Homeland 133548 195353       61805\n142 1205.00                               Midtown  37500  55170       17670\n143 2601.01                     Cedonia/Frankford  46789  68983       22194\n144 2703.01                            Lauraville  54623  80655       26032\n145 1501.00       Sandtown-Winchester/Harlem Park  18097  26989        8892\n146 1307.00  Medfield/Hampden/Woodberry/Remington  48634  72648       24014\n147 1202.01       Greater Charles Village/Barclay  54000  81393       27393\n148  604.00                   Oldtown/Middle East  35139  53472       18333\n149 2604.04         Orangeville/East Highlandtown  35708  54375       18667\n150 2008.00         Allendale/Irvington/S. Hilton  32711  49875       17164\n151 2605.01         Orangeville/East Highlandtown  42298  64706       22408\n152  302.00              Harbor East/Little Italy  63885  98254       34369\n153 1605.00                      Greater Rosemont  24556  37819       13263\n154 1206.00       Greater Charles Village/Barclay  23488  36221       12733\n155 2714.00       Greater Roland Park/Poplar Hill  97891 151146       53255\n156 2604.01                   Claremont/Armistead  31392  48661       17269\n157 1207.00  Medfield/Hampden/Woodberry/Remington  39730  61653       21923\n158 2101.00            Washington Village/Pigtown  48857  76368       27511\n159 2609.00                          Highlandtown  68571 107438       38867\n160  603.00           Patterson Park North & East  53813  84318       30505\n161 2611.00                          Highlandtown  83207 130769       47562\n162 2301.00             Inner Harbor/Federal Hill  62222  98125       35903\n163  201.00                           Fells Point  60439  95536       35097\n164  802.00                         Clifton-Berea  24200  38289       14089\n165 2704.02                              Hamilton  53125  84556       31431\n166 2007.02         Allendale/Irvington/S. Hilton  27305  43734       16429\n167 1502.00       Sandtown-Winchester/Harlem Park  25224  40644       15420\n168  104.00                                Canton  70313 114946       44633\n169 2803.02                  Forest Park/Walbrook  33864  56004       22140\n170 2610.00           Patterson Park North & East  36343  60893       24550\n171 2404.00                       South Baltimore  72122 121685       49563\n172  803.02                         Clifton-Berea  24423  41298       16875\n173 1803.00 Poppleton/The Terraces/Hollins Market  37879  64125       26246\n174  101.00                                Canton  75938 128839       52901\n175 1306.00  Medfield/Hampden/Woodberry/Remington  53654  91635       37981\n176  203.00                           Fells Point  59236 101815       42579\n177 2401.00                       South Baltimore  87619 151659       64040\n178 1401.00                               Midtown  32917  57292       24375\n179 2303.00                       South Baltimore  53194  92841       39647\n180 2502.07                           Cherry Hill  23310  41518       18208\n181 1512.00                 Southern Park Heights  12386  22632       10246\n182  202.00                           Fells Point  47131  86125       38994\n183  704.00                   Oldtown/Middle East  14527  27321       12794\n184 1302.00             Penn North/Reservoir Hill  30255  57457       27202\n185 1102.00                               Midtown  32563  62703       30140\n186 1403.00                   Upton/Druid Heights  21607  41913       20306\n187 1002.00                   Oldtown/Middle East   9862  19172        9310\n188  103.00                                Canton  77841 151389       73548\n189 1308.06  Medfield/Hampden/Woodberry/Remington  48750  95142       46392\n190  807.00                       Greenmount East  20679  40885       20206\n191 2711.02     North Baltimore/Guilford/Homeland  99300 199531      100231\n192 2607.00         Orangeville/East Highlandtown  36901  80000       43099\n193  102.00           Patterson Park North & East  58409 130357       71948\n194  703.00                      Madison/East End  15000  35000       20000\n195  402.00                   Downtown/Seton Hill  13229  43333       30104\n196 1003.00                      Unassigned--Jail     NA     NA          NA\n197 1801.00 Poppleton/The Terraces/Hollins Market  18429     NA          NA\n198 1802.00 Poppleton/The Terraces/Hollins Market  16585     NA          NA\n199 2506.00     Brooklyn/Curtis Bay/Hawkins Point     NA     NA          NA\n200 2603.03                   Claremont/Armistead  30192     NA          NA\n    Diff_Pct_2020\n1          -37.36\n2          -21.99\n3          -21.74\n4          -20.85\n5          -19.93\n6          -19.49\n7          -18.89\n8          -18.77\n9          -17.22\n10         -17.18\n11         -15.72\n12         -12.02\n13         -11.90\n14          -9.25\n15          -9.17\n16          -7.72\n17          -7.38\n18          -6.92\n19          -6.76\n20          -6.47\n21          -6.11\n22          -5.02\n23          -4.62\n24          -4.23\n25          -2.79\n26          -2.24\n27          -1.86\n28          -1.09\n29          -0.43\n30           0.85\n31           1.06\n32           1.92\n33           2.22\n34           2.46\n35           2.47\n36           3.28\n37           4.63\n38           4.89\n39           4.98\n40           5.25\n41           5.46\n42           5.70\n43           5.75\n44           6.12\n45           6.86\n46           6.95\n47           7.28\n48           7.89\n49           8.99\n50           9.87\n51          11.91\n52          11.95\n53          12.89\n54          13.30\n55          13.45\n56          13.97\n57          13.97\n58          14.74\n59          15.16\n60          15.84\n61          15.87\n62          16.06\n63          16.22\n64          17.02\n65          17.83\n66          18.59\n67          18.59\n68          19.78\n69          20.01\n70          20.64\n71          20.73\n72          20.96\n73          20.99\n74          21.15\n75          21.46\n76          22.04\n77          22.38\n78          23.07\n79          23.34\n80          23.64\n81          23.77\n82          25.13\n83          25.33\n84          25.51\n85          25.74\n86          26.83\n87          27.01\n88          27.07\n89          27.49\n90          28.24\n91          28.52\n92          28.72\n93          28.72\n94          28.87\n95          28.90\n96          29.92\n97          29.93\n98          30.85\n99          31.07\n100         31.35\n101         31.66\n102         31.79\n103         33.31\n104         33.45\n105         33.53\n106         34.03\n107         34.36\n108         34.42\n109         35.22\n110         35.33\n111         35.48\n112         35.52\n113         35.58\n114         36.25\n115         36.79\n116         36.85\n117         36.89\n118         37.46\n119         37.66\n120         38.78\n121         39.72\n122         39.87\n123         40.52\n124         41.08\n125         41.81\n126         41.85\n127         41.91\n128         42.80\n129         42.82\n130         42.84\n131         43.87\n132         44.06\n133         44.14\n134         44.67\n135         44.95\n136         44.99\n137         45.30\n138         45.40\n139         45.71\n140         45.71\n141         46.28\n142         47.12\n143         47.43\n144         47.66\n145         49.14\n146         49.38\n147         50.73\n148         52.17\n149         52.28\n150         52.47\n151         52.98\n152         53.80\n153         54.01\n154         54.21\n155         54.40\n156         55.01\n157         55.18\n158         56.31\n159         56.68\n160         56.69\n161         57.16\n162         57.70\n163         58.07\n164         58.22\n165         59.16\n166         60.17\n167         61.13\n168         63.48\n169         65.38\n170         67.55\n171         68.72\n172         69.09\n173         69.29\n174         69.66\n175         70.79\n176         71.88\n177         73.09\n178         74.05\n179         74.53\n180         78.11\n181         82.72\n182         82.74\n183         88.07\n184         89.91\n185         92.56\n186         93.98\n187         94.40\n188         94.48\n189         95.16\n190         97.71\n191        100.94\n192        116.80\n193        123.18\n194        133.33\n195        227.56\n196            NA\n197            NA\n198            NA\n199            NA\n200            NA\n\n\n\n15.0.1 Using mutate to clean data\nFor this example, we’ll examine a dataset of loans small businesses obtained to stay afloat during the Covid-19 pandemic. It’s called the PPP loan database which stands for Paycheck Protection Program loans.\n\n#ppp_applications_md_small <- read_csv(\"assets/data/ppp_applications_md.csv\") %>%\n#  select(name, amount, city, servicing_lender_state)\n\n#write_csv(ppp_applications_md_small,\"assets/data/ppp_applications_md_small.csv\")\n\nmaryland_ppp <- read_csv(\"assets/data/ppp_applications_md_small.csv\")\n\nRows: 195869 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, city, servicing_lender_state\ndbl (1): amount\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#working on this with your laptop, uncomment and use this code below\n#maryland_ppp <- read.csv(\"ppp_applications_md.csv\")\n\nTake a look at the city column in our data.\n\n#View(maryland_ppp)\n\nYou’ll notice that there’s a mix of styles: “Baltimore” and “BALTIMORE” for example. R will think those are two different cities, and that will mean that any aggregates we create based on city won’t be accurate.\nSo how can we fix that? Mutate is not just for math! And a function called str_to_upper that will convert a character column into all uppercase.\n\nmaryland_ppp %>%\n  mutate(\n    upper_city = str_to_upper(city)\n) %>% \n   select(city, upper_city, name, amount)\n\n# A tibble: 195,869 × 4\n   city             upper_city       name                                 amount\n   <chr>            <chr>            <chr>                                 <dbl>\n 1 Columbia         COLUMBIA         BAYWOOD HOTELS INC.                  2   e6\n 2 Lanham           LANHAM           PARTNERS ELECTRIC SERVICE INC.       1.09e6\n 3 Bethesda         BETHESDA         RP3 LLC                              7.88e5\n 4 Chevy Chase      CHEVY CHASE      SUSHI KO CHEVY CHASE LLC.            6.56e5\n 5 Capitol Heights  CAPITOL HEIGHTS  VILLAGE ACADEMY OF MARYLAND          4.70e5\n 6 Brentwood        BRENTWOOD        GOSHEN HOUSE TRADING LLC             3.46e5\n 7 Tracys Landing   TRACYS LANDING   WEAVER BOAT WORKS                    3.31e5\n 8 Essex            ESSEX            BOBS OVERHEAD DOOR REPAIR AND SERVI… 3.18e5\n 9 Annapolis        ANNAPOLIS        PARENTS FOR MONTESSORI EDUCATION INC 2.84e5\n10 Prince Frederick PRINCE FREDERICK HARVEST PRINCE FREDERICK LLC         2.42e5\n# … with 195,859 more rows\n\n\nNotice we kept the original data in city and transformed it in a new column, upper_city. Always a good practice to keep your original data intact in case you make a coding mistake or need to change the original for reference.\nWe could do the same thing with the address column in order to standardize that for analysis, too.\nThink of how easy this is compared to a similar data cleaning lesson in Google Sheets.\n\n\n15.0.2 A more powerful use\nMutate is even more useful when combined with some additional functions. Let’s say you want to know if the servicing lender is located in Maryland or outside the state. There are three possible answers:\n\nThe lender is in Maryland\nThe lender is outside Maryland\nThe data doesn’t tell us (servicing_lender_state is blank or NA)\n\nWe can create a new column that accounts for these possibilities and populate it using mutate and case_when, which is like an if/else statement but for more than two options.\n\nmaryland_with_in_out <- maryland_ppp %>%\n  mutate(\n    in_out = case_when(\n        servicing_lender_state == 'NA' ~ \"NA\",\n        servicing_lender_state == 'MD' ~ \"IN\",\n        servicing_lender_state != 'MD' ~ \"OUT\"\n      )\n  ) \n\nmaryland_with_in_out %>% \n  select(name, amount, servicing_lender_state, in_out)\n\n# A tibble: 195,869 × 4\n   name                                        amount servicing_lender_…¹ in_out\n   <chr>                                        <dbl> <chr>               <chr> \n 1 BAYWOOD HOTELS INC.                       2000000  MD                  IN    \n 2 PARTNERS ELECTRIC SERVICE INC.            1086257  NY                  OUT   \n 3 RP3 LLC                                    787500  VA                  OUT   \n 4 SUSHI KO CHEVY CHASE LLC.                  655732  NY                  OUT   \n 5 VILLAGE ACADEMY OF MARYLAND                469800  WV                  OUT   \n 6 GOSHEN HOUSE TRADING LLC                   346500  NJ                  OUT   \n 7 WEAVER BOAT WORKS                          330700  MD                  IN    \n 8 BOBS OVERHEAD DOOR REPAIR AND SERVICE INC  317504  MD                  IN    \n 9 PARENTS FOR MONTESSORI EDUCATION INC       284185  MD                  IN    \n10 HARVEST PRINCE FREDERICK LLC               242094. MO                  OUT   \n# … with 195,859 more rows, and abbreviated variable name\n#   ¹​servicing_lender_state\n\n\nWe can then use our new in_out column in group_by statements to make summarizing easier.\nIn this case there are no Maryland loans where servicing_lender_state has a value of ‘NA’, but you should never assume that will be the case for a dataset. If you know that the only options are the lender is in Maryland or is outside it, you can rewrite the previous code as an if/else statement:\n\nmaryland_with_in_out <- maryland_ppp %>%\n  mutate(\n    in_out = if_else(\n        servicing_lender_state == 'MD', \"IN\", \"OUT\"\n      )\n  )\n\nmaryland_with_in_out %>% \n  select(name, amount, servicing_lender_state, in_out)\n\n# A tibble: 195,869 × 4\n   name                                        amount servicing_lender_…¹ in_out\n   <chr>                                        <dbl> <chr>               <chr> \n 1 BAYWOOD HOTELS INC.                       2000000  MD                  IN    \n 2 PARTNERS ELECTRIC SERVICE INC.            1086257  NY                  OUT   \n 3 RP3 LLC                                    787500  VA                  OUT   \n 4 SUSHI KO CHEVY CHASE LLC.                  655732  NY                  OUT   \n 5 VILLAGE ACADEMY OF MARYLAND                469800  WV                  OUT   \n 6 GOSHEN HOUSE TRADING LLC                   346500  NJ                  OUT   \n 7 WEAVER BOAT WORKS                          330700  MD                  IN    \n 8 BOBS OVERHEAD DOOR REPAIR AND SERVICE INC  317504  MD                  IN    \n 9 PARENTS FOR MONTESSORI EDUCATION INC       284185  MD                  IN    \n10 HARVEST PRINCE FREDERICK LLC               242094. MO                  OUT   \n# … with 195,859 more rows, and abbreviated variable name\n#   ¹​servicing_lender_state\n\n\nMutate is an essential tool to make your data more useful and allows you to ask more questions.\n\n\n16 Pre-Lab Questions:\nBased on the skills you have learned above, use this dataset of Maryland cities ‘assets/data/city_md_income.csv’ (it is included in your prelab folder)\nAnd answer the following questions:\nQuestion #1: \n\nCalculate the difference in median income from 2010 to 2020 for all places listed. Calculate the percentage change.\n\n\n\n\nWhich city had the highest absolute change? Which had the highest percentage change?\n\n\n\nProduce a table with the top 25 places ranked by the highest percentage change.\n\n\n\nProduce a second table with the top 25 places ranked by the lowest or negative percentage change.\n\n\n\nQuestion #2:  Answer this question in English: Write in Elms\n\nBuilding on the previous lesson on filtering, describe how you would determine the median value of the percentage change and produce a table with the 10 neighborhoods with the highest change and the 10 with the lowest change."
  },
  {
    "objectID": "r_visualizing_reporting.html",
    "href": "r_visualizing_reporting.html",
    "title": "16  Visualizing your data for reporting",
    "section": "",
    "text": "Before you launch into trying to chart or map your data, take a minute to think about the many roles that static and interactive graphic elements play in your journalism.\nSometimes, you can help yourself and your story by just creating a quick chart, which helps you see patterns in the data that wouldn’t otherwise surface. In the reporting phase, visualizations can:\n\nHelp you identify themes and questions for the rest of your reporting\nIdentify outliers – good stories, or perhaps errors, in your data\nHelp you find typical examples\nShow you holes in your reporting\n\nVisualizations also play multiple roles in publishing:\n\nIllustrate a point made in a story in a more compelling way\nRemove unnecessarily technical information from prose\nAllow exploration, provide transparency about your reporting to readers\n\nVisualizing data is becoming a much greater part of journalism. Large news organizations are creating graphics desks that create complex visuals with data to inform the public about important events. To do it well is a course on its own. And not every story needs a feat of programming and art.\nGood news: one of the best libraries for visualizing data is in the tidyverse and it’s pretty simple to make simple charts quickly with just a little bit of code. It’s called ggplot2.\nLet’s revisit some data we’ve used in the past and turn it into charts. First, let’s load libraries. When we load the tidyverse, we get ggplot2.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "r_visualizing_reporting.html#line-charts",
    "href": "r_visualizing_reporting.html#line-charts",
    "title": "16  Visualizing your data for reporting",
    "section": "16.1 Line charts",
    "text": "16.1 Line charts\nLet’s look at how to make another common chart type that will help you understand patterns in your data.\nLine charts can show change over time. It works much the same as a bar chart, code wise, but instead of a weight, it uses a y. We’ll put the year on the x axis and count of loans, or n, on the y axis.\n\nyears %>%\n  ggplot() +\n  geom_line(aes(x=year, y=n)) \n\n\n\n\nIt’s not super pretty, but there’s an obvious pattern, a bog peak in 2017.\nLet’s build another table with some detail by day.\n\ndays <- SF %>% \n  count(call_date2) \ndays\n\n# A tibble: 1,340 × 2\n   call_date2     n\n   <date>     <int>\n 1 2016-03-31    10\n 2 2016-04-01   130\n 3 2016-04-02   119\n 4 2016-04-03   148\n 5 2016-04-04   171\n 6 2016-04-05   170\n 7 2016-04-06   170\n 8 2016-04-07   105\n 9 2016-04-08   142\n10 2016-04-09    95\n# … with 1,330 more rows\n\n\nHere is the chart by day\n\ndays %>% \n  ggplot(aes(x = call_date2, y = n, fill = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"311 Calls for Service By Day, San Francisco\", \n       subtitle = \"SF PD Service Call Data, 2016-2019\",\n       caption = \"Graphic by Wells\",\n       y=\"Number of Calls\",\n       x=\"Day\")\n\n\n\n\nBy charting this, we can quickly see a pattern that can help guide our reporting: why was there such a huge drop in mid-2018?\nLet’s build a line chart for just a few variables based on year and disposition: Citations of people, Cancelled Calls and Admonished people on the scene.\n\ndispo <- SF %>% \n  filter(disposition == c(\"CAN\", \"CIT\", \"ADM\"))\n\nWarning in disposition == c(\"CAN\", \"CIT\", \"ADM\"): longer object length is not a\nmultiple of shorter object length\n\ndispo <- dispo %>% \n  select(year, disposition) %>% \n  group_by(year) %>% \n  count(disposition)\n\ndispo_pivot <- dispo %>% \n  pivot_wider(names_from = \"disposition\", values_from = \"n\") %>% \n  as.data.frame()\n\nWith this dataframe, we will now use separate lines per column to graph the results\n\nggplot(dispo_pivot, aes(x=year)) +\n  geom_line(aes(y=ADM), color = \"blue\") +\n  geom_line(aes(y=CAN), color = \"brown\") +\n  geom_line(aes(y=CIT), color = \"green\") \n\n\n\n\nOr you can plot them with side-by-side columns. Use dispo and plot the columns by type of disposition. Each individual variable in disposition is assigned its own column and color. The years are plotted on the x axis.\n\nggplot(dispo, aes(year, n, fill = disposition)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nWe’re just scratching the surface of what ggplot can do, and chart types. There’s so much more you can do, so many other chart types you can make. But the basics we’ve shown here will get you started."
  },
  {
    "objectID": "r-join-bind.html",
    "href": "r-join-bind.html",
    "title": "17  Joins",
    "section": "",
    "text": "In this chapter:\n\n\nA join combines two or more tables (data frames) by column.\njoining to data frames requires exact matches on one or more columns. Close matches don’t count.\nUse codes in one data frame to “look up” information in another, and attach it to a row, such as the translation of codes to words or demographics of a Census tract.\nMany public records databases come with “lookup tables” or “code sheets”. Make sure to ask for the codes AND their translations in a data request.\nReporters don’t always stick to matchmaking the way the database designers intended. “Enterprise” joins are those that let you find needles in a haystack, such as bus drivers with a history of DUIs.\nMatching one data frame against an entirely different one will always produce errors. You can minimize the kind of error you fear most – false positives or false negatives – but you likely will have to report out your findings on the ground."
  },
  {
    "objectID": "r-join-bind.html#join-basics",
    "href": "r-join-bind.html#join-basics",
    "title": "17  Joins",
    "section": "17.1 Join basics",
    "text": "17.1 Join basics\n“Join” in computer programming mean adding columns to a data frame by combining it with another data frame or table. Reporters use it in many ways, some intended by the people who made the data source, and some not.\nFrom now on, we’ll start using the term “table” instead of “data frame”, since we can talk about several different ones at the same time.\nMany databases are created expecting you to join tables (data frames) because it’s a more efficient way to store and work with large databases. This is what’s called a “relational database”, and they’re everywhere.\nHere’s an example, using campaign finance information. The Federal Elections Commission distributes campaign contribution in related tables, each referring to a different noun. One table lists donations, the other lists candidates and other political action committees. They link together using a common code:\n\n\n\nCampaign finance join\n\n\nThe reason to do this is that you never have to worry that any changes to the candidate information – the treasurer, the address or the office sought – carries over to the donation. It’s only listed once in the candidate table. Most large databases are constructed this way. For example:\n\nYour school records are held using your student ID, which means that your address and home email only needs to be changed once, not in every class or in every account you have with the school.\nInspection records, such as those for restaurants, hospitals, housing code violations and workplace safety, typically have at least three tables: The establishment (like a restaurant or a workplace), an inspection (an event on a date), and a violation (something that they found). They’re linked together using establishment ID’s.\nA court database usually has many types of records: A master case record links to information on charges, defendants, lawyers, sentences and court hearings.\n\nEach table, then, is described using a different noun – candidates or contributions; defendants or cases; students or courses. This conforms to the tidy data principle that different types of information are stored in different tables."
  },
  {
    "objectID": "r-join-bind.html#matchmaking-with-joins",
    "href": "r-join-bind.html#matchmaking-with-joins",
    "title": "17  Joins",
    "section": "17.2 Matchmaking with joins",
    "text": "17.2 Matchmaking with joins\nThe political data type of join described above is often referred to as a “lookup table”. You’ll match codes to their meanings in a way that was intended by the people who made the database. But there are other ways reporters use joins:\n\n“Enterprise” joins\nJournalists have taken to calling a specific kind of join “enterprise”, referring to the enterprising reporters who do this. Here, you’ll look for needles in a haystack. Some of the most famous data journalism investigations relied on joining two databases that started from completely different sources, such as:\n\nBus drivers who had DUI citations\nDonors to a governor who got contracts from the state\nDay care workers with criminal records\nSmall businesses that have defaulted on government backed loans that got a PPP loan anyway.\n\nWhen you match these kinds of datasets, you will always have some error. You always have to report out any suspected matches, so they are time consuming stories.\nIn the mid-2000s, when some politicians insisted that dead people were voting and proposed measures to restrict registration, almost every regional news organization sent reporters on futile hunts for the dead voters. They got lists of people on the voter rolls, then lists of people who had died through the Social Security Death Index or local death certificates. I never met anyone who found a single actual dead voter, but months of reporter-hours were spent tracking down each lead.\nIt’s very common for two people to have the same name in a city. In fact, it’s common to have two people at the same home with the same name – they’ve just left off “Jr.” and “Sr.” in the database. In this case, you’ll find matches that you shouldn’t. These are false positives, or Type I errors in statistics.\nAlso, we rarely get dates of birth or Social Security Numbers in public records, so we have to join by name and sometimes location. If someone has moved, sometimes uses a nickname, or the government has recorded the spelling incorrectly, the join will fail – you’ll miss some of the possible matches. This is very common with company names, which can change with mergers and other changes in management, and can be listed in many different ways.\nThese are false negatives, or Type II errors in statistics.1\nIn different contexts, you’ll want to minimize different kinds of errors. For example, if you are looking for something extremely rare, and you want to examine every possible case – like a child sex offender working in a day care center – you might choose to make a “loose” match and get lots of false positives, which you can check. If you want to limit your reporting only to the most promising leads, you’ll be willing to live with missing some cases in order to be more sure of the joins you find.\nYou’ll see stories of this kind write around the lack of precision – they’ll often say, “we verified x cases of….” rather than pretend that they know of them all.\n\n\nFind cases with interesting characteristics\nYou’ll often want to learn more about a geographic area’s demographics or voting habits or other characteristics, and match it to other data. Sometimes it’s simple: Find the demographics of counties that switched from Trump to Biden as a way to isolate places you might want to visit. Another example from voting might be to find the precinct that has the highest percentage of Latino citizens in the county, then match that precinct against the voter registration rolls to get a list of people you might want to interview on election day. In these instances, the join is used for a filter.\nThis is also common when you have data by zip code or some other geography, and you want to find clusters of interesting potential stories, such as PPP loans in minority neighborhoods.\n\n\nSummarize data against another dataset\nThe previous examples all result in lists of potential story people or places. If you use join on summarized data, you can characterize a broad range of activity across new columns. Simplified, this is how you can write that more PPP money went to predominantly white neighborhoods than those that were majority Black.\n\n\n17.2.1 Types of joins\nThere are several types of joins, each connecting two tables in a slightly different matter. The most important ones are:\n\ninner_join\nleft_join\nanti_join\n\nWe’ll explore how each of these work in example code below:\n\n\n17.2.2 Load libraries\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\n17.2.3 Load data\nWe’re going to load four tables to demonstrate different types of joins. All contain information from the U.S. Census 2019 American Community Survey for Maryland counties. Load and examine them.\n\n###\n# Total population for each Maryland county \n# County identified by GEOID (a 5-digit code), not name\n###\n\nmaryland_county_population <- read_rds(\"assets/data/maryland_county_population.rds\")\n\n###\n# A lookup table that shows the name of each Maryland county, paired with GEOID\n###\n\n\nmaryland_county_lookup_table <- read_rds(\"assets/data/maryland_county_lookup_table.rds\")\n\n###\n# Total population for each Maryland county, EXCEPT for Prince George's County\n# County identified by GEOID (a 5-digit code), not name\n###\n\nmaryland_county_population_no_pg <- read_rds(\"assets/data/maryland_county_population_no_pg.rds\")\n\n###\n# Display the tables\n###\n\nmaryland_county_population\n\n# A tibble: 24 × 2\n   geoid total_pop\n   <chr>     <dbl>\n 1 24001     71002\n 2 24003    575421\n 3 24005    828193\n 4 24009     92094\n 5 24011     33260\n 6 24013    168233\n 7 24015    102889\n 8 24017    161448\n 9 24019     31994\n10 24021    255955\n# … with 14 more rows\n\nmaryland_county_lookup_table\n\n# A tibble: 24 × 2\n   geoid name                         \n   <chr> <chr>                        \n 1 24001 Allegany County, Maryland    \n 2 24003 Anne Arundel County, Maryland\n 3 24005 Baltimore County, Maryland   \n 4 24009 Calvert County, Maryland     \n 5 24011 Caroline County, Maryland    \n 6 24013 Carroll County, Maryland     \n 7 24015 Cecil County, Maryland       \n 8 24017 Charles County, Maryland     \n 9 24019 Dorchester County, Maryland  \n10 24021 Frederick County, Maryland   \n# … with 14 more rows\n\nmaryland_county_population_no_pg \n\n# A tibble: 23 × 2\n   geoid total_pop\n   <chr>     <dbl>\n 1 24001     71002\n 2 24003    575421\n 3 24005    828193\n 4 24009     92094\n 5 24011     33260\n 6 24013    168233\n 7 24015    102889\n 8 24017    161448\n 9 24019     31994\n10 24021    255955\n# … with 13 more rows\n\n\n\n\n17.2.4 Inner Join\nThe table maryland_county_population has two columns, one with a geoid code representing each Maryland county (and Baltimore city), and another with total population. The maryland_county_lookup_table is a classic example of a lookup table. It contains the same geoid representing each Maryland county, plus a name column with the proper name.\nWe are going to join maryland_county_population, using the one column both share: geoid. We’ll use an inner_join. The code below says: “Connect the two tables on the geoid column. If a given geoid exists in both tables, include it in our new table. If it isn’t in both, don’t include it.”\n\nupdated_maryland_county_population <- maryland_county_lookup_table %>%\n  inner_join(maryland_county_population, by=\"geoid\")\n\nupdated_maryland_county_population \n\n# A tibble: 24 × 3\n   geoid name                          total_pop\n   <chr> <chr>                             <dbl>\n 1 24001 Allegany County, Maryland         71002\n 2 24003 Anne Arundel County, Maryland    575421\n 3 24005 Baltimore County, Maryland       828193\n 4 24009 Calvert County, Maryland          92094\n 5 24011 Caroline County, Maryland         33260\n 6 24013 Carroll County, Maryland         168233\n 7 24015 Cecil County, Maryland           102889\n 8 24017 Charles County, Maryland         161448\n 9 24019 Dorchester County, Maryland       31994\n10 24021 Frederick County, Maryland       255955\n# … with 14 more rows\n\n\nBecause both tables contained the same 24 counties, our new table has 24 counties as well.\n(In this case, they have the same column name, but they don’t have to. As long as they CONTAIN the same thing, they can have different names. You can also join using more than one column).\nWhat happens when they aren’t identical?\nTo illustrate what happens, let’s run the same code, but this time use population table that contains every Maryland county EXCEPT for Prince George’s County. This table only has 23 rows. The lookup table still has 24.\n\n\n# A tibble: 23 × 3\n   geoid name                          total_pop\n   <chr> <chr>                             <dbl>\n 1 24001 Allegany County, Maryland         71002\n 2 24003 Anne Arundel County, Maryland    575421\n 3 24005 Baltimore County, Maryland       828193\n 4 24009 Calvert County, Maryland          92094\n 5 24011 Caroline County, Maryland         33260\n 6 24013 Carroll County, Maryland         168233\n 7 24015 Cecil County, Maryland           102889\n 8 24017 Charles County, Maryland         161448\n 9 24019 Dorchester County, Maryland       31994\n10 24021 Frederick County, Maryland       255955\n# … with 13 more rows\n\n\nWhen we join, we get only 23 rows. One table had every Maryland county. The other had every county but Prince George’s. When we inner join, only values that both tables have in common are returned. Thus, we get a table without P.G.\nThis gif from https://github.com/gadenbuie/tidyexplain shows what happens with inner_joins.\n\n\n\n\n\n\n\n17.2.5 Left Joins\nLet’s stick with our mismatched tables to illustrate what happens with another type of join, a left join (and it’s cousin, the right join). The code is the same, but we’ve swapped left_join for inner_join. Remember, our population table is missing P.G. county, but the lookup table has it.\n\nupdated_maryland_county_population_no_pg <- maryland_county_lookup_table %>%\n  left_join(maryland_county_population_no_pg, by=\"geoid\")\n\nupdated_maryland_county_population_no_pg\n\n# A tibble: 24 × 3\n   geoid name                          total_pop\n   <chr> <chr>                             <dbl>\n 1 24001 Allegany County, Maryland         71002\n 2 24003 Anne Arundel County, Maryland    575421\n 3 24005 Baltimore County, Maryland       828193\n 4 24009 Calvert County, Maryland          92094\n 5 24011 Caroline County, Maryland         33260\n 6 24013 Carroll County, Maryland         168233\n 7 24015 Cecil County, Maryland           102889\n 8 24017 Charles County, Maryland         161448\n 9 24019 Dorchester County, Maryland       31994\n10 24021 Frederick County, Maryland       255955\n# … with 14 more rows\n\n\nWhat happens? Everywhere there’s a match on GEOID, the population value is returned.\nBut look closely at Prince George’s County. Its value is NA.\nA left_join says “Return every single value from our first table – in this case the lookup table, with 24 rows. Where there’s a match with the population table, make the match. If there is no match, just put NA.”\nUnlike the inner_join, the left_join returned P.G. county despite it’s lack of a match.\nThis gif from https://github.com/gadenbuie/tidyexplain shows what happens with left_joins.\n\n\n\n\n\nA note: left_join has a close cousin called the right_join which isn’t used very often, and we won’t delve into here. It’s basically the same thing, it just starts from the second table, not the first."
  },
  {
    "objectID": "r-join-bind.html#anti-joins",
    "href": "r-join-bind.html#anti-joins",
    "title": "17  Joins",
    "section": "17.3 Anti joins",
    "text": "17.3 Anti joins\nAn anti_join is useful for seeing what values exist in one table that are missing from another table. It comes in handy during data cleaning and when writing more advanced functions. The code below says: “Connect the two tables, but only show me rows that exist in the lookup table, but DO NOT exist in the population table.”\n\nupdated_maryland_county_population_no_pg <- maryland_county_lookup_table %>%\n  anti_join(maryland_county_population_no_pg, by=\"geoid\")\n\nupdated_maryland_county_population_no_pg\n\n# A tibble: 1 × 2\n  geoid name                            \n  <chr> <chr>                           \n1 24033 Prince George's County, Maryland\n\n\nThis gif from https://github.com/gadenbuie/tidyexplain shows what happens with anti_joins."
  },
  {
    "objectID": "r-join-bind.html#joining-risks",
    "href": "r-join-bind.html#joining-risks",
    "title": "17  Joins",
    "section": "17.4 Joining risks",
    "text": "17.4 Joining risks\nThere are lots of risks in joining tables that you created yourself, or that were created outside a big relational database system. Keep an eye on the number of rows returned every time that you join – you should know what to expect.\n\n17.4.1 Double counting with joins\nWe won’t go into this in depth, but just be aware it’s easy to double-count rows when you join. Here’s a made-up example, in which a zip code is on the border and is in two counties:\nSay you want to use some data on zip codes :\n\n\n\nzip code\ncounty\ninfo\n\n\n\n\n21012\nPG\nsome data\n\n\n21012\nBaltimore\nsome more data\n\n\n\nand match it to a list of restaurants in a zip code:\n\n\n\nzip code\nrestaurant name\n\n\n\n\n21012\nMy favorite restaurant\n\n\n21012\nMy second-favorite restaurant\n\n\n\nWhen you match these on zip code, you’ll get 4 rows:\n\n\n\nzip code\ncounty\ninfo\nrestaurant name\n\n\n\n\n21012\nPG\nsome data\nMy favorite restaurant\n\n\n21012\nBaltimore\nsome more data\nMy favorite restaurant\n\n\n21012\nPG\nsome data\nMy second-favorite restaurant\n\n\n21012\nBaltimore\nsome more data\nMy second-favority restaurant\n\n\n\nNow, every time you try to count restaurants, these two will be double-counted.\nIn computing, this is called a “many-to-many” relationship – there are many rows of zip codes and many rows of restaurants. In journalism, we call it spaghetti. It’s usually an unintended mess.\nHere’s a gif that shows the double counting in action.\n\n\n\n\n\n\n\n17.4.2 Losing rows with joins\nThe opposite can occur if you aren’t careful and there are items you want to keep that are missing in your reference table. That’s what happened in the immunization data above for the seven schools that I couldn’t find.\n\n\n17.4.3 Merging or Binding\nJoins are one way to combine two data sets, using shared values in one or more columns. Binding two data sets together is another way.\nLet’s suppose we have two tables with population information for Maryland counties, each with the same columns: geoid, name and total population.\n\nOne has information for Maryland’s 10 largest counties by population\nThe other has information for every other Maryland county.\n\n\n###\n# Total population for each top-10 largest Maryland county \n# County identified by GEOID (a 5-digit code) and name\n###\n\nmaryland_large_county_population <- read_rds(\"assets/data/maryland_large_county_population.rds\")\n\n\n###\n# Total population for all other Maryland counties\n# County identified by GEOID (a 5-digit code) and name\n###\nmaryland_small_county_population <- read_rds(\"assets/data/maryland_small_county_population.rds\")\n\n\n# Display\nmaryland_large_county_population\n\n# A tibble: 10 × 3\n   geoid name                             total_pop\n   <chr> <chr>                                <dbl>\n 1 24031 Montgomery County, Maryland        1047661\n 2 24033 Prince George's County, Maryland    910551\n 3 24005 Baltimore County, Maryland          828193\n 4 24510 Baltimore city, Maryland            602274\n 5 24003 Anne Arundel County, Maryland       575421\n 6 24027 Howard County, Maryland             322407\n 7 24021 Frederick County, Maryland          255955\n 8 24025 Harford County, Maryland            253736\n 9 24013 Carroll County, Maryland            168233\n10 24017 Charles County, Maryland            161448\n\nmaryland_small_county_population\n\n# A tibble: 14 × 3\n   geoid name                          total_pop\n   <chr> <chr>                             <dbl>\n 1 24029 Kent County, Maryland             19456\n 2 24039 Somerset County, Maryland         25699\n 3 24023 Garrett County, Maryland          29155\n 4 24019 Dorchester County, Maryland       31994\n 5 24011 Caroline County, Maryland         33260\n 6 24041 Talbot County, Maryland           37087\n 7 24035 Queen Anne's County, Maryland     50163\n 8 24047 Worcester County, Maryland        51967\n 9 24001 Allegany County, Maryland         71002\n10 24009 Calvert County, Maryland          92094\n11 24015 Cecil County, Maryland           102889\n12 24045 Wicomico County, Maryland        103222\n13 24037 St. Mary's County, Maryland      113182\n14 24043 Washington County, Maryland      150575\n\n\nIf we want to combine these tables into one dataset with every county’s information, a join isn’t the way to go. For that, we’d use bind_rows(). Thick of bind_rows as stacking two tables on top of each other. This code gives us 24 rows.\n\nall_county_population <- maryland_small_county_population %>%\n  bind_rows(maryland_large_county_population)\n\nall_county_population\n\n# A tibble: 24 × 3\n   geoid name                          total_pop\n   <chr> <chr>                             <dbl>\n 1 24029 Kent County, Maryland             19456\n 2 24039 Somerset County, Maryland         25699\n 3 24023 Garrett County, Maryland          29155\n 4 24019 Dorchester County, Maryland       31994\n 5 24011 Caroline County, Maryland         33260\n 6 24041 Talbot County, Maryland           37087\n 7 24035 Queen Anne's County, Maryland     50163\n 8 24047 Worcester County, Maryland        51967\n 9 24001 Allegany County, Maryland         71002\n10 24009 Calvert County, Maryland          92094\n# … with 14 more rows\n\n\nQuestion #1:  Answer this question in English: Write in Elms\nYou have a dataframe that contains information on the population of each Maryland county, structured like this example (the column headers and one example row):\ngeoid | name | total_pop 24029 | Kent County, Maryland | 19456\nYou have another dataframe with a count of large employers (over 10000 workers) by county in Maryland, structured like this example (the column headers and one example row):\nname | number_of_large employers Kent County | 15\nYou want to join these two dataframes to answer the question “which Maryland county has the highest number of large employees per person?”\nWhat do you think will happen when you attempt to “inner_join” these two tables?"
  },
  {
    "objectID": "r-join-bind.html#resources",
    "href": "r-join-bind.html#resources",
    "title": "17  Joins",
    "section": "17.5 Resources",
    "text": "17.5 Resources\n\nThe “Relational data” chapter in the R for Data Science textbook has details on exactly how a complex data set might fit together.\nAn example using a superheroes dataset, from Stat 545 at the University of British Columbia"
  },
  {
    "objectID": "r-data-cleaning.html",
    "href": "r-data-cleaning.html",
    "title": "18  Data Cleaning",
    "section": "",
    "text": "Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?\nOne of the first things you should do is give it the smell test.\nFailure to give data the smell test can lead you to miss stories and get your butt kicked on a competitive story.\nWith data smells, we’re trying to find common mistakes in data. For more on data smells, read the GitHub wiki post that started it all. Some common data smells are:\nNot all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.\nThis chapter will take us through the first three, and look at common solutions."
  },
  {
    "objectID": "r-data-cleaning.html#wrong-type",
    "href": "r-data-cleaning.html#wrong-type",
    "title": "18  Data Cleaning",
    "section": "18.1 Wrong Type",
    "text": "18.1 Wrong Type\nFirst, let’s look at Wrong Type Of Data.\nLoad the tidyverse.\n\n# Remove scientific notation\noptions(scipen=999)\n# Load the tidyverse\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nThen lets load a dataframe of Census population statistics, one row per state per year between the period of 2015 and 2020, with overall totals and totals by Census race categories. I have intentionally introduced some flaws that we’ll have to clean.\n\nstate_population_dirty <- read_rds(\"assets/data/state_population_dirty.rds\")\n\nglimpse(state_population_dirty)\n\nRows: 255\nColumns: 11\n$ geoid                        <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"02\", \"02\",…\n$ state                        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama…\n$ year                         <dbl> 2015, 2016, 2017, 2018, 2020, 2015, 2016,…\n$ total_pop                    <chr> \"4830620\", \"4841164\", \"4850771\", \"4864680…\n$ white_alone_pop              <dbl> 3325464, 3325037, 3317702, 3317453, 33028…\n$ black_alone_pop              <dbl> NA, NA, NA, NA, NA, 25022, 24443, 23702, …\n$ amer_ind_ak_native_alone_pop <dbl> 23850, 23919, 25098, 25576, 24764, 101313…\n$ asian_alone_pop              <dbl> 59599, 60744, 62815, 64609, 67909, 42921,…\n$ native_hi_alone_pop          <dbl> 2439, 2008, 2213, 2182, 2042, 8841, 8862,…\n$ other_race_alone_pop         <dbl> 61078, 61991, 66942, 70055, 74996, 9273, …\n$ two_or_more_races_pop        <dbl> 81646, 85412, 88834, 91619, 119322, 61755…\n\n\nLet’s sort the dataframe by total population to identify the state with the largest population in 2020.\n\nstate_population_dirty %>%\n  filter(year == 2020) %>%\n  arrange(desc(total_pop))\n\n# A tibble: 51 × 11\n   geoid state      year total…¹ white…² black…³ amer_…⁴ asian…⁵ nativ…⁶ other…⁷\n   <chr> <chr>     <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 26    Michigan   2020 9973907 7735902 1360149   50035  316844    3117  131580\n 2 10    Delaware   2020 967679   652587  212795    3560   38528     705   21937\n 3 34    New Jers…  2020 8885418 5820147 1189681   22288  857873    3156  564662\n 4 46    South Da…  2020 879336   735228   18836   74975   12413     544    7320\n 5 51    Virginia   2020 8509358 5643436 1620649   22553  570398    5994  240542\n 6 38    North Da…  2020 760394   651470   23959   39165   11979    1004    8875\n 7 53    Washingt…  2020 7512465 5523881  290245   91766  662902   51117  360578\n 8 02    Alaska     2020 736990   466961   23894  107298   47289   10485   12231\n 9 04    Arizona    2020 7174064 5292498  325105  311014  239190   14633  492027\n10 11    District…  2020 701974   288306  318631    2438   28762     328   33764\n# … with 41 more rows, 1 more variable: two_or_more_races_pop <dbl>, and\n#   abbreviated variable names ¹​total_pop, ²​white_alone_pop, ³​black_alone_pop,\n#   ⁴​amer_ind_ak_native_alone_pop, ⁵​asian_alone_pop, ⁶​native_hi_alone_pop,\n#   ⁷​other_race_alone_pop\n\n\nSomething seems off. The largest U.S. states – New York, California, Texas – aren’t on this list. It’s topped by Michigan, with 9.9 million people, followed by Delaware with 967,000, followed by New Jersey with 8.8 million. It’s not treating the values in this column as numbers, but sorting them … alphabetically, in a sense.\nLet’s use glimpse to figure out why.\n\nglimpse(state_population_dirty)\n\nRows: 255\nColumns: 11\n$ geoid                        <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"02\", \"02\",…\n$ state                        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama…\n$ year                         <dbl> 2015, 2016, 2017, 2018, 2020, 2015, 2016,…\n$ total_pop                    <chr> \"4830620\", \"4841164\", \"4850771\", \"4864680…\n$ white_alone_pop              <dbl> 3325464, 3325037, 3317702, 3317453, 33028…\n$ black_alone_pop              <dbl> NA, NA, NA, NA, NA, 25022, 24443, 23702, …\n$ amer_ind_ak_native_alone_pop <dbl> 23850, 23919, 25098, 25576, 24764, 101313…\n$ asian_alone_pop              <dbl> 59599, 60744, 62815, 64609, 67909, 42921,…\n$ native_hi_alone_pop          <dbl> 2439, 2008, 2213, 2182, 2042, 8841, 8862,…\n$ other_race_alone_pop         <dbl> 61078, 61991, 66942, 70055, 74996, 9273, …\n$ two_or_more_races_pop        <dbl> 81646, 85412, 88834, 91619, 119322, 61755…\n\n\nHere we can see the column name, sample values, and the data type. Most of the number columns in this dataframe are stored as numbers (“dbl”). But not the value in total_pop. It’s stored as “character”. R is interpreting as a text string. To sort properly, we’ll need to change it. Let’s update the dataframe by mutating that column to change the data type to numeric.\n\nstate_population_dirty <- state_population_dirty %>%\n  mutate(total_pop = as.numeric(total_pop))\n\nglimpse(state_population_dirty)\n\nRows: 255\nColumns: 11\n$ geoid                        <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"02\", \"02\",…\n$ state                        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama…\n$ year                         <dbl> 2015, 2016, 2017, 2018, 2020, 2015, 2016,…\n$ total_pop                    <dbl> 4830620, 4841164, 4850771, 4864680, 48931…\n$ white_alone_pop              <dbl> 3325464, 3325037, 3317702, 3317453, 33028…\n$ black_alone_pop              <dbl> NA, NA, NA, NA, NA, 25022, 24443, 23702, …\n$ amer_ind_ak_native_alone_pop <dbl> 23850, 23919, 25098, 25576, 24764, 101313…\n$ asian_alone_pop              <dbl> 59599, 60744, 62815, 64609, 67909, 42921,…\n$ native_hi_alone_pop          <dbl> 2439, 2008, 2213, 2182, 2042, 8841, 8862,…\n$ other_race_alone_pop         <dbl> 61078, 61991, 66942, 70055, 74996, 9273, …\n$ two_or_more_races_pop        <dbl> 81646, 85412, 88834, 91619, 119322, 61755…\n\n\nNow when we sort, it works.\n\nstate_population_dirty %>%\n  filter(year == 2020) %>%\n  arrange(desc(total_pop))\n\n# A tibble: 51 × 11\n   geoid state      year total…¹ white…² black…³ amer_…⁴ asian…⁵ nativ…⁶ other…⁷\n   <chr> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 06    Californ…  2020  3.93e7  2.21e7 2250962  311629 5834312  149636 5623747\n 2 48    Texas      2020  2.86e7  1.98e7 3464424  137921 1415664   25328 1788398\n 3 12    Florida    2020  2.12e7  1.52e7 3381061   55655  590668   13339  699596\n 4 36    New York   2020  1.95e7  1.22e7 3002401   76535 1674216    9376 1670723\n 5 42    Pennsylv…  2020  1.28e7  1.02e7 1419582   20798  449320    4268  312888\n 6 17    Illinois   2020  1.27e7  8.87e6 1796660   33972  709567    5196  757150\n 7 39    Ohio       2020  1.17e7  9.39e6 1442655   20442  268527    3907  129717\n 8 13    Georgia    2020  1.05e7  6.02e6 3319844   34962  434603    7127  306609\n 9 37    North Ca…  2020  1.04e7  7.02e6 2217522  120272  308958    7368  334295\n10 26    Michigan   2020  9.97e6  7.74e6 1360149   50035  316844    3117  131580\n# … with 41 more rows, 1 more variable: two_or_more_races_pop <dbl>, and\n#   abbreviated variable names ¹​total_pop, ²​white_alone_pop, ³​black_alone_pop,\n#   ⁴​amer_ind_ak_native_alone_pop, ⁵​asian_alone_pop, ⁶​native_hi_alone_pop,\n#   ⁷​other_race_alone_pop"
  },
  {
    "objectID": "r-data-cleaning.html#missing-data",
    "href": "r-data-cleaning.html#missing-data",
    "title": "18  Data Cleaning",
    "section": "18.2 Missing Data",
    "text": "18.2 Missing Data\nThe second smell we can find in code is missing data.\nLet’s try to calculate the total Black alone population for the U.S.\n\nstate_population_dirty %>%\n  filter(year == 2020) %>%\n  summarise(\n    total_us_black_alone_population = sum(black_alone_pop)\n  )\n\n# A tibble: 1 × 1\n  total_us_black_alone_population\n                            <dbl>\n1                              NA\n\n\nWe get an NA value, which isn’t correct. Let’s examine the values in that column to see why.\n\nstate_population_dirty \n\n# A tibble: 255 × 11\n   geoid state    year total_pop white…¹ black…² amer_…³ asian…⁴ nativ…⁵ other…⁶\n   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 01    Alabama  2015   4830620 3325464      NA   23850   59599    2439   61078\n 2 01    Alabama  2016   4841164 3325037      NA   23919   60744    2008   61991\n 3 01    Alabama  2017   4850771 3317702      NA   25098   62815    2213   66942\n 4 01    Alabama  2018   4864680 3317453      NA   25576   64609    2182   70055\n 5 01    Alabama  2020   4893186 3302834      NA   24764   67909    2042   74996\n 6 02    Alaska   2015    733375  484250   25022  101313   42921    8841    9273\n 7 02    Alaska   2016    736855  483518   24443  103574   44218    8862    9900\n 8 02    Alaska   2017    738565  481971   23702  104995   45604    9075   10505\n 9 02    Alaska   2018    738516  478834   24129  106660   46556    8849   11027\n10 02    Alaska   2020    736990  466961   23894  107298   47289   10485   12231\n# … with 245 more rows, 1 more variable: two_or_more_races_pop <dbl>, and\n#   abbreviated variable names ¹​white_alone_pop, ²​black_alone_pop,\n#   ³​amer_ind_ak_native_alone_pop, ⁴​asian_alone_pop, ⁵​native_hi_alone_pop,\n#   ⁶​other_race_alone_pop\n\n\nAh, we see a missing value for Alabama. Because of that value, the summarise calculation won’t work. We could tell R to ignore NA values when doing the calculation, by adding na.rm=TRUE to the sum function.\n\nstate_population_dirty %>%\n  filter(year == 2020) %>%\n  summarise(\n    total_us_black_alone_population = sum(black_alone_pop, na.rm=TRUE)\n  )\n\n# A tibble: 1 × 1\n  total_us_black_alone_population\n                            <dbl>\n1                        39926065\n\n\nBut! That’s not the right answer. We want to know the total for the U.S. including Alabama in 2020. So, we need to fix that value. We look up the value on the Census website, and determine it’s 1,301,319 for 2020. So let’s update the column. This uses a function called case_when() with mutate().\nHere’s how to interpret what the function below says.\n“Overwrite the black_alone_pop column with new values in one of the rows, and the old values for every other row. If the state column equals Alabama AND (the &) the year column equals 2020 THEN (the tilde or ~) put the value 1301319. In any other case (any other state, or Alabama in any other year than 2020), THEN keep the value that currently exists in the black_alone_pop column.\n\nstate_population_dirty <- state_population_dirty %>%\n  mutate(black_alone_pop = case_when(\n    state == \"Alabama\" & year == 2020 ~ 1301319,\n    TRUE ~ black_alone_pop\n  ))\n\nstate_population_dirty\n\n# A tibble: 255 × 11\n   geoid state    year total_pop white…¹ black…² amer_…³ asian…⁴ nativ…⁵ other…⁶\n   <chr> <chr>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 01    Alabama  2015   4830620 3325464      NA   23850   59599    2439   61078\n 2 01    Alabama  2016   4841164 3325037      NA   23919   60744    2008   61991\n 3 01    Alabama  2017   4850771 3317702      NA   25098   62815    2213   66942\n 4 01    Alabama  2018   4864680 3317453      NA   25576   64609    2182   70055\n 5 01    Alabama  2020   4893186 3302834 1301319   24764   67909    2042   74996\n 6 02    Alaska   2015    733375  484250   25022  101313   42921    8841    9273\n 7 02    Alaska   2016    736855  483518   24443  103574   44218    8862    9900\n 8 02    Alaska   2017    738565  481971   23702  104995   45604    9075   10505\n 9 02    Alaska   2018    738516  478834   24129  106660   46556    8849   11027\n10 02    Alaska   2020    736990  466961   23894  107298   47289   10485   12231\n# … with 245 more rows, 1 more variable: two_or_more_races_pop <dbl>, and\n#   abbreviated variable names ¹​white_alone_pop, ²​black_alone_pop,\n#   ³​amer_ind_ak_native_alone_pop, ⁴​asian_alone_pop, ⁵​native_hi_alone_pop,\n#   ⁶​other_race_alone_pop\n\n\nNow when we run our summarization code, it works.\n\nstate_population_dirty %>%\n  filter(year == 2020) %>%\n  summarise(\n    total_us_black_alone_population = sum(black_alone_pop)\n  )\n\n# A tibble: 1 × 1\n  total_us_black_alone_population\n                            <dbl>\n1                        41227384"
  },
  {
    "objectID": "r-data-cleaning.html#gaps-in-data",
    "href": "r-data-cleaning.html#gaps-in-data",
    "title": "18  Data Cleaning",
    "section": "18.3 Gaps in data",
    "text": "18.3 Gaps in data\nLet’s now look at gaps in data. One type of gap in data has to do with time.\nLet’s calculate the average white alone population in Maryland over the period represented in the data, which is 2015 to 2020.\n\nstate_population_dirty %>%\n  filter(state == \"Maryland\") %>%\n  summarise(\n    mean_white_alone_pop = mean(white_alone_pop)\n  )\n\n# A tibble: 1 × 1\n  mean_white_alone_pop\n                 <dbl>\n1             3373558.\n\n\nDoes this represent the average white population from 2015 to 2020? Let’s take a closer look at the years represented in the data.\n\nstate_population_dirty %>%\n  filter(state == \"Maryland\") %>%\n  select(year)\n\n# A tibble: 5 × 1\n   year\n  <dbl>\n1  2015\n2  2016\n3  2017\n4  2018\n5  2020\n\n\nWe have 2015, 2016, 2017, 2018 and 2020. We can’t accurately say this represents the average over this period without 2019. So let’s add it, using a function called add_row() to put in the correct value 3,343,003. Note that it adds a row for 2019.\n\nstate_population_dirty %>%\n  filter(state == \"Maryland\") %>%\n  select(state,year,white_alone_pop) %>%\n  add_row(\n    state = \"Maryland\",\n    year = 2019,\n    white_alone_pop = 3343003\n  )\n\n# A tibble: 6 × 3\n  state     year white_alone_pop\n  <chr>    <dbl>           <dbl>\n1 Maryland  2015         3416107\n2 Maryland  2016         3408240\n3 Maryland  2017         3395212\n4 Maryland  2018         3373181\n5 Maryland  2020         3275048\n6 Maryland  2019         3343003\n\n\nNow, when we take the average for Maryland, it’s accurate.\n\nstate_population_dirty %>%\n  filter(state == \"Maryland\") %>%\n  select(state,year,white_alone_pop) %>%\n  add_row(\n    state = \"Maryland\",\n    year = 2019,\n    white_alone_pop = 3343003\n  ) %>%  \n  summarise(\n    mean_white_alone_pop = mean(white_alone_pop)\n  )\n\n# A tibble: 1 × 1\n  mean_white_alone_pop\n                 <dbl>\n1             3368465.\n\n\nQuestion #1:  Answer this question in English: Write in Elms\nYou have a dataframe that contains population statistics for each of 24 counties in Maryland, and you want to group and summarize to determine the total population for the state.\nThe first three rows of the data look like this:\nstate | county | population Maryland | PG | 1000000 MD | Montgomery | 1100000 md | Baltimore city | 900000\nWhat tidyverse method described in this lab would you use to clean the state column so that the group and summarization works properly.\nYou don’t need to write code, just explain in English on ELMS."
  },
  {
    "objectID": "r-data-cleaning.html#resources",
    "href": "r-data-cleaning.html#resources",
    "title": "18  Data Cleaning",
    "section": "18.4 Resources",
    "text": "18.4 Resources\n\nThe “Relational data” chapter in the R for Data Science textbook has details on exactly how a complex data set might fit together.\nAn example using a superheroes dataset, from Stat 545 at the University of British Columbia"
  },
  {
    "objectID": "r-geographic-basics.html",
    "href": "r-geographic-basics.html",
    "title": "19  Geographic data basics",
    "section": "",
    "text": "A common way data journalists look for patterns in data is geographically.\nIs there a spatial pattern to our data? Can we learn anything by using distance as a metric? What if we merge non-geographic data into geographic data?\nWe’ll use several libraries in this chapter:"
  },
  {
    "objectID": "r-geographic-basics.html#loading-libraries",
    "href": "r-geographic-basics.html#loading-libraries",
    "title": "19  Geographic data basics",
    "section": "19.1 Loading libraries",
    "text": "19.1 Loading libraries\nLoad libraries as usual. You will need to install sf and leaflet\n\n#install.packages('sf')\n#install.packages('leaflet')\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)"
  },
  {
    "objectID": "r-geographic-basics.html#getting-started",
    "href": "r-geographic-basics.html#getting-started",
    "title": "19  Geographic data basics",
    "section": "19.2 Getting started",
    "text": "19.2 Getting started\nOur motivating question for this chapter is this: do Prince George’s County elementary schools in lower-income areas have lower attendance rate than elementary schools in wealthier areas?\nWe’ll visualize data to help us get insight into answer this question, followed by a more precise analysis.\nWe’ll start by loading a file with the location of Prince George’s County elementary schools.\nThis is a special kind of dataframe called an “SF” dataframe, meaning it has location information embedded. The last column: “Geometry” has a pair of latitude and longitude coordinates that will allow us to map these.\n\npg_elementary_school_locations <- read_rds(\"assets/data/pg_elementary_school_locations.rds\")\n\npg_elementary_school_locations\n\nSimple feature collection with 96 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.01563 ymin: 38.66074 xmax: -76.71038 ymax: 39.10973\nGeodetic CRS:  NAD83\nFirst 10 features:\n   objectid          county psc_number                       school_name\n1       231 Prince George's     16.108              Greenbelt Elementary\n2       554 Prince George's     16.210 Fort Washington Forest Elementary\n3       558 Prince George's     16.205              Allenwood Elementary\n4       559 Prince George's     16.242             Longfields Elementary\n5       562 Prince George's     16.195             Kenilworth Elementary\n6       563 Prince George's     16.237                Concord Elementary\n7       566 Prince George's     16.244              Mattaponi Elementary\n8       567 Prince George's     16.233              Bond Mill Elementary\n9       569 Prince George's     16.228                  Baden Elementary\n10      573 Prince George's     16.214             Fort Foote Elementary\n                      street             city state   zip grades school_type\n1              66 RIDGE ROAD        GREENBELT    MD 20770 PreK-5  Elementary\n2         1300 FILLMORE ROAD  FORT WASHINGTON    MD 20744 PreK-6  Elementary\n3           6300 HARLEY LANE     TEMPLE HILLS    MD 20748 PreK-5  Elementary\n4        3300 NEWKIRK AVENUE      FORESTVILLE    MD 20747 PreK-6  Elementary\n5      12520 KEMBRIDGE DRIVE            BOWIE    MD 20715    K-5  Elementary\n6          2004 CONCORD LANE DISTRICT HEIGHTS    MD 20747 PreK-6  Elementary\n7   11701 DULEY STATION ROAD   UPPER MARLBORO    MD 20772    K-5  Elementary\n8      16001 SHERWOOD AVENUE           LAUREL    MD 20707    K-5  Elementary\n9  13601 BADEN-WESTWOOD ROAD       BRANDYWINE    MD 20613 PreK-6  Elementary\n10       8300 OXON HILL ROAD  FORT WASHINGTON    MD 20744 PreK-6  Elementary\n    type          latitude         longitude                   geometry\n1  Point -76.8771904543881  39.0123008214389  POINT (-76.87719 39.0123)\n2  Point -76.9853514953182  38.7090504890857 POINT (-76.98535 38.70905)\n3  Point -76.9301491524381  38.8030977354676  POINT (-76.93015 38.8031)\n4  Point -76.8804317232569  38.8448411219415 POINT (-76.88043 38.84484)\n5  Point -76.7345002966556  38.9592415465534  POINT (-76.7345 38.95924)\n6  Point  -76.907815584594  38.8632358639543 POINT (-76.90782 38.86324)\n7  Point -76.8051377927846  38.7478960342483  POINT (-76.80514 38.7479)\n8  Point -76.8955323025717  39.1097291984083 POINT (-76.89553 39.10973)\n9  Point -76.7703607256923  38.6607425148816 POINT (-76.77036 38.66074)\n10 Point -77.0056595337748  38.7758939117784 POINT (-77.00566 38.77589)\n\n\nLet’s use ggplot to show the location of the schools, using a new function for us, geom_sf\n\nggplot() +\n  geom_sf(data=pg_elementary_school_locations)\n\n\n\n\nThat’s not super helpful. If you know what the shape of Prince George’s County looks like, you can make it out there in the dots, but it’s hard.\nLet’s load an outline of the county to make it more explicit. Unlike the schools, which are “points”, the county outline is a “shape” or a “multipolygon.” It’s a collection of latitude and longitude pairs, with the dots connected.\n\npg_county <- read_rds(\"assets/data/pg_county.rds\")\n\nLet’s add the PG county outline to our map.\n\nggplot() +\n  geom_sf(data=pg_county) +\n  geom_sf(data=pg_elementary_school_locations)\n\n\n\n\nWe can visualize the school locations data on an interactive map, too, using the leaflet package.\n\nleaflet() %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addCircles(data=pg_elementary_school_locations)\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\n\n\n\n\n\naddProviderTiles adds the base map on which the dots appear. addCircles adds the school locations.\nLet’s update it to add the outline of P.G. County with addPolygons\n\nleaflet() %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(data=pg_county) %>%\n  addCircles(data=pg_elementary_school_locations)\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\n\n\n\n\n\nWe can even add little labels to each of the dots that includes the school name when we hover over each dot.\n\nleaflet() %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(data=pg_county) %>%\n  addCircles(data=pg_elementary_school_locations,\n             label=pg_elementary_school_locations$school_name)\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'"
  },
  {
    "objectID": "r-geographic-basics.html#augmenting-geographic-data",
    "href": "r-geographic-basics.html#augmenting-geographic-data",
    "title": "19  Geographic data basics",
    "section": "19.3 Augmenting geographic data",
    "text": "19.3 Augmenting geographic data\nWe can join information about each school with our geographic data, and represent that on a map. In this case, we’re going to load data about the “attendance rate” from the Maryland Dept. of Education for each Prince George’s County elementary school. It ranges from 99 percent to 79 percent.\n\nattendance <- read_rds(\"assets/data/attendance.rds\")\n\nattendance\n\n# A tibble: 96 × 6\n   `row_number()` county          county_number school_number school_n…¹ atten…²\n            <int> <chr>           <chr>         <chr>         <chr>        <dbl>\n 1              2 Prince George's 16            0105          Calverton…    88.7\n 2              5 Prince George's 16            0111          Vansville…    94.3\n 3              7 Prince George's 16            0205          Bladensbu…    87.4\n 4              8 Prince George's 16            0210          Rogers He…    90.7\n 5             10 Prince George's 16            0213          Cooper La…    91.1\n 6             11 Prince George's 16            0214          Templeton…    88.6\n 7             12 Prince George's 16            0217          Port Town…    86.8\n 8             13 Prince George's 16            0304          Perrywood…    95.7\n 9             14 Prince George's 16            0305          Patuxent …    91.1\n10             15 Prince George's 16            0504          Fort Wash…    93.3\n# … with 86 more rows, and abbreviated variable names ¹​school_name,\n#   ²​attend_rate_pct\n\n\nNow let’s join this attendance dataframe to our spatial dataframe.\nThe st_as_sf() function turns it back into a proper SF dataframe for visualization.\n\nattendance_locations <- attendance %>%\n  inner_join(pg_elementary_school_locations) %>%\n  st_as_sf()\n\nJoining, by = c(\"county\", \"school_name\")\n\n\nAnd let’s visualize the data again, but this time, we’ll let the attendance rate set the color value of the dots. Immediately, we see the beginnings of a pattern. Schools with higher attendance rates (lighter blue) are at right, in the more rural parts of the county. The schools with lower attendance rates (darker blue) are clustered to the left, closer the Washington, D.C. border.\n\nggplot() +\n  geom_sf(data=pg_county,fill=\"white\") +\n  geom_sf(data=attendance_locations, aes(color=attend_rate_pct),size=2)"
  },
  {
    "objectID": "r-geographic-basics.html#spatial-joins",
    "href": "r-geographic-basics.html#spatial-joins",
    "title": "19  Geographic data basics",
    "section": "19.4 Spatial joins",
    "text": "19.4 Spatial joins\nLet’s return to our motivating question for this chapter: do Prince George’s County elementary schools in lower-income areas have lower attendance rates than elementary schools in wealthier areas?\nFor geographic areas, we’ll use Census tracts, and as a measure of income, we’ll use median household income, which we’ll load in a second.\nTo answer this question, we need to connect our dataframe of attendance by school to our dataframe of income by Census tract.\nIf our dataframe of schools with attendance had a column indicating which Census tract it was in, we could simply do a regular join to our dataframe of Census tract income. But it does not.\nFortunately, there’s still a way we join them, using a special kind of join that only works with geographic data – a spatial join.\nFirst, let’s load a spatial dataframe of Census tracts with income information attached.\n\npg_tract_income <- read_rds(\"assets/data/pg_tract_income.rds\")\n\npg_tract_income\n\nSimple feature collection with 214 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -77.08627 ymin: 38.53537 xmax: -76.66952 ymax: 39.13129\nGeodetic CRS:  NAD83\nFirst 10 features:\n         geoid                                                   name\n1  24033802804 Census Tract 8028.04, Prince George's County, Maryland\n2  24033801701 Census Tract 8017.01, Prince George's County, Maryland\n3  24033801308 Census Tract 8013.08, Prince George's County, Maryland\n4  24033805601 Census Tract 8056.01, Prince George's County, Maryland\n5  24033800102 Census Tract 8001.02, Prince George's County, Maryland\n6  24033801411 Census Tract 8014.11, Prince George's County, Maryland\n7  24033802002 Census Tract 8020.02, Prince George's County, Maryland\n8  24033803527 Census Tract 8035.27, Prince George's County, Maryland\n9  24033800413 Census Tract 8004.13, Prince George's County, Maryland\n10 24033805802 Census Tract 8058.02, Prince George's County, Maryland\n   median_household_income                       geometry\n1                    56368 MULTIPOLYGON (((-76.89815 3...\n2                    71250 MULTIPOLYGON (((-76.95858 3...\n3                   129020 MULTIPOLYGON (((-77.01587 3...\n4                    57188 MULTIPOLYGON (((-76.99068 3...\n5                    77652 MULTIPOLYGON (((-76.86649 3...\n6                   106806 MULTIPOLYGON (((-77.0172 38...\n7                    91758 MULTIPOLYGON (((-76.93032 3...\n8                   102292 MULTIPOLYGON (((-76.832 38....\n9                    79821 MULTIPOLYGON (((-76.83767 3...\n10                   81172 MULTIPOLYGON (((-76.97962 3...\n\n\nLet’s visualize it. We can see that, like the lower attendance rate schools, the lower income tracts tend to be clustered closer to Washington, D.C.\n\nggplot() +\n  geom_sf(data=pg_tract_income, aes(fill=median_household_income))\n\n\n\n\nWe can add schools shaded by attendance rate, but it’s hard to derive much meaning from this map.\n\nggplot() +\n  geom_sf(data=pg_tract_income, aes(fill=median_household_income)) +\n  geom_sf(data=attendance_locations, aes(color=attend_rate_pct),size=2) \n\n\n\n\nTo truly answer our question, we’ll need to assign a Census tract to each school.\nTo do that, we’ll connect our attendance data and our income data with a “spatial join” or st_join(). We see now that for each school, it has been assigned a Census tract.\n\nincome_to_attendance <- attendance_locations %>%\n  st_join(pg_tract_income) \n\nincome_to_attendance\n\nSimple feature collection with 96 features and 20 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.01563 ymin: 38.66074 xmax: -76.71038 ymax: 39.10973\nGeodetic CRS:  NAD83\n# A tibble: 96 × 21\n   row_num…¹ county count…² schoo…³ schoo…⁴ atten…⁵ objec…⁶ psc_n…⁷ street city \n *     <int> <chr>  <chr>   <chr>   <chr>     <dbl>   <int> <chr>   <chr>  <chr>\n 1         2 Princ… 16      0105    Calver…    88.7     934 16.182  3400 … BELT…\n 2         5 Princ… 16      0111    Vansvi…    94.3     975 16.255  6813 … BELT…\n 3         7 Princ… 16      0205    Bladen…    87.4     656 16.106  4915 … BLAD…\n 4         8 Princ… 16      0210    Rogers…    90.7     949 16.051  4301 … BLAD…\n 5        10 Princ… 16      0213    Cooper…    91.1     636 16.131  3817 … LAND…\n 6        11 Princ… 16      0214    Temple…    88.6     617 16.155  6001 … RIVE…\n 7        12 Princ… 16      0217    Port T…    86.8     594 16.218  4351 … BLAD…\n 8        13 Princ… 16      0304    Perryw…    95.7    1038 16.207  501 W… UPPE…\n 9        14 Princ… 16      0305    Patuxe…    91.1     599 16.209  4410 … UPPE…\n10        15 Princ… 16      0504    Fort W…    93.3     554 16.210  1300 … FORT…\n# … with 86 more rows, 11 more variables: state <chr>, zip <chr>, grades <chr>,\n#   school_type <chr>, type <chr>, latitude <chr>, longitude <chr>,\n#   geometry <POINT [°]>, geoid <chr>, name <chr>,\n#   median_household_income <dbl>, and abbreviated variable names\n#   ¹​`row_number()`, ²​county_number, ³​school_number, ⁴​school_name,\n#   ⁵​attend_rate_pct, ⁶​objectid, ⁷​psc_number\n\n\nHere’s what happened when we did st_join.\nThe code looks at the latitude and longitude coordinates of each school and determines which Census tract shape it sits inside of. If you’re ever heard of “geocoding” an address to locate it on a map, the process is similar.\nFrom here, we can set about answering our question. We’ll create a column called attendance_bucket that classifies each school into one of two buckets – high attendance (top half) and low attendance (bottom half). We then group by that new attendance bucket and calculate the average income in each of those groups.\n\nincome_to_attendance %>%\n  mutate(attendance_bucket = ntile(attend_rate_pct,2)) %>%\n  mutate(attendance_bucket = case_when(\n    attendance_bucket == 1 ~ \"low_attendance\",\n    attendance_bucket == 2 ~ \"high_attendance\"\n  )) %>%\n  group_by(attendance_bucket) %>%\n  summarise(\n    median_household_income = mean(median_household_income)\n  )\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -77.01563 ymin: 38.66074 xmax: -76.71038 ymax: 39.10973\nGeodetic CRS:  NAD83\n# A tibble: 2 × 3\n  attendance_bucket median_household_income                             geometry\n  <chr>                               <dbl>                     <MULTIPOINT [°]>\n1 high_attendance                   105587. ((-76.78485 38.88359), (-76.81416 3…\n2 low_attendance                     80262. ((-76.71038 38.82766), (-77.00433 3…\n\n\nAnd finally our answer. Schools with high attendance are located in Census tracts that have, on average, an income of $105,587. Schools with low attendance are located in Census tracts that have, on average, income of $80,261.\nQuestion #1:  Answer this question in English: Write in Elms\nDevelop two questions that you could answer with a spatial join as described in this chapter, outside of the domain of education. Briefly describe the two geographic dataframes you would need to answer each question."
  },
  {
    "objectID": "r-geographic-analysis.html",
    "href": "r-geographic-analysis.html",
    "title": "20  Geographic data analysis",
    "section": "",
    "text": "We’re going to continue our work on geographic analysis by looking at how these methods can help us answer a slightly harder question.\nOur motivating question for this chapter is this: how many homicides occur near schools in Washington, D.C.?"
  },
  {
    "objectID": "r-geographic-analysis.html#loading-libraries",
    "href": "r-geographic-analysis.html#loading-libraries",
    "title": "20  Geographic data analysis",
    "section": "20.1 Loading libraries",
    "text": "20.1 Loading libraries\nLoad libraries as usual.\n\n#install.packages('sf')\n#install.packages('leaflet')\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)"
  },
  {
    "objectID": "r-geographic-analysis.html#data",
    "href": "r-geographic-analysis.html#data",
    "title": "20  Geographic data analysis",
    "section": "20.2 Data",
    "text": "20.2 Data\nWe’re loading two dataframes to answer our question.\n\nWashington Homicides | spatial points | location and details about each Washington homicide between 2007 and 2016. Details on the data: https://github.com/washingtonpost/data-homicides\nWashington Schools | spatial points | locations of each Washington, D.C. school. https://nces.ed.gov/programs/edge/geographic/schoollocations\nWashington D.C. shapefile | shapefile | outline of Washington, D.C.\n\n\n###\n# Load dataframe of washington homicides\n###\n\nwashington_homicides <- read_rds(\"assets/data/washington_homicides.rds\")\n\n###\n# Load dataframe of washington schools\n###\n\nwashington_schools <- read_rds(\"assets/data/washington_schools.rds\")\n\n###\n# Load outline of DC\n###\n\nwashington_shapefile <- read_rds(\"assets/data/washington_shapefile.rds\")\n\nHere are the 239 schools in Washington.\n\nggplot() +\n  geom_sf(data=washington_shapefile, fill=\"white\") + \n  geom_sf(data=washington_schools, color=\"purple\")\n\n\n\n\nHere are the 1345 homicides in Washington between 2007 and 2017.\n\nggplot() +\n  geom_sf(data=washington_shapefile, fill=\"white\") + \n  geom_sf(data=washington_homicides, color=\"red\")\n\n\n\n\nAnd here are both on a map.\n\nggplot() +\n  geom_sf(data=washington_shapefile, fill=\"white\") + \n  geom_sf(data=washington_homicides, color=\"red\") +\n  geom_sf(data=washington_schools, color=\"purple\") \n\n\n\n\nLet’s think back to our question: how many homicides occurred near schools?\nIn the prior lab, we used something called a “spatial join” to tie a school – a geographic point – to a Census tract – a geographic polygon.\nWe could try something similar here. The code below says: create a new object called schools_homicides by attempting to connect each point in our schools table with each point in our homicides table. If there’s an overlap, return that match to our new table.\n\nschools_homicides <- washington_schools %>%\n  st_join(washington_homicides, left=\"FALSE\")\n\nschools_homicides\n\nSimple feature collection with 0 features and 15 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  NAD83\n# A tibble: 0 × 16\n# … with 16 variables: school_name <chr>, address <chr>, city.x <chr>,\n#   state.x <chr>, zip <chr>, geometry <GEOMETRY [°]>, uid <chr>,\n#   reported_date <dbl>, victim_last <chr>, victim_first <chr>,\n#   victim_race <chr>, victim_age <chr>, victim_sex <chr>, city.y <chr>,\n#   state.y <chr>, disposition <chr>\n\n\nNotice that it returned zero rows. That’s because for there to be a match, a school point had to be essentially on top of a shooting point. The points are very small, and it’s unlikely there will be an exact match in our data.\nWe can fix that by making the size of the point bigger, by converting it to a polygon. How do we define “near” for the purpose of our question? Let’s use “within 100 meters” of the centerpoint of the school as our definition.\nWe’re going to use a new function st_buffer to create a “buffer” around our latitude longitude point. We’re going to change our geometry from a small point to a circle with a radius of 100 meters centered on that point.\n\nwashington_schools_buffered <- washington_schools %>%\n  mutate(geometry = st_buffer(geometry, dist=100))\n\nwashington_schools_buffered\n\nSimple feature collection with 239 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -77.10182 ymin: 38.82285 xmax: -76.91885 ymax: 38.98553\nGeodetic CRS:  NAD83\n# A tibble: 239 × 6\n   school_name               address city  state zip                    geometry\n * <chr>                     <chr>   <chr> <chr> <chr>             <POLYGON [°]>\n 1 Cesar Chavez PCS for Pub… 3701 H… Wash… DC    20019 ((-76.94809 38.90084, -7…\n 2 Cesar Chavez Public Char… 3701 H… Wash… DC    20019 ((-76.94809 38.90084, -7…\n 3 Friendship PCS - Collegi… 4095 M… Wash… DC    20019 ((-76.94558 38.89821, -7…\n 4 Friendship PCS - Southea… 645 MI… Wash… DC    20032 ((-76.99599 38.84797, -7…\n 5 Friendship PCS - Technol… 2705 M… Wash… DC    20032 ((-76.99592 38.84915, -7…\n 6 Friendship PCS - Blow Pi… 725 19… Wash… DC    20002 ((-76.97583 38.89864, -7…\n 7 Friendship PCS - Blow Pi… 725 19… Wash… DC    20002 ((-76.97583 38.89864, -7…\n 8 Friendship PCS - Chamber… 1345 P… Wash… DC    20003 ((-76.9878 38.87966, -76…\n 9 Friendship PCS - Woodrid… 2959 C… Wash… DC    20018 ((-76.96495 38.92914, -7…\n10 Friendship PCS - Woodrid… 2959 C… Wash… DC    20018 ((-76.96495 38.92914, -7…\n# … with 229 more rows\n\n\nLet’s visualize it. It’s hard to see the difference from this map, but those purple circles represent a 100 meter radius circle around our school point.\n\nggplot() +\n  geom_sf(data=washington_shapefile, fill=\"white\") + \n  geom_sf(data=washington_schools_buffered, color=\"purple\") \n\n\n\n\nIt’s easier to see with a leaflet map that shows both the points and the buffered circle. Zoom in.\n\nleaflet() %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(data=washington_schools_buffered, weight=1, fill=\"purple\") %>%\n  addCircles(data=washington_schools,\n             label=washington_schools$school_name)\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\n\n\n\n\n\nWe can also see the difference in the geometry column between our buffered dataframe and our point dataframe.\nOur washington_schools dataframe is a collection of points.\n\nwashington_schools$geometry\n\nGeometry set for 239 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.10065 ymin: 38.82376 xmax: -76.92001 ymax: 38.98462\nGeodetic CRS:  NAD83\nFirst 5 geometries:\n\n\nPOINT (-76.94906 38.90134)\nPOINT (-76.94906 38.90134)\n\n\nPOINT (-76.94631 38.89751)\n\n\nPOINT (-76.997 38.84754)\n\n\nPOINT (-76.99629 38.8483)\n\n\nOur washington_schools_buffered dataframe is a collection of polygons\n\nwashington_schools_buffered$geometry\n\nGeometry set for 239 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -77.10182 ymin: 38.82285 xmax: -76.91885 ymax: 38.98553\nGeodetic CRS:  NAD83\nFirst 5 geometries:\n\n\nPOLYGON ((-76.94809 38.90084, -76.94808 38.9008...\nPOLYGON ((-76.94809 38.90084, -76.94808 38.9008...\n\n\nPOLYGON ((-76.94558 38.89821, -76.94558 38.8982...\n\n\nPOLYGON ((-76.99599 38.84797, -76.99599 38.8479...\n\n\nPOLYGON ((-76.99592 38.84915, -76.99592 38.8491...\n\n\nNow that we have a buffered schools file, we can redo our spatial join.\n\nschools_homicides <- washington_schools_buffered %>%\n  st_join(washington_homicides, left=\"FALSE\")\n\nschools_homicides\n\nSimple feature collection with 87 features and 15 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -77.04171 ymin: 38.82285 xmax: -76.91885 ymax: 38.9614\nGeodetic CRS:  NAD83\n# A tibble: 87 × 16\n   school…¹ address city.x state.x zip                    geometry uid   repor…²\n * <chr>    <chr>   <chr>  <chr>   <chr>             <POLYGON [°]> <chr>   <dbl>\n 1 Friends… 4095 M… Washi… DC      20019 ((-76.94558 38.89821, -7… Was-…  2.01e7\n 2 Friends… 4095 M… Washi… DC      20019 ((-76.94558 38.89821, -7… Was-…  2.01e7\n 3 Friends… 645 MI… Washi… DC      20032 ((-76.99599 38.84797, -7… Was-…  2.01e7\n 4 Friends… 1351 N… Washi… DC      20011 ((-77.03121 38.95965, -7… Was-…  2.02e7\n 5 Friends… 645 MI… Washi… DC      20032 ((-76.99599 38.84806, -7… Was-…  2.01e7\n 6 Moten ES 1565 M… Washi… DC      20020 ((-76.98158 38.8566, -76… Was-…  2.01e7\n 7 Stanton… 2701 N… Washi… DC      20020 ((-76.96953 38.85878, -7… Was-…  2.02e7\n 8 Sousa MS 3650 E… Washi… DC      20019 ((-76.95386 38.88319, -7… Was-…  2.01e7\n 9 Savoy ES 2400 S… Washi… DC      20020 ((-76.99378 38.86371, -7… Was-…  2.01e7\n10 Marie R… 2201 1… Washi… DC      20009 ((-77.0414 38.91974, -77… Was-…  2.01e7\n# … with 77 more rows, 8 more variables: victim_last <chr>, victim_first <chr>,\n#   victim_race <chr>, victim_age <chr>, victim_sex <chr>, city.y <chr>,\n#   state.y <chr>, disposition <chr>, and abbreviated variable names\n#   ¹​school_name, ²​reported_date\n\n\nWe get 87 rows. This does NOT mean:\n\nWe had 87 homicides near schools. Some individual homicides occurred within 100 meters of more than one school.\nThere were homicides near 87 schools. Some schools had multiple homicides.\n\nTo answer our original question, “how many homicides occurred near (within 100 meters) of a DC school?”, we need to distinct our list of schools and count them.\n\nschools_homicides %>%\n  distinct(school_name) %>%\n  count()\n\n# A tibble: 1 × 1\n      n\n  <int>\n1    65\n\n\nThere’s our answer. Out of 239 DC schools, 65 – more than 25 percent – had a homicide occur within 100 meters.\nQuestion #1:  Answer this question in English: Write in Elms\nDevelop two questions that you could answer with a spatial buffer as described in this chapter, outside of the domain of education and crime. Briefly describe the two geographic dataframes you would need to answer each question."
  },
  {
    "objectID": "r-tidycensus-api.html",
    "href": "r-tidycensus-api.html",
    "title": "21  Intro to APIs: The Census",
    "section": "",
    "text": "When we’ve brought data in R to analyze up to this point, we’ve mostly been working with CSVs stored on our local computers or read them off the internet.\nBut there are many, many ways to bring data into R. Today we’re going to learn a new method that will allow us to pull data stored in a third-party database using a thing called an API or application programming interface. API means different things in different contexts, but in this case it means “a way to get data we want stored in a third-party database, in specific ways that third-party allows.”\nTwitter’s API lets us pull in a user’s tweets for text analysis.\nSpotify let’s us ingest a bunch of information about what people are listening to.\nYou can access data available through pretty much any API, or application programming interface, by sending a “query” and getting back a response, using the excellent HTTR package. That process is a bit complicated for this introduction.\nFortunately, users in the R communities have developed dozens of packages that allow us to pull data from APIs without learning a bunch of complex new programming concepts. These packages hide all the messy bits of connecting to an API inside of handy functions that are easier to use.\nHere are some great examples:\nTidycensus: for loading and working with all manner of U.S. Census Data. This is what we’ll work with today. RTweet: want to analyze all of Trump’s tweets (before he got banned) this will help you. SpotifyR: want to know what key most Taylor Swift songs are in? This can help you. cfbfastR: want to analyze play-by-play data for your favorite college football team? Look no further. wehoop: want to know more about your favorite WNBA team? Here you go. sportsdataverse: is a collection of API wrappers for a dozen sports leagues, including hockey, soccer, baseball and even chess. ARCOS: want to know where the most opioids were shipped by drug companies over the last decade. This API from the post will help. gmailr Feel like ingesting your email inbox into R and analyzing it? Or sending emails from R Studio? This will help.\nWe’ll be working with U.S. Census data today, and the tidycensus package."
  },
  {
    "objectID": "r-tidycensus-api.html#using-tidycensus",
    "href": "r-tidycensus-api.html#using-tidycensus",
    "title": "21  Intro to APIs: The Census",
    "section": "21.2 Using Tidycensus",
    "text": "21.2 Using Tidycensus\nThere is truly an astonishing amount of data collected by the US Census Bureau. First, there’s the Census that most people know – the every 10 year census. That’s the one mandated by the Constitution where the government attempts to count every person in the US. It’s a mind-boggling feat to even try, and billions get spent on it. That data is used first for determining how many representatives each state gets in Congress. From there, the Census gets used to divide up billions of dollars of federal spending.\nTo answer the questions the government needs to do that, a ton of data gets collected. That, unfortunately, means the Census is exceedingly complicated to work with.\nThe good news is, the Census has an API – an application programming interface. What that means is we can get data directly through the Census Bureau’s database via calls over the internet.\nLet’s demonstrate.\nWe’re going to use a library called tidycensus which makes calls to the Census API in a very tidy way, and gives you back tidy data. That means we don’t have to go through the process of importing the data from a file.\nI can’t tell you how amazing this is, speaking from experience. The documentation for this library is here.\nFirst we need to install tidycensus using the console: install.packages(\"tidycensus\")\nNow load both the tidyverse and tidycensus and janitor.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidycensus)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nTo use the API, you need an API key. An API key is just a password. You’ll run a function to get data from the Census, essentially knocking on the agency’s data door. Before it will let you know, it will ask for your password.\nSome APIs are open for access without a password. But most are like the Census, which require you to have some sort of authentication key.\nTo get that for the Census, you need to apply for an API key with the Census Bureau. It takes a few minutes and you need to activate your key via email. Once you have your key, you need to set that for this session. Just FYI: Your key is your key. Do not share it around.\nIn the code block below, replace YOUR KEY HERE with your key you get by email. It’s a long string of letters and numbers, like this: 549950d36c22ff16455fe196bbbd01d63cfba1cf. Don’t use this one. It won’t work. Use your own.\n\ncensus_api_key(\"YOUR KEY HERE\", install=TRUE)\n\nThe two main functions in tidycensus are get_decennial, which retrieves data from the 2000 and 2010 Censuses (and soon the 2020 Census), and get_acs, which pulls data from the American Community Survey, a between-Censuses annual survey that provides estimates, not hard counts, but asks more detailed questions.\nIf you’re new to Census data, there’s a very good set of slides from Kyle Walker, the creator of tidycensus, and he’s working on a book that you can read for free online.\nIt’s important to keep in mind that Census data represents people - you, your neighbors and total strangers. It also requires some level of definitions, especially about race & ethnicity, that may or may not match how you define yourself or how others define themselves.\nSo to give you some idea of how complicated the data is, let’s pull up just one file from the decennial Census. We’ll use a file called “pl”, which is short for “public law”, which is named that for reasons too boring to delve into here. It represents the first set of data released from the 2020 Census, and it only contains basics about total population counts, counts by race, and housing information.\nThis function load_variables returns a dataframe of all of the different variables available in the pl dataset. This table has three columns.\n\nname: a unique code for a given variable.\nlabel: a description of what that variable describes.\nconcept: a larger category that the label sits under.\n\n\npl_2020 <- load_variables(2020, \"pl\", cache = TRUE)\n\npl_2020\n\n# A tibble: 301 × 3\n   name    label                                                         concept\n   <chr>   <chr>                                                         <chr>  \n 1 H1_001N \" !!Total:\"                                                   OCCUPA…\n 2 H1_002N \" !!Total:!!Occupied\"                                         OCCUPA…\n 3 H1_003N \" !!Total:!!Vacant\"                                           OCCUPA…\n 4 P1_001N \" !!Total:\"                                                   RACE   \n 5 P1_002N \" !!Total:!!Population of one race:\"                          RACE   \n 6 P1_003N \" !!Total:!!Population of one race:!!White alone\"             RACE   \n 7 P1_004N \" !!Total:!!Population of one race:!!Black or African Americ… RACE   \n 8 P1_005N \" !!Total:!!Population of one race:!!American Indian and Ala… RACE   \n 9 P1_006N \" !!Total:!!Population of one race:!!Asian alone\"             RACE   \n10 P1_007N \" !!Total:!!Population of one race:!!Native Hawaiian and Oth… RACE   \n# … with 291 more rows\n\n\nFor example: P1_001N (row 4) represents the total population. P1_003N (row 5) represents the total white population.\nLet’s start by getting the total population of each state in 2020. For that we’ll use a function called get_decennial and we’re feeding it three arguments.\n\ngeography: do we want the information for each state? for each county? for all the census tracts in a single county? Depending on what we geography we select, we get something else back. We’ll pick state here.\nvariables: what information do we want from the Census. We used the load_variables function to identify the code for total population above, so we’ll plug it in here P1_001N.\nyear: do we want 2020 data? 2010? We’ll pick 2020 here.\n\n\ntotal_pop_2020 <- get_decennial(geography=\"state\", variables=\"P1_001N\", year=2020)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\n\nNote: 2020 decennial Census data use differential privacy, a technique that\nintroduces errors into data to preserve respondent confidentiality.\nℹ Small counts should be interpreted with caution.\nℹ See https://www.census.gov/library/fact-sheets/2021/protecting-the-confidentiality-of-the-2020-census-redistricting-data.html for additional guidance.\nThis message is displayed once per session.\n\ntotal_pop_2020\n\n# A tibble: 52 × 4\n   GEOID NAME                 variable    value\n   <chr> <chr>                <chr>       <dbl>\n 1 42    Pennsylvania         P1_001N  13002700\n 2 06    California           P1_001N  39538223\n 3 54    West Virginia        P1_001N   1793716\n 4 49    Utah                 P1_001N   3271616\n 5 36    New York             P1_001N  20201249\n 6 11    District of Columbia P1_001N    689545\n 7 02    Alaska               P1_001N    733391\n 8 12    Florida              P1_001N  21538187\n 9 45    South Carolina       P1_001N   5118425\n10 38    North Dakota         P1_001N    779094\n# … with 42 more rows\n\n\nWhat’s the biggest state by population? We can sort by the value column.\n\ntotal_pop_2020 <- get_decennial(geography=\"state\", variables=\"P1_001N\", year=2020) %>%\n  arrange(desc(value))\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\ntotal_pop_2020\n\n# A tibble: 52 × 4\n   GEOID NAME           variable    value\n   <chr> <chr>          <chr>       <dbl>\n 1 06    California     P1_001N  39538223\n 2 48    Texas          P1_001N  29145505\n 3 12    Florida        P1_001N  21538187\n 4 36    New York       P1_001N  20201249\n 5 42    Pennsylvania   P1_001N  13002700\n 6 17    Illinois       P1_001N  12812508\n 7 39    Ohio           P1_001N  11799448\n 8 13    Georgia        P1_001N  10711908\n 9 37    North Carolina P1_001N  10439388\n10 26    Michigan       P1_001N  10077331\n# … with 42 more rows\n\n\nLet’s also clean those funky column names, rename value and name to what they actually represent, and get rid of the geoid and variable column.\n\ntotal_pop_2020 <- get_decennial(geography=\"state\", variables=\"P1_001N\", year=2020) %>%\n  clean_names() %>%\n  arrange(desc(value)) %>%\n  rename(total_population = value,\n         state = name) %>%\n  select(state, total_population)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\ntotal_pop_2020\n\n# A tibble: 52 × 2\n   state          total_population\n   <chr>                     <dbl>\n 1 California             39538223\n 2 Texas                  29145505\n 3 Florida                21538187\n 4 New York               20201249\n 5 Pennsylvania           13002700\n 6 Illinois               12812508\n 7 Ohio                   11799448\n 8 Georgia                10711908\n 9 North Carolina         10439388\n10 Michigan               10077331\n# … with 42 more rows\n\n\nWhat if we wanted to calculate the percentage of the population of the entire US that identified as white alone? We can write a slightly modified function. We’ll set the geography to US and add another variable code, the one for the white alone population P1_003N.\n\npct_white_2020 <- get_decennial(geography=\"us\", variables=c(\"P1_001N\",\"P1_003N\"), year=2020) \n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\npct_white_2020\n\n# A tibble: 2 × 4\n  GEOID NAME          variable     value\n  <chr> <chr>         <chr>        <dbl>\n1 1     United States P1_001N  331449281\n2 1     United States P1_003N  204277273\n\n\nIt returns two rows, one for each variable. This “long” format will make it hard to answer our question, so we’re going to run our function again, but reshape the data a bit to make it “wide”. (Behind the scenes, they’re using pivot_wider()).\n\npct_white_2020 <- get_decennial(geography=\"us\", variables=c(\"P1_001N\",\"P1_003N\"), year=2020, output=\"wide\") \n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\npct_white_2020\n\n# A tibble: 1 × 4\n  GEOID NAME            P1_001N   P1_003N\n  <chr> <chr>             <dbl>     <dbl>\n1 1     United States 331449281 204277273\n\n\nThose column names are not helpful, so let’s rename them, and remove the GEOID.\n\npct_white_2020 <- get_decennial(geography=\"us\", variables=c(\"P1_001N\",\"P1_003N\"), year=2020, output=\"wide\") %>%\n  rename(geography = NAME,\n         total_pop = P1_001N,\n         white_pop = P1_003N) %>%\n  select(-GEOID)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\npct_white_2020\n\n# A tibble: 1 × 3\n  geography     total_pop white_pop\n  <chr>             <dbl>     <dbl>\n1 United States 331449281 204277273\n\n\nFinally, let’s calculate the percentage.\n\npct_white_2020 <- get_decennial(geography=\"us\", variables=c(\"P1_001N\",\"P1_003N\"), year=2020, output=\"wide\") %>%\n  rename(geography = NAME,\n         total_pop = P1_001N,\n         white_pop = P1_003N) %>%\n  select(-GEOID) %>%\n  mutate(pct_white = white_pop/total_pop*100)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\npct_white_2020\n\n# A tibble: 1 × 4\n  geography     total_pop white_pop pct_white\n  <chr>             <dbl>     <dbl>     <dbl>\n1 United States 331449281 204277273      61.6"
  },
  {
    "objectID": "r-tidycensus-api.html#the-acs",
    "href": "r-tidycensus-api.html#the-acs",
    "title": "21  Intro to APIs: The Census",
    "section": "21.3 The ACS",
    "text": "21.3 The ACS\nBetween the decennial censuses, the bureau releases results from the American Community Survey on a yearly schedule. It asks much more detailed questions than the decennial census.\nThe bad news is that it’s more complicated because it’s more like survey data with a large sample. That means there’s margins of error and confidence intervals to worry about.\nBy default, using get_acs fetches data from the 5-year average estimates (currently 2016-2021), but you can specify 1-year estimates for jurisdictions with at least 65,000 people (many counties and cities).\nHere’s an example using the 5-year ACS estimates. Let’s ask: what is Maryland’s richest county?\nOne way we can measure this is by median household income. That variable is B19013_001, so we can get that data like this:\n\nmd <- get_acs(geography = \"county\",\n              variables = c(median_income = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020) %>%\n  arrange(desc(estimate))\n\nGetting data from the 2016-2020 5-year ACS\n\nmd\n\n# A tibble: 24 × 5\n   GEOID NAME                          variable      estimate   moe\n   <chr> <chr>                         <chr>            <dbl> <dbl>\n 1 24027 Howard County, Maryland       median_income   124042  3448\n 2 24009 Calvert County, Maryland      median_income   112696  3287\n 3 24031 Montgomery County, Maryland   median_income   111812  1361\n 4 24017 Charles County, Maryland      median_income   103678  1654\n 5 24003 Anne Arundel County, Maryland median_income   103225  1817\n 6 24021 Frederick County, Maryland    median_income   100685  1927\n 7 24013 Carroll County, Maryland      median_income    99569  3051\n 8 24035 Queen Anne's County, Maryland median_income    96467  4785\n 9 24037 St. Mary's County, Maryland   median_income    95864  3872\n10 24025 Harford County, Maryland      median_income    94003  2398\n# … with 14 more rows\n\n\nHoward, Calvert, Montgomery, Anne Arundel, Charles. What do they all have in common? Lots of suburban flight from DC and Baltimore.\nBut do the margins of error – moe – let us say one county is richer than the other?\nWe can find this out visually using error bars. Don’t worry much about the code here – we’ll cover that in a later lab.\n\nmd %>%\n  mutate(NAME = gsub(\" County, Maryland\", \"\", NAME)) %>%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) +\n  geom_point(color = \"red\") +\n  labs(title = \"Household income by county in Maryland\",\n       subtitle = \"2015-2019 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate (bars represent margin of error)\")\n\n\n\n\nAs you can see, some of the error bars are quite wide. Some are narrow. But if the bars overlap, it means the difference between the two counties is within the margin of error, and the differences aren’t statistically significant. So is the difference between Calvert and Montgomery significant? Nope. Is the difference between Howard and everyone else significant? Yes it is.\nQuestion #1: \nRun this code to load a table of variables available in the American Community Survey.\n\nacs_variable <- load_variables(dataset = \"acs5\", year=2020)\n\nWrite code to answer this question, from the American Community Survey.\nWhich US state has the largest male population.\nWhat is the state and what is the percentage? Post the state, the population AND the code you wrote to produce it on ELMS. Use the American Community Survey for this."
  },
  {
    "objectID": "appendix-math.html",
    "href": "appendix-math.html",
    "title": "(APPENDIX) Appendix",
    "section": "",
    "text": "Note the disclaimer in this story on police shootings by The Washington Post, in which changes in the rates of police shootings may just be random.↩︎\nIt’s impossible for a number to fall more than 100 percent. That would mean it went below zero and then no formula works. There’s no good way to show a percent change when a figure like annual company earnings goes from profit to loss.↩︎\nThey are part of the “Numbers in the Newsroom” book from which this guide is derived.↩︎\nI’m using the term “average” freely here. Technically, a simple average and median are measures of central tendency, but I’ll treat them as different types of averages for simplicity sake.↩︎\nThis is sort of an example of “Simpson’s paradox” in that an average hides meaningful trends among sub-populations.↩︎\nIn statistical programs like R, there are various ways to specify how to deal with medians when there are ties like this. This is the most common way, but it may not be the way your program handles it.↩︎"
  }
]